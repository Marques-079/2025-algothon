{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_pips(data: np.array, n_pips: int, dist_measure: int):\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points from a 1D series.\n",
    "    dist_measure: 1=Euclidean, 2=Perpendicular, 3=Vertical.\n",
    "    \"\"\"\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    pips_y = [data[0], data[-1]]\n",
    "\n",
    "    for curr_point in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        insert_at = None\n",
    "\n",
    "        # ensure sorted pips_x\n",
    "        sorted_indices = sorted(range(len(pips_x)), key=lambda i: pips_x[i])\n",
    "        for k in range(len(sorted_indices) - 1):\n",
    "            left = sorted_indices[k]\n",
    "            right = sorted_indices[k + 1]\n",
    "            x1 = pips_x[left]\n",
    "            y1 = pips_y[left]\n",
    "            x2 = pips_x[right]\n",
    "            y2 = pips_y[right]\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            slope = dy / dx if dx != 0 else 0.0\n",
    "            intercept = y1 - slope * x1\n",
    "\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if i in pips_x:\n",
    "                    continue\n",
    "                y0 = data[i]\n",
    "                if dist_measure == 1:\n",
    "                    d = np.hypot(i - x1, y0 - y1) + np.hypot(i - x2, y0 - y2)\n",
    "                elif dist_measure == 2:\n",
    "                    d = abs(slope * i + intercept - y0) / np.hypot(slope, 1)\n",
    "                else:\n",
    "                    d = abs(slope * i + intercept - y0)\n",
    "\n",
    "                if d > max_dist:\n",
    "                    max_dist = d\n",
    "                    max_idx  = i\n",
    "                    insert_at = right\n",
    "\n",
    "        if max_idx is not None:\n",
    "            pips_x.insert(insert_at, max_idx)\n",
    "            pips_y.insert(insert_at, data[max_idx])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    paired = sorted(zip(pips_x, pips_y), key=lambda xy: xy[0])\n",
    "    xs, ys = zip(*paired)\n",
    "    return list(xs), list(ys)\n",
    "\n",
    "'''\n",
    "# ─── Load close-only price data ─────────────────────────────────────────────\n",
    "prices = pd.read_csv(\n",
    "    '../../prices.txt',\n",
    "    delim_whitespace=True,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "N_PIPS = 5\n",
    "DIST_MEASURE = 24\n",
    "LAST_N = 50\n",
    "\n",
    "# ─── Compute and plot for each instrument, slicing last LAST_N points ─────\n",
    "for inst in prices.columns[:3]:\n",
    "    series = prices[inst].values\n",
    "    # take last LAST_N points\n",
    "    slice_start = max(0, len(series) - LAST_N)\n",
    "    slice_data = series[slice_start:]\n",
    "    xs, ys = find_pips(slice_data, N_PIPS, DIST_MEASURE)\n",
    "    # convert xs back to global indices if needed:\n",
    "    global_xs = [slice_start + x for x in xs]\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(range(slice_start, len(series)), slice_data, color=\"tab:blue\", label=f\"Inst {inst}\")\n",
    "    plt.scatter(global_xs, ys, color=\"tab:red\", s=40, label=\"PIPs\")\n",
    "    plt.title(f\"Instrument {inst}: Last {LAST_N} Points with PIPs\")\n",
    "    plt.xlabel(\"Time index\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    # Always include first and last points\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    # Iteratively add PIPs\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        # examine each segment\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    # perpendicular distance\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Slide a window of length window_size over each instrument's log-price series,\n",
    "    extract PIP indices, normalize their values, and count pattern frequencies.\n",
    "\n",
    "    log_data: array shape (n_instruments, n_timesteps)\n",
    "    Returns Counter mapping normalized-PIP-value-tuples to counts.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            if max_v - min_v != 0:\n",
    "                norm = (vals - min_v) / (max_v - min_v)\n",
    "            else:\n",
    "                norm = np.zeros_like(vals)\n",
    "            pattern = tuple(np.round(norm, 3))\n",
    "            patterns[pattern] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(pattern_counts: Counter,\n",
    "                     n_clusters: int,\n",
    "                     threshold: float) -> Counter:\n",
    "    \"\"\"\n",
    "    Cluster the normalized PIP patterns and accumulate counts for patterns\n",
    "    whose distance to their cluster center is below threshold.\n",
    "\n",
    "    pattern_counts: Counter mapping pattern tuples to frequencies\n",
    "    n_clusters: number of clusters for KMeans\n",
    "    threshold: max distance to centroid to include in accumulation\n",
    "    Returns Counter mapping cluster_id to total frequency.\n",
    "    \"\"\"\n",
    "    patterns = list(pattern_counts.keys())\n",
    "    freqs = list(pattern_counts.values())\n",
    "    data = np.array(patterns)\n",
    "\n",
    "    # Run KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Accumulate counts for close matches\n",
    "    cluster_counts = Counter()\n",
    "    for label, vec, cnt in zip(labels, data, freqs):\n",
    "        dist = np.linalg.norm(vec - centers[label])\n",
    "        if dist <= threshold:\n",
    "            cluster_counts[label] += cnt\n",
    "    return cluster_counts\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'  # whitespace-delimited txt with 750 rows × 50 cols\n",
    "    WINDOW_SIZE = 100                # sliding window length\n",
    "    N_PIPS = 8                       # number of PIPs per window\n",
    "    DIST_MEASURE = 2                 # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                 # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.5         # distance threshold for cluster matching\n",
    "\n",
    "    # Load price data (50 instruments × 750 timesteps)\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "\n",
    "    # Convert to log-prices\n",
    "    log_prices = np.log(df.values.T)\n",
    "\n",
    "    # Extract PIP patterns\n",
    "    pattern_counts = extract_pip_patterns(log_prices, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    print(f\"Found {len(pattern_counts)} unique PIP patterns.\")\n",
    "\n",
    "    # Cluster and accumulate\n",
    "    cluster_counts = cluster_patterns(pattern_counts, N_CLUSTERS, CLUSTER_THRESHOLD)\n",
    "    print(\"\\nCluster ID -> Accumulated Frequency (within threshold)\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v - min_v != 0 else np.zeros_like(vals)\n",
    "            pattern = tuple(np.round(norm, 3))\n",
    "            patterns[pattern] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 100                    # sliding window length\n",
    "    N_PIPS = 8                           # number of PIPs per window\n",
    "    DIST_MEASURE = 2                     # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                      # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.5              # max distance to centroid to include in accumulation\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "    log_prices = np.log(df.values.T)     # shape (50, 750)\n",
    "\n",
    "    # 1) Extract PIP patterns\n",
    "    pattern_counts = extract_pip_patterns(log_prices, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    patterns = list(pattern_counts.keys())\n",
    "    freqs = list(pattern_counts.values())\n",
    "    print(f\"Found {len(patterns)} unique PIP patterns.\")\n",
    "\n",
    "    # 2) Cluster patterns\n",
    "    kmeans, data, labels = cluster_patterns(patterns, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 3) Accumulate cluster frequencies (within threshold)\n",
    "    cluster_counts = Counter()\n",
    "    pattern_to_cluster = {}\n",
    "    for pat, vec, lbl, cnt in zip(patterns, data, labels, freqs):\n",
    "        dist = np.linalg.norm(vec - centers[lbl])\n",
    "        if dist <= CLUSTER_THRESHOLD:\n",
    "            cluster_counts[lbl] += cnt\n",
    "            pattern_to_cluster[pat] = lbl\n",
    "\n",
    "    print(\"\\nCluster ID -> Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    # 4) Evaluate within-cluster PIP-based next move\n",
    "    move_counts = {cid: {'Up': 0, 'Down': 0} for cid in cluster_counts}\n",
    "    n_inst, n_t = log_prices.shape\n",
    "    for inst in range(n_inst):\n",
    "        series = log_prices[inst]\n",
    "        for end in range(WINDOW_SIZE, n_t):\n",
    "            window = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            # last two PIP indices (relative to window start)\n",
    "            idx_prev, idx_last = pips_idx[-2], pips_idx[-1]\n",
    "            val_prev = window[idx_prev]\n",
    "            val_last = window[idx_last]\n",
    "            # normalize pattern\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v - min_v != 0 else np.zeros_like(vals)\n",
    "            pat = tuple(np.round(norm, 3))\n",
    "            if pat in pattern_to_cluster:\n",
    "                cid = pattern_to_cluster[pat]\n",
    "                move = 'Up' if val_last - val_prev > 0 else 'Down'\n",
    "                move_counts[cid][move] += 1\n",
    "\n",
    "    print(\"\\nCluster ID -> Last PIP Move Distribution\")\n",
    "    for cid, counts in move_counts.items():\n",
    "        print(f\"Cluster {cid}: Up = {counts['Up']}, Down = {counts['Down']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v - min_v != 0 else np.zeros_like(vals)\n",
    "            pattern = tuple(np.round(norm, 3))\n",
    "            patterns[pattern] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 12                    # sliding window length\n",
    "    N_PIPS = 3                           # number of PIPs per window\n",
    "    DIST_MEASURE = 2                     # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                      # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.9              # max distance to centroid to include in accumulation\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "    log_prices = np.log(df.values.T)     # shape (50, 750)\n",
    "\n",
    "    # 1) Extract PIP patterns\n",
    "    pattern_counts = extract_pip_patterns(log_prices, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    patterns = list(pattern_counts.keys())\n",
    "    freqs = list(pattern_counts.values())\n",
    "    print(f\"Found {len(patterns)} unique PIP patterns.\")\n",
    "\n",
    "    # 2) Cluster patterns\n",
    "    kmeans, data, labels = cluster_patterns(patterns, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 3) Accumulate cluster frequencies (within threshold)\n",
    "    cluster_counts = Counter()\n",
    "    pattern_to_cluster = {}\n",
    "    for pat, vec, lbl, cnt in zip(patterns, data, labels, freqs):\n",
    "        dist = np.linalg.norm(vec - centers[lbl])\n",
    "        if dist <= CLUSTER_THRESHOLD:\n",
    "            cluster_counts[lbl] += cnt\n",
    "            pattern_to_cluster[pat] = lbl\n",
    "\n",
    "    print(\"\\nCluster ID -> Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    # 4) Evaluate within-cluster PIP-based last move\n",
    "    move_counts = {cid: {'Up': 0, 'Down': 0} for cid in cluster_counts}\n",
    "    n_inst, n_t = log_prices.shape\n",
    "    for inst in range(n_inst):\n",
    "        series = log_prices[inst]\n",
    "        for end in range(WINDOW_SIZE, n_t):\n",
    "            window = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            idx_prev, idx_last = pips_idx[-2], pips_idx[-1]\n",
    "            val_prev, val_last = window[idx_prev], window[idx_last]\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v - min_v != 0 else np.zeros_like(vals)\n",
    "            pat = tuple(np.round(norm, 3))\n",
    "            if pat in pattern_to_cluster:\n",
    "                cid = pattern_to_cluster[pat]\n",
    "                move = 'Up' if val_last - val_prev > 0 else 'Down'\n",
    "                move_counts[cid][move] += 1\n",
    "\n",
    "    print(\"\\nCluster ID -> Last PIP Move Distribution\")\n",
    "    for cid, counts in move_counts.items():\n",
    "        print(f\"Cluster {cid}: Up = {counts['Up']}, Down = {counts['Down']}\")\n",
    "\n",
    "    # 5) Overlay clusters on a single plot\n",
    "    x = np.arange(N_PIPS)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        first = True\n",
    "        for vec, lbl in zip(data, labels):\n",
    "            if lbl == cid:\n",
    "                if first:\n",
    "                    plt.plot(x, vec, label=f\"Cluster {cid}\", alpha=0.3)\n",
    "                    first = False\n",
    "                else:\n",
    "                    plt.plot(x, vec, alpha=0.1)\n",
    "    plt.title('Normalized PIP Patterns by Cluster')\n",
    "    plt.xlabel('PIP Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v - min_v != 0 else np.zeros_like(vals)\n",
    "            pattern = tuple(np.round(norm, 3))\n",
    "            patterns[pattern] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 50                    # sliding window length\n",
    "    N_PIPS = 5                          # number of PIPs per window\n",
    "    DIST_MEASURE = 2                     # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                      # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.95              # max distance to centroid to include in accumulation\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "    log_prices = np.log(df.values.T)     # shape (50, 750)\n",
    "\n",
    "    # 1) Extract PIP patterns\n",
    "    pattern_counts = extract_pip_patterns(log_prices, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    patterns = list(pattern_counts.keys())\n",
    "    freqs = list(pattern_counts.values())\n",
    "    print(f\"Found {len(patterns)} unique PIP patterns.\")\n",
    "\n",
    "    # 2) Cluster patterns\n",
    "    kmeans, data, labels = cluster_patterns(patterns, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 3) Accumulate cluster frequencies (within threshold)\n",
    "    cluster_counts = Counter()\n",
    "    pattern_to_cluster = {}\n",
    "    for pat, vec, lbl, cnt in zip(patterns, data, labels, freqs):\n",
    "        dist = np.linalg.norm(vec - centers[lbl])\n",
    "        if dist <= CLUSTER_THRESHOLD:\n",
    "            cluster_counts[lbl] += cnt\n",
    "            pattern_to_cluster[pat] = lbl\n",
    "\n",
    "    print(\"\\nCluster ID -> Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    # 4) Evaluate within-cluster PIP-based last move\n",
    "    move_counts = {cid: {'Up': 0, 'Down': 0} for cid in cluster_counts}\n",
    "    n_inst, n_t = log_prices.shape\n",
    "    for inst in range(n_inst):\n",
    "        series = log_prices[inst]\n",
    "        for end in range(WINDOW_SIZE, n_t):\n",
    "            window = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            idx_prev, idx_last = pips_idx[-2], pips_idx[-1]\n",
    "            val_prev, val_last = window[idx_prev], window[idx_last]\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v - min_v != 0 else np.zeros_like(vals)\n",
    "            pat = tuple(np.round(norm, 3))\n",
    "            if pat in pattern_to_cluster:\n",
    "                cid = pattern_to_cluster[pat]\n",
    "                move = 'Up' if val_last - val_prev > 0 else 'Down'\n",
    "                move_counts[cid][move] += 1\n",
    "\n",
    "    print(\"\\nCluster ID -> Last PIP Move Distribution\")\n",
    "    for cid, counts in move_counts.items():\n",
    "        print(f\"Cluster {cid}: Up = {counts['Up']}, Down = {counts['Down']}\")\n",
    "\n",
    "    # 5) Plot each cluster on its own figure\n",
    "x = np.arange(N_PIPS)\n",
    "for cid in range(N_CLUSTERS):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    any_plotted = False\n",
    "    for vec, lbl in zip(data, labels):\n",
    "        if lbl == cid:\n",
    "            plt.plot(x, vec, alpha=0.3)\n",
    "            any_plotted = True\n",
    "    if not any_plotted:\n",
    "        continue\n",
    "    # overlay cluster centroid prominently\n",
    "    plt.plot(x, centers[cid], label=f\"Centroid {cid}\", linewidth=2)\n",
    "    plt.title(f'Cluster {cid} (n={cluster_counts.get(cid, 0)})')\n",
    "    plt.xlabel('PIP Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 24                    # sliding window length\n",
    "    N_PIPS = 5                          # number of PIPs per window\n",
    "    DIST_MEASURE = 2                    # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                     # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.95            # max distance to centroid to include in accumulation\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "    log_prices = np.log(df.values.T)     # shape (50, 750)\n",
    "\n",
    "    # Split train/test\n",
    "    n_train = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data = log_prices[:, n_train:]\n",
    "    n_inst, n_test = test_data.shape\n",
    "\n",
    "    # 1) Train clusters on first 500 timesteps\n",
    "    train_patterns = extract_pip_patterns(train_data, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    train_vecs = list(train_patterns.keys())\n",
    "    train_freqs = list(train_patterns.values())\n",
    "    kmeans, train_array, train_labels = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 2) Evaluate test set: assign and accumulate\n",
    "    cluster_counts_test = Counter()\n",
    "    cluster_patterns_test = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "    move_counts_test = Counter({cid: {'Up': 0, 'Down': 0} for cid in range(N_CLUSTERS)})\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for end in range(WINDOW_SIZE, n_test):\n",
    "            window = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            lbl = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "            cluster_counts_test[lbl] += 1\n",
    "            cluster_patterns_test[lbl].append(tuple(np.round(norm, 3)))\n",
    "            move = 'Up' if vals[-1] - vals[-2] > 0 else 'Down'\n",
    "            move_counts_test[lbl][move] += 1\n",
    "\n",
    "    # 3) Print test results\n",
    "    print(\"\\nTest Set: Cluster ID -> Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts_test.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "    print(\"\\nTest Set: Cluster ID -> Last PIP Move Distribution\")\n",
    "    for cid, counts in move_counts_test.items():\n",
    "        up, down = counts['Up'], counts['Down']\n",
    "        if up + down > 0:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # 4) Plot each test-set cluster overlayed patterns\n",
    "    x = np.arange(N_PIPS)\n",
    "    for cid, pats in cluster_patterns_test.items():\n",
    "        if not pats:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for pat in pats:\n",
    "            plt.plot(x, pat, alpha=0.1)\n",
    "        plt.plot(x, centers[cid], color='black', linewidth=2, label=f'Centroid {cid}')\n",
    "        plt.title(f'Test Set Cluster {cid} (n={cluster_counts_test[cid]})')\n",
    "        plt.xlabel('PIP Index')\n",
    "        plt.ylabel('Normalized Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 24                    # sliding window length\n",
    "    N_PIPS = 5                          # number of PIPs per window\n",
    "    DIST_MEASURE = 2                    # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                     # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.95            # max distance to centroid to include in accumulation\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "    log_prices = np.log(df.values.T)     # shape (50, 750)\n",
    "\n",
    "    # Split train/test\n",
    "    n_train = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data = log_prices[:, n_train:]\n",
    "    n_inst, n_test = test_data.shape\n",
    "\n",
    "    # 1) Train clusters on first 500 timesteps\n",
    "    train_patterns = extract_pip_patterns(train_data, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    train_vecs = list(train_patterns.keys())\n",
    "    kmeans, _, _ = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 2) Evaluate test set: assign, accumulate, and record matches for instrument 0\n",
    "    matched_ends_inst0 = []\n",
    "    cluster_counts_test = Counter()\n",
    "    cluster_patterns_test = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "    move_counts_test = Counter({cid: {'Up': 0, 'Down': 0} for cid in range(N_CLUSTERS)})\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for end in range(WINDOW_SIZE, n_test):\n",
    "            window = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            lbl = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "            cluster_counts_test[lbl] += 1\n",
    "            cluster_patterns_test[lbl].append(tuple(np.round(norm, 3)))\n",
    "            move = 'Up' if vals[-1] - vals[-2] > 0 else 'Down'\n",
    "            move_counts_test[lbl][move] += 1\n",
    "            if inst == 0:\n",
    "                matched_ends_inst0.append(end)\n",
    "\n",
    "    # 3) Print test results\n",
    "    print(\"\\nTest Set: Cluster ID -> Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts_test.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "    print(\"\\nTest Set: Cluster ID -> Last PIP Move Distribution\")\n",
    "    for cid, counts in move_counts_test.items():\n",
    "        up, down = counts['Up'], counts['Down']\n",
    "        if up + down > 0:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # 4) Plot each test-set cluster overlayed patterns\n",
    "    x = np.arange(N_PIPS)\n",
    "    for cid, pats in cluster_patterns_test.items():\n",
    "        if not pats:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for pat in pats:\n",
    "            plt.plot(x, pat, alpha=0.1)\n",
    "        plt.plot(x, centers[cid], color='black', linewidth=2, label=f'Centroid {cid}')\n",
    "        plt.title(f'Test Set Cluster {cid} (n={cluster_counts_test[cid]})')\n",
    "        plt.xlabel('PIP Index')\n",
    "        plt.ylabel('Normalized Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5) Plot instrument 0 price with shaded pattern windows\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    times = np.arange(n_train, n_train + n_test)\n",
    "    prices0 = np.exp(test_data[0])\n",
    "    plt.plot(times, prices0, label='Instrument 0 Price')\n",
    "    for end in matched_ends_inst0:\n",
    "        start = n_train + end - WINDOW_SIZE\n",
    "        stop = n_train + end\n",
    "        plt.axvspan(start, stop, color='orange', alpha=0.3)\n",
    "    plt.title('Instrument 0 Test-Set Price with Pattern Matches')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 24                    # sliding window length\n",
    "    N_PIPS = 5                          # number of PIPs per window\n",
    "    DIST_MEASURE = 2                    # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                     # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.3            # max distance to centroid to include in accumulation\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    if df.shape[1] < 50:\n",
    "        raise ValueError(f\"Expected at least 50 instruments, found {df.shape[1]}\")\n",
    "    log_prices = np.log(df.values.T)     # shape (50, 750)\n",
    "\n",
    "    # Split train/test\n",
    "    n_train = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data = log_prices[:, n_train:]\n",
    "    n_inst, n_test = test_data.shape\n",
    "\n",
    "    # 1) Train clusters on first 500 timesteps\n",
    "    train_patterns = extract_pip_patterns(train_data, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    train_vecs = list(train_patterns.keys())\n",
    "    kmeans, _, _ = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 2) Evaluate test set: assign, accumulate, and record matches for instrument 0\n",
    "    matched_ends_inst0 = []\n",
    "    cluster_counts_test = Counter()\n",
    "    cluster_patterns_test = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "    move_counts_test = Counter({cid: {'Up': 0, 'Down': 0} for cid in range(N_CLUSTERS)})\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for end in range(WINDOW_SIZE, n_test):\n",
    "            window = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            lbl = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "            cluster_counts_test[lbl] += 1\n",
    "            cluster_patterns_test[lbl].append(tuple(np.round(norm, 3)))\n",
    "            move = 'Up' if vals[-1] - vals[-2] > 0 else 'Down'\n",
    "            move_counts_test[lbl][move] += 1\n",
    "            if inst == 0:\n",
    "                matched_ends_inst0.append(end)\n",
    "\n",
    "    # 3) Print test results\n",
    "    print(\"\\nTest Set: Cluster ID -> Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts_test.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "    print(\"\\nTest Set: Cluster ID -> Last PIP Move Distribution\")\n",
    "    for cid, counts in move_counts_test.items():\n",
    "        up, down = counts['Up'], counts['Down']\n",
    "        if up + down > 0:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # 4) Plot each test-set cluster overlayed patterns\n",
    "    x = np.arange(N_PIPS)\n",
    "    for cid, pats in cluster_patterns_test.items():\n",
    "        if not pats:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for pat in pats:\n",
    "            plt.plot(x, pat, alpha=0.1)\n",
    "        plt.plot(x, centers[cid], color='black', linewidth=2, label=f'Centroid {cid}')\n",
    "        plt.title(f'Test Set Cluster {cid} (n={cluster_counts_test[cid]})')\n",
    "        plt.xlabel('PIP Index')\n",
    "        plt.ylabel('Normalized Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- New cell: custom range plot ---\n",
    "# 6) Plot instrument 0 price over a specific timestep range and shade matching windows\n",
    "custom_start = 500\n",
    "custom_end   = 520\n",
    "\n",
    "# full time index and price for instrument 0\n",
    "times_full      = np.arange(n_train + n_test)\n",
    "prices0_full    = np.exp(log_prices[0])  # back to price scale\n",
    "\n",
    "# select the slice\n",
    "mask = (times_full >= custom_start) & (times_full <= custom_end)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(times_full[mask], prices0_full[mask], label=f'Price [{custom_start}:{custom_end}]')\n",
    "# shade only those pattern windows that intersect this slice\n",
    "for end in matched_ends_inst0:\n",
    "    start_idx = n_train + end - WINDOW_SIZE\n",
    "    stop_idx  = n_train + end\n",
    "    # check for overlap with custom slice\n",
    "    if stop_idx < custom_start or start_idx > custom_end:\n",
    "        continue\n",
    "    span_start = max(start_idx, custom_start)\n",
    "    span_stop  = min(stop_idx,  custom_end)\n",
    "    plt.axvspan(span_start, span_stop, color='orange', alpha=0.3)\n",
    "\n",
    "plt.title(f'Instrument 0 Price and Shaded Pattern Matches (t={custom_start}..{custom_end})')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Plot instrument 0 price with shaded pattern windows\n",
    "plt.figure(figsize=(12, 4))\n",
    "times = np.arange(n_train, n_train + n_test)\n",
    "prices0 = np.exp(test_data[1])\n",
    "plt.plot(times, prices0, label='Instrument 0 Price')\n",
    "for end in matched_ends_inst0:\n",
    "    start = n_train + end - WINDOW_SIZE\n",
    "    stop = n_train + end\n",
    "    plt.axvspan(start, stop, color='orange', alpha=0.1)\n",
    "plt.title('Instrument 0 Test-Set Price with Pattern Matches')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 36                    # sliding window length\n",
    "    N_PIPS = 8                          # total PIPs (we use N_PIPS-1 for feature)\n",
    "    DIST_MEASURE = 2                    # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                     # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.30            # max distance to centroid for assignment\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    log_prices = np.log(df.values.T)   # shape (50, 750)\n",
    "    n_inst, n_t = log_prices.shape\n",
    "\n",
    "    # Split train/test\n",
    "    n_train = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data = log_prices[:, n_train:]\n",
    "    _, n_test = test_data.shape\n",
    "\n",
    "    # 1) Train clusters on first 500 timesteps using N_PIPS-1 features (avoiding lookahead)\n",
    "# Extract only N_PIPS-1 PIPs from each train window\n",
    "train_patterns = Counter()\n",
    "for inst in range(train_data.shape[0]):\n",
    "    series = train_data[inst]\n",
    "    for t in range(WINDOW_SIZE, train_data.shape[1]):\n",
    "        window_past = series[t - WINDOW_SIZE : t]\n",
    "        pips_idx = find_pips(window_past, N_PIPS - 1, DIST_MEASURE)\n",
    "        if len(pips_idx) < 2:\n",
    "            continue\n",
    "        vals = window_past[pips_idx]\n",
    "        mn, mx = vals.min(), vals.max()\n",
    "        norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "        train_patterns[tuple(np.round(norm,3))] += 1\n",
    "train_vecs = list(train_patterns.keys())\n",
    "kmeans, _, _ = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# 2) Evaluate test set without look-ahead\n",
    "cluster_counts = Counter()\n",
    "move_counts = Counter({cid: {'Up': 0, 'Down': 0} for cid in range(N_CLUSTERS)})\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    series = test_data[inst]\n",
    "    for t in range(WINDOW_SIZE, n_test):\n",
    "        # (a) build past window excluding bar t\n",
    "        window_past = series[t - WINDOW_SIZE : t]\n",
    "        # (b) extract N_PIPS-1 PIPs from past data\n",
    "        pips_idx = find_pips(window_past, N_PIPS - 1, DIST_MEASURE)\n",
    "        if len(pips_idx) < 2:\n",
    "            continue\n",
    "        vals = window_past[pips_idx]\n",
    "        # normalize\n",
    "        mn, mx = vals.min(), vals.max()\n",
    "        norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "        # (c) assign cluster\n",
    "        lbl = kmeans.predict([norm])[0]\n",
    "        dist = np.linalg.norm(norm - centers[lbl][:N_PIPS-1])\n",
    "        if dist > CLUSTER_THRESHOLD:\n",
    "            continue\n",
    "        # record cluster hit\n",
    "        cluster_counts[lbl] += 1\n",
    "        # (d) observe next bar at t and compare to last past PIP\n",
    "        price_prev = series[t - 1]\n",
    "        price_next = series[t]\n",
    "        move = 'Up' if price_next > price_prev else 'Down'\n",
    "        move_counts[lbl][move] += 1\n",
    "\n",
    "# 3) Print results on test set\n",
    "print(\"\\nTest Set: Cluster ID -> Frequency\")\n",
    "for cid, freq in cluster_counts.most_common():\n",
    "    print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "print(\"\\nTest Set: Cluster ID -> Move Distribution\")\n",
    "for cid, cnts in move_counts.items():\n",
    "    up, down = cnts['Up'], cnts['Down']\n",
    "    if up + down:\n",
    "        print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "# 4) Visualize raw test-series with shaded hits for instrument 0\n",
    "times = np.arange(n_train, n_train + n_test)\n",
    "prices0 = np.exp(test_data[0])\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(times, prices0, label='Instrument 0 Price')\n",
    "for t in range(WINDOW_SIZE, n_test):\n",
    "    window_past = test_data[0][t - WINDOW_SIZE : t]\n",
    "    pips_idx = find_pips(window_past, N_PIPS - 1, DIST_MEASURE)\n",
    "    if len(pips_idx) < 2:\n",
    "        continue\n",
    "    vals = window_past[pips_idx]\n",
    "    mn, mx = vals.min(), vals.max()\n",
    "    norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "    lbl = kmeans.predict([norm])[0]\n",
    "    dist = np.linalg.norm(norm - centers[lbl][:N_PIPS-1])\n",
    "    if dist <= CLUSTER_THRESHOLD:\n",
    "        start = n_train + t - WINDOW_SIZE\n",
    "        stop  = n_train + t\n",
    "        plt.axvspan(start, stop, color='orange', alpha=0.2)\n",
    "plt.title('Instrument 0 Test-Set Price with Causal Pattern Matches')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nTest Set: Cluster ID -> Frequency\")\n",
    "for cid, freq in cluster_counts.most_common():\n",
    "    print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "print(\"\\nTest Set: Cluster ID -> Move Distribution\")\n",
    "# sort by cluster ID to keep it consistent\n",
    "for cid in sorted(move_counts):\n",
    "    up = move_counts[cid]['Up']\n",
    "    down = move_counts[cid]['Down']\n",
    "    # only print clusters that actually had hits\n",
    "    if up + down > 0:\n",
    "        print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "# ——— Now plot each cluster’s patterns + centroid ———\n",
    "for cid, patterns in cluster_counts_test.items():\n",
    "    if not patterns:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # overlay all member patterns\n",
    "    for pat in patterns:\n",
    "        plt.plot(range(len(pat)), pat, alpha=0.2)\n",
    "    # bold centroid\n",
    "    plt.plot(range(len(centers[cid])),\n",
    "                centers[cid],\n",
    "                color='black',\n",
    "                linewidth=2,\n",
    "                label=f\"Centroid {cid}\")\n",
    "    plt.title(f\"Test Set Cluster {cid} (n={len(patterns)})\")\n",
    "    plt.xlabel(\"PIP Index\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            min_v, max_v = vals.min(), vals.max()\n",
    "            norm = (vals - min_v) / (max_v - min_v) if max_v != min_v else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int) -> tuple[KMeans, np.ndarray, np.ndarray]:\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'      # whitespace-delimited txt (750 rows × 50 cols)\n",
    "    WINDOW_SIZE = 50                    # sliding window length\n",
    "    N_PIPS = 5                          # total PIPs (we use N_PIPS-1 for feature)\n",
    "    DIST_MEASURE = 2                    # distance measure for PIPs\n",
    "    N_CLUSTERS = 10                     # number of clusters to form\n",
    "    CLUSTER_THRESHOLD = 0.95            # max distance to centroid for assignment\n",
    "\n",
    "    # Load and log-transform data\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    log_prices = np.log(df.values.T)   # shape (50, 750)\n",
    "    n_inst, n_t = log_prices.shape\n",
    "\n",
    "    # Split train/test\n",
    "    n_train = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data = log_prices[:, n_train:]\n",
    "    _, n_test = test_data.shape\n",
    "\n",
    "    # 1) Train clusters on first 500 timesteps using N_PIPS-1 features (avoiding lookahead)\n",
    "# Extract only N_PIPS-1 PIPs from each train window\n",
    "train_patterns = Counter()\n",
    "for inst in range(train_data.shape[0]):\n",
    "    series = train_data[inst]\n",
    "    for t in range(WINDOW_SIZE, train_data.shape[1]):\n",
    "        window_past = series[t - WINDOW_SIZE : t]\n",
    "        pips_idx = find_pips(window_past, N_PIPS - 1, DIST_MEASURE)\n",
    "        if len(pips_idx) < 2:\n",
    "            continue\n",
    "        vals = window_past[pips_idx]\n",
    "        mn, mx = vals.min(), vals.max()\n",
    "        norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "        train_patterns[tuple(np.round(norm,3))] += 1\n",
    "train_vecs = list(train_patterns.keys())\n",
    "kmeans, _, _ = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 2) Evaluate test set without look-ahead\n",
    "    cluster_counts = Counter()\n",
    "    move_counts = Counter({cid: {'Up': 0, 'Down': 0} for cid in range(N_CLUSTERS)})\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_test):\n",
    "            # (a) build past window excluding bar t\n",
    "            window_past = series[t - WINDOW_SIZE : t]\n",
    "            # (b) extract N_PIPS-1 PIPs from past data\n",
    "            pips_idx = find_pips(window_past, N_PIPS - 1, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window_past[pips_idx]\n",
    "            # normalize\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            # (c) assign cluster\n",
    "            lbl = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl][:N_PIPS-1])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "            # record cluster hit\n",
    "            cluster_counts[lbl] += 1\n",
    "            # (d) observe next bar at t and compare to last past PIP\n",
    "            price_prev = series[t - 1]\n",
    "            price_next = series[t]\n",
    "            move = 'Up' if price_next > price_prev else 'Down'\n",
    "            move_counts[lbl][move] += 1\n",
    "\n",
    "    # 3) Print results on test set\n",
    "    print(\"\\nTest Set: Cluster ID -> Frequency\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    print(\"\\nTest Set: Cluster ID -> Move Distribution\")\n",
    "    for cid, cnts in move_counts.items():\n",
    "        up, down = cnts['Up'], cnts['Down']\n",
    "        if up + down:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # 4) Visualize raw test-series with shaded hits for instrument 0\n",
    "    times = np.arange(n_train, n_train + n_test)\n",
    "    prices0 = np.exp(test_data[0])\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(times, prices0, label='Instrument 0 Price')\n",
    "    for t in range(WINDOW_SIZE, n_test):\n",
    "        window_past = test_data[0][t - WINDOW_SIZE : t]\n",
    "        pips_idx = find_pips(window_past, N_PIPS - 1, DIST_MEASURE)\n",
    "        if len(pips_idx) < 2:\n",
    "            continue\n",
    "        vals = window_past[pips_idx]\n",
    "        mn, mx = vals.min(), vals.max()\n",
    "        norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "        lbl = kmeans.predict([norm])[0]\n",
    "        dist = np.linalg.norm(norm - centers[lbl][:N_PIPS-1])\n",
    "        if dist <= CLUSTER_THRESHOLD:\n",
    "            start = n_train + t - WINDOW_SIZE\n",
    "            stop  = n_train + t\n",
    "            plt.axvspan(start, stop, color='orange', alpha=0.2)\n",
    "    plt.title('Instrument 0 Test-Set Price with Causal Pattern Matches')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search for Best Pattern Clusters ---\n",
    "window_sizes    = [10, 18, 36, 54]\n",
    "n_pips_list     = [3, 5, 7, 9]\n",
    "n_clusters_list = [8, 10, 12]\n",
    "thresholds      = [0.25, 0.30, 0.35, 0.40]\n",
    "\n",
    "results = []  # (edge, total, win, pips, k, th, cid)\n",
    "combo_counter = 0\n",
    "for WIN in window_sizes:\n",
    "    for PIPS in n_pips_list:\n",
    "        for K in n_clusters_list:\n",
    "            for TH in thresholds:\n",
    "                combo_counter += 1\n",
    "                print(f\"\\n[Combo {combo_counter}] WIN={WIN}, PIPS={PIPS}, K={K}, TH={TH} → training…\")\n",
    "\n",
    "                # --- Train on causal (PIPS-1) features ---\n",
    "                train_patterns = Counter()\n",
    "                for inst in range(n_inst):\n",
    "                    series = train_data[inst]\n",
    "                    for t in range(WIN, n_train):\n",
    "                        win_past = series[t - WIN : t]\n",
    "                        idx = find_pips(win_past, PIPS - 1, DIST_MEASURE)\n",
    "                        if len(idx) < 2:\n",
    "                            continue\n",
    "                        vals = win_past[idx]\n",
    "                        mn, mx = vals.min(), vals.max()\n",
    "                        vec = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "                        train_patterns[tuple(np.round(vec, 3))] += 1\n",
    "                train_vecs = list(train_patterns.keys())\n",
    "                if len(train_vecs) < K:\n",
    "                    print(\"  ↳ skipped (not enough unique patterns for given K)\")\n",
    "                    continue\n",
    "                kmeans, _, _ = cluster_patterns(train_vecs, K)\n",
    "                centers = kmeans.cluster_centers_\n",
    "\n",
    "                # --- Test on unseen data ---\n",
    "                move_cnt = {cid: {'Up': 0, 'Down': 0} for cid in range(K)}\n",
    "                tot_cnt  = Counter()\n",
    "                for inst in range(n_inst):\n",
    "                    series = test_data[inst]\n",
    "                    for t in range(WIN, n_test):\n",
    "                        win_past = series[t - WIN : t]\n",
    "                        idx = find_pips(win_past, PIPS - 1, DIST_MEASURE)\n",
    "                        if len(idx) < 2:\n",
    "                            continue\n",
    "                        vals = win_past[idx]\n",
    "                        mn, mx = vals.min(), vals.max()\n",
    "                        vec = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "                        cid = kmeans.predict([vec])[0]\n",
    "                        dist = np.linalg.norm(vec - centers[cid][:PIPS-1])\n",
    "                        if dist > TH:\n",
    "                            continue\n",
    "                        tot_cnt[cid] += 1\n",
    "                        mv = 'Up' if series[t] > series[t-1] else 'Down'\n",
    "                        move_cnt[cid][mv] += 1\n",
    "\n",
    "                # --- Record edges per cluster ---\n",
    "                best_edge_combo = 0\n",
    "                for cid in range(K):\n",
    "                    up   = move_cnt[cid]['Up']\n",
    "                    down = move_cnt[cid]['Down']\n",
    "                    tot  = up + down\n",
    "                    if tot == 0:\n",
    "                        continue\n",
    "                    edge = abs(up - down) / tot\n",
    "                    if edge > best_edge_combo:\n",
    "                        best_edge_combo = edge\n",
    "                    results.append((edge, tot, WIN, PIPS, K, TH, cid))\n",
    "                print(f\"  ↳ finished. Best edge in this combo = {best_edge_combo:.3f} (across its clusters)\")\n",
    "\n",
    "# sort by edge then total\n",
    "results.sort(key=lambda x: (x[0], x[1]), reverse=True)\n",
    "print(\"\\n=========== TOP 10 PARAMETER + CLUSTER COMBINATIONS ===========\")\n",
    "print(\"edge\\ttotal\\twindow\\tpips\\tK\\tthreshold\\tcid\")\n",
    "for edge, tot, win, pips, k, th, cid in results[:10]:\n",
    "    print(f\"{edge:.3f}\\t{tot}\\t{win}\\t{pips}\\t{k}\\t{th}\\t{cid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance), 2=Perpendicular distance to chord, 3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den != 0 else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "# --- Grid Search for Best Pattern Clusters ---\n",
    "window_sizes    = [10, 18, 36, 54]\n",
    "n_pips_list     = [5, 7, 9]\n",
    "n_clusters_list = [8, 10, 12]\n",
    "thresholds      = [0.25, 0.30, 0.35, 0.40, 0.50]\n",
    "\n",
    "results = []  # (edge, total, win, pips, k, th, cid)\n",
    "combo_counter = 0\n",
    "for WIN in window_sizes:\n",
    "    for PIPS in n_pips_list:\n",
    "        for K in n_clusters_list:\n",
    "            for TH in thresholds:\n",
    "                combo_counter += 1\n",
    "                print(f\"\\n[Combo {combo_counter}] WIN={WIN}, PIPS={PIPS}, K={K}, TH={TH} → training…\")\n",
    "\n",
    "                # --- Train on causal (PIPS-1) features ---\n",
    "                train_patterns = Counter()\n",
    "                for inst in range(n_inst):\n",
    "                    series = train_data[inst]\n",
    "                    for t in range(WIN, n_train):\n",
    "                        win_past = series[t - WIN : t]\n",
    "                        idx = find_pips(win_past, PIPS - 1, DIST_MEASURE)\n",
    "                        if len(idx) < 2:\n",
    "                            continue\n",
    "                        vals = win_past[idx]\n",
    "                        mn, mx = vals.min(), vals.max()\n",
    "                        vec = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "                        train_patterns[tuple(np.round(vec, 3))] += 1\n",
    "                train_vecs = list(train_patterns.keys())\n",
    "                if len(train_vecs) < K:\n",
    "                    print(\"  ↳ skipped (not enough unique patterns for given K)\")\n",
    "                    continue\n",
    "                kmeans, _, _ = cluster_patterns(train_vecs, K)\n",
    "                centers = kmeans.cluster_centers_\n",
    "\n",
    "                # --- Test on unseen data ---\n",
    "                move_cnt = {cid: {'Up': 0, 'Down': 0} for cid in range(K)}\n",
    "                tot_cnt  = Counter()\n",
    "                for inst in range(n_inst):\n",
    "                    series = test_data[inst]\n",
    "                    for t in range(WIN, n_test):\n",
    "                        win_past = series[t - WIN : t]\n",
    "                        idx = find_pips(win_past, PIPS - 1, DIST_MEASURE)\n",
    "                        if len(idx) < 2:\n",
    "                            continue\n",
    "                        vals = win_past[idx]\n",
    "                        mn, mx = vals.min(), vals.max()\n",
    "                        vec = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "                        cid = kmeans.predict([vec])[0]\n",
    "                        dist = np.linalg.norm(vec - centers[cid][:PIPS-1])\n",
    "                        if dist > TH:\n",
    "                            continue\n",
    "                        tot_cnt[cid] += 1\n",
    "                        mv = 'Up' if series[t] > series[t-1] else 'Down'\n",
    "                        move_cnt[cid][mv] += 1\n",
    "\n",
    "                # --- Record edges per cluster ---\n",
    "                best_edge_combo = 0\n",
    "                for cid in range(K):\n",
    "                    up   = move_cnt[cid]['Up']\n",
    "                    down = move_cnt[cid]['Down']\n",
    "                    tot  = up + down\n",
    "                    if tot == 0:\n",
    "                        continue\n",
    "                    edge = abs(up - down) / tot\n",
    "                    if edge > best_edge_combo:\n",
    "                        best_edge_combo = edge\n",
    "                    results.append((edge, tot, WIN, PIPS, K, TH, cid))\n",
    "                print(f\"  ↳ finished. Best edge in this combo = {best_edge_combo:.3f} (across its clusters)\")\n",
    "\n",
    "# sort by edge, then by total count (frequency)\n",
    "results.sort(key=lambda x: (-x[0], -x[1]))\n",
    "print(\"\\n=========== TOP 20 PARAMETER + CLUSTER COMBINATIONS ===========\")\n",
    "print(\"edge\\ttotal\\twindow\\tpips\\tK\\tthreshold\\tcid\")\n",
    "for edge, tot, win, pips, k, th, cid in results[:192]:\n",
    "    print(f\"{edge:.3f}\\t{tot}\\t{win}\\t{pips}\\t{k}\\t{th}\\t{cid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include random seed in Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance),\n",
    "                  2=Perpendicular distance to chord,\n",
    "                  3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i -\n",
    "                              (x2 - x1) * data[i] +\n",
    "                              x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int):\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Configuration ───────────────────────────────────────────────────────\n",
    "    PRICE_FILE       = '../../prices.txt'  # (750 rows × 50 cols, whitespace-delimited)\n",
    "    WINDOW_SIZE      = 10                  # sliding window length\n",
    "    N_PIPS           = 7                   # total PIPs (we use N_PIPS-1 for features)\n",
    "    DIST_MEASURE     = 2                   # distance measure for PIPs\n",
    "    N_CLUSTERS       = 12                  # number of clusters to form\n",
    "    CLUSTER_THRESHOLD= 0.25                # max distance to centroid for assignment\n",
    "\n",
    "    # ─── Load & pre-process ──────────────────────────────────────────────────\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    log_prices = np.log(df.values.T)       # shape: (n_inst, 750)\n",
    "    n_inst, n_t = log_prices.shape\n",
    "\n",
    "    # split into train/test\n",
    "    n_train   = 500\n",
    "    train_data= log_prices[:, :n_train]\n",
    "    test_data = log_prices[:, n_train:]\n",
    "    _, n_test = test_data.shape\n",
    "\n",
    "    # ─── 1) Train clusters on first 500 timesteps ───────────────────────────\n",
    "    train_patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = train_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            window = series[t - WINDOW_SIZE : t]\n",
    "            pips_idx = find_pips(window, N_PIPS - 1, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            train_patterns[tuple(np.round(norm, 3))] += 1\n",
    "\n",
    "    train_vecs = list(train_patterns.keys())\n",
    "    kmeans, _, _ = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # ─── 2) Evaluate on test set (bars 501–750) ─────────────────────────────\n",
    "    cluster_counts   = Counter()\n",
    "    move_counts      = Counter({cid: {'Up': 0, 'Down': 0}\n",
    "                                for cid in range(N_CLUSTERS)})\n",
    "    cluster_examples = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_test):\n",
    "            window = series[t - WINDOW_SIZE : t]\n",
    "            pips_idx = find_pips(window, N_PIPS - 1, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "\n",
    "            lbl  = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # record\n",
    "            cluster_counts[lbl] += 1\n",
    "            cluster_examples[lbl].append(norm)\n",
    "\n",
    "            # move Up/Down\n",
    "            prev_p = series[t - 1]\n",
    "            next_p = series[t]\n",
    "            move = 'Up' if next_p > prev_p else 'Down'\n",
    "            move_counts[lbl][move] += 1\n",
    "\n",
    "    # ─── 3) Print summary ────────────────────────────────────────────────────\n",
    "    print(\"\\nTest Set: Cluster ID -> Frequency\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    print(\"\\nTest Set: Cluster ID -> Move Distribution\")\n",
    "    for cid in sorted(move_counts):\n",
    "        up   = move_counts[cid]['Up']\n",
    "        down = move_counts[cid]['Down']\n",
    "        if up + down:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # ─── 4) Plot each cluster’s overlaid patterns + centroid ────────────────\n",
    "    for cid, patterns in cluster_examples.items():\n",
    "        if not patterns:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        # all member patterns\n",
    "        for pat in patterns:\n",
    "            plt.plot(range(len(pat)), pat, alpha=0.2)\n",
    "\n",
    "        # bold centroid\n",
    "        plt.plot(range(len(centers[cid])),\n",
    "                 centers[cid],\n",
    "                 color='black',\n",
    "                 linewidth=2,\n",
    "                 label=f\"Centroid {cid}\")\n",
    "\n",
    "        plt.title(f\"Test Set Cluster {cid} (n={len(patterns)})\")\n",
    "        plt.xlabel(\"PIP Index\")\n",
    "        plt.ylabel(\"Normalized Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # … your existing configuration & training code …\n",
    "\n",
    "    # ─── 2) Evaluate on test set (bars 501–750) ─────────────────────────────\n",
    "    cluster_counts   = Counter()\n",
    "    move_counts      = Counter({cid: {'Up': 0, 'Down': 0}\n",
    "                                for cid in range(N_CLUSTERS)})\n",
    "    # now each example will be length (N_PIPS-1)+1\n",
    "    cluster_examples = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_test):\n",
    "            window = series[t - WINDOW_SIZE : t]\n",
    "            pips_idx = find_pips(window, N_PIPS - 1, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "\n",
    "            lbl  = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # record frequency\n",
    "            cluster_counts[lbl] += 1\n",
    "\n",
    "            # ** new: normalize the *next* bar and append **\n",
    "            next_price = series[t]\n",
    "            next_norm  = (next_price - mn) / (mx - mn) if mx != mn else 0.0\n",
    "            pattern_with_next = np.concatenate([norm, [next_norm]])\n",
    "            cluster_examples[lbl].append(pattern_with_next)\n",
    "\n",
    "            # record Up/Down\n",
    "            move = 'Up' if next_price > series[t-1] else 'Down'\n",
    "            move_counts[lbl][move] += 1\n",
    "\n",
    "    # ─── 4) Plot each cluster’s overlaid patterns + centroid ────────────────\n",
    "    for cid, patterns in cluster_examples.items():\n",
    "        if not patterns:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        # plot every window+next\n",
    "        for pat in patterns:\n",
    "            plt.plot(range(len(pat)), pat, alpha=0.2)\n",
    "\n",
    "        # plot **only** the original centroid over the first N_PIPS-1 points\n",
    "        plt.plot(range(len(centers[cid])),\n",
    "                 centers[cid],\n",
    "                 color='black',\n",
    "                 linewidth=2,\n",
    "                 label=f\"Centroid {cid}\")\n",
    "\n",
    "        # mark the “next bar” vertical for clarity\n",
    "        plt.axvline(len(centers[cid]) - 0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "        plt.title(f\"Test Set Cluster {cid} (n={len(patterns)})\")\n",
    "        plt.xlabel(\"Index (0…PIPs-2 are PIPs, last point is next bar)\")\n",
    "        plt.ylabel(\"Normalized Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # ─── Configuration ───────────────────────────────────────────────────────\n",
    "    PRICE_FILE        = '../../prices.txt'\n",
    "    WINDOW_SIZE       = 10\n",
    "    N_PIPS            = 7\n",
    "    DIST_MEASURE      = 2\n",
    "    N_CLUSTERS        = 12\n",
    "    CLUSTER_THRESHOLD = 0.25\n",
    "\n",
    "    # ─── Load & pre-process ──────────────────────────────────────────────────\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    log_prices = np.log(df.values.T)       # shape: (n_inst, 750)\n",
    "    n_inst, n_t = log_prices.shape\n",
    "\n",
    "    n_train    = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data  = log_prices[:, n_train:]\n",
    "    _, n_test  = test_data.shape\n",
    "\n",
    "    # ─── 1) Train clusters on first 500 timesteps ───────────────────────────\n",
    "    train_patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = train_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            window   = series[t - WINDOW_SIZE : t]\n",
    "            pips_idx = find_pips(window, N_PIPS - 1, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            train_patterns[tuple(np.round(norm, 3))] += 1\n",
    "\n",
    "    train_vecs = list(train_patterns.keys())\n",
    "    kmeans, _, _ = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # ─── 2) Single pass over test set (bars 501–750) ────────────────────────\n",
    "    cluster_counts   = Counter()\n",
    "    move_counts      = Counter({cid: {'Up': 0, 'Down': 0}\n",
    "                                for cid in range(N_CLUSTERS)})\n",
    "    cluster_returns  = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "    cluster_examples = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_test):\n",
    "            window   = series[t - WINDOW_SIZE : t]\n",
    "            pips_idx = find_pips(window, N_PIPS - 1, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "\n",
    "            lbl  = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # 2a) record counts & raw return\n",
    "            cluster_counts[lbl] += 1\n",
    "            ret = (series[t] - series[t-1]) / series[t-1]\n",
    "            cluster_returns[lbl].append(ret)\n",
    "            move = 'Up' if ret > 0 else 'Down'\n",
    "            move_counts[lbl][move] += 1\n",
    "\n",
    "            # 2b) store the normalized pattern _plus_ next bar\n",
    "            next_norm = (series[t] - mn) / (mx - mn) if mx != mn else 0.0\n",
    "            pattern_with_next = np.concatenate([norm, [next_norm]])\n",
    "            cluster_examples[lbl].append(pattern_with_next)\n",
    "\n",
    "    # ─── 3) Print frequency & move distributions ───────────────────────────\n",
    "    print(\"\\nTest Set: Cluster ID → Frequency\")\n",
    "    for cid, freq in cluster_counts.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    print(\"\\nTest Set: Cluster ID → Move Distribution\")\n",
    "    for cid in sorted(move_counts):\n",
    "        up, down = move_counts[cid]['Up'], move_counts[cid]['Down']\n",
    "        if up + down:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # ─── 4) Compute p_up, E[up], E[down], expectancy ───────────────────────\n",
    "    cluster_stats = {}\n",
    "    for cid, rets in cluster_returns.items():\n",
    "        if not rets:\n",
    "            continue\n",
    "        rets    = np.array(rets)\n",
    "        mask_up = rets > 0\n",
    "        p_up    = mask_up.mean()\n",
    "        e_up    = rets[mask_up].mean()  if mask_up.any()  else 0.0\n",
    "        e_down  = rets[~mask_up].mean() if (~mask_up).any() else 0.0\n",
    "        expct   = p_up*e_up + (1-p_up)*e_down\n",
    "\n",
    "        cluster_stats[cid] = {\n",
    "            'count'     : len(rets),\n",
    "            'p_up'      : p_up,\n",
    "            'e_up'      : e_up,\n",
    "            'e_down'    : e_down,\n",
    "            'expectancy': expct\n",
    "        }\n",
    "\n",
    "    print(\"\\nCluster  Count   P(up)    E[up]    E[down]   Expectancy\")\n",
    "    for cid in sorted(cluster_stats):\n",
    "        s = cluster_stats[cid]\n",
    "        print(f\"{cid:>3d}    {s['count']:>4d}   \"\n",
    "              f\"{s['p_up']:.2f}   \"\n",
    "              f\"{s['e_up']:.4f}   \"\n",
    "              f\"{s['e_down']:.4f}   \"\n",
    "              f\"{s['expectancy']:.4f}\")\n",
    "\n",
    "    # ─── 5) Select tradeable clusters & plot their patterns ────────────────\n",
    "    tradeable = [cid for cid,s in cluster_stats.items() if s['expectancy'] > 0]\n",
    "    print(f\"\\nTradeable clusters (expectancy>0): {tradeable}\")\n",
    "\n",
    "    for cid in tradeable:\n",
    "        patterns = cluster_examples[cid]\n",
    "        if not patterns:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        # overlay every historical+next pattern\n",
    "        for pat in patterns:\n",
    "            plt.plot(range(len(pat)), pat, alpha=0.2)\n",
    "\n",
    "        # centroid (only the PIP‐portion)\n",
    "        plt.plot(range(len(centers[cid])),\n",
    "                 centers[cid],\n",
    "                 color='black',\n",
    "                 linewidth=2,\n",
    "                 label=f\"Centroid {cid}\")\n",
    "\n",
    "        # divider between PIP and next bar\n",
    "        plt.axvline(len(centers[cid]) - 0.5,\n",
    "                    color='gray',\n",
    "                    linestyle='--',\n",
    "                    alpha=0.5)\n",
    "\n",
    "        plt.title(f\"Tradeable Cluster {cid} (n={len(patterns)})\")\n",
    "        plt.xlabel(\"Index (0…PIPs-2 are PIPs, last is next bar)\")\n",
    "        plt.ylabel(\"Normalized Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
