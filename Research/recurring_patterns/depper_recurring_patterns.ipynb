{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance),\n",
    "                  2=Perpendicular distance to chord,\n",
    "                  3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i -\n",
    "                              (x2 - x1) * data[i] +\n",
    "                              x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int):\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Configuration ───────────────────────────────────────────────────────\n",
    "    PRICE_FILE       = '../../prices.txt'  # (750 rows × 50 cols)\n",
    "    WINDOW_SIZE      = 24\n",
    "    N_PIPS           = 5\n",
    "    DIST_MEASURE     = 2\n",
    "    N_CLUSTERS       = 10\n",
    "    CLUSTER_THRESHOLD= 0.30\n",
    "\n",
    "    # ─── Load & pre-process ──────────────────────────────────────────────────\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    log_prices = np.log(df.values.T)       # shape (50, 750)\n",
    "    n_inst, n_t = log_prices.shape\n",
    "\n",
    "    n_train    = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data  = log_prices[:, n_train:]\n",
    "    _, n_test  = test_data.shape\n",
    "\n",
    "    # ─── 1) Train clusters on first 500 timesteps ───────────────────────────\n",
    "    train_patterns = extract_pip_patterns(train_data, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    train_vecs     = list(train_patterns.keys())\n",
    "    kmeans, _, _   = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers        = kmeans.cluster_centers_\n",
    "\n",
    "    # ─── 2) Single pass over test set ────────────────────────────────────────\n",
    "    cluster_counts_test   = Counter()\n",
    "    move_counts_test      = Counter({cid: {'Up': 0, 'Down': 0}\n",
    "                                     for cid in range(N_CLUSTERS)})\n",
    "    cluster_patterns_test = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "    matched_ends_inst0    = []\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for end in range(WINDOW_SIZE, n_test):\n",
    "            window   = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "\n",
    "            lbl  = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            cluster_counts_test[lbl] += 1\n",
    "            cluster_patterns_test[lbl].append(tuple(np.round(norm, 3)))\n",
    "\n",
    "            move = 'Up' if vals[-1] - vals[-2] > 0 else 'Down'\n",
    "            move_counts_test[lbl][move] += 1\n",
    "\n",
    "            if inst == 0:\n",
    "                matched_ends_inst0.append(end)\n",
    "\n",
    "    # ─── 3) Print test-set cluster results ──────────────────────────────────\n",
    "    print(\"\\nTest Set: Cluster ID → Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts_test.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    print(\"\\nTest Set: Cluster ID → Last-PIP Move Distribution\")\n",
    "    for cid, counts in move_counts_test.items():\n",
    "        up, down = counts['Up'], counts['Down']\n",
    "        if up + down > 0:\n",
    "            print(f\"Cluster {cid}: Up = {up}, Down = {down}\")\n",
    "\n",
    "    # ─── 4) Plot each test-set cluster patterns + centroid ──────────────────\n",
    "    x = np.arange(N_PIPS)\n",
    "    for cid, pats in cluster_patterns_test.items():\n",
    "        if not pats:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for pat in pats:\n",
    "            plt.plot(x, pat, alpha=0.1)\n",
    "        plt.plot(x, centers[cid], color='black', linewidth=2, label=f'Centroid {cid}')\n",
    "        plt.title(f'Test Set Cluster {cid} (n={cluster_counts_test[cid]})')\n",
    "        plt.xlabel('PIP Index')\n",
    "        plt.ylabel('Normalized Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ─── 5) Plot instrument 0 price with shaded pattern windows ─────────────\n",
    "    times = np.arange(n_train, n_train + n_test)\n",
    "    prices0 = np.exp(test_data[0])\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(times, prices0, label='Instrument 0 Price')\n",
    "    for end in matched_ends_inst0:\n",
    "        start = n_train + end - WINDOW_SIZE\n",
    "        stop  = n_train + end\n",
    "        plt.axvspan(start, stop, color='orange', alpha=0.1)\n",
    "    plt.title('Instrument 0 Test-Set Price with Pattern Matches')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ─── 6) EMA crossover rule & prediction vs actual ───────────────────────\n",
    "    FAST_EMA = 12\n",
    "    SLOW_EMA = 26\n",
    "\n",
    "    ema_preds   = []\n",
    "    ema_actuals = []\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        price_series = pd.Series(np.exp(test_data[inst]),\n",
    "                                 index=np.arange(n_train, n_train + n_test))\n",
    "        ema_f = price_series.ewm(span=FAST_EMA, adjust=False).mean()\n",
    "        ema_s = price_series.ewm(span=SLOW_EMA, adjust=False).mean()\n",
    "\n",
    "        start_t = max(FAST_EMA, SLOW_EMA) + n_train\n",
    "        for t in range(start_t, n_train + n_test):\n",
    "            pred   = 'Up'   if ema_f.loc[t-1] > ema_s.loc[t-1] else 'Down'\n",
    "            actual = 'Up'   if price_series.loc[t] > price_series.loc[t-1] else 'Down'\n",
    "            ema_preds.append(pred)\n",
    "            ema_actuals.append(actual)\n",
    "\n",
    "    ema_counts = Counter(zip(ema_preds, ema_actuals))\n",
    "    print(\"\\nEMA Crossover Predictions vs Actual Moves\")\n",
    "    for (p, a), cnt in ema_counts.items():\n",
    "        print(f\"Pred {p}, Actual {a}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1+1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2)/2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1)*i - (x2 - x1)*data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num/den if den else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1)*(i - x1)/(x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int):\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Config ───────────────────────────────────────────────────────────────\n",
    "    PRICE_FILE       = '../../prices.txt'  # (750 × 50)\n",
    "    WINDOW_SIZE      = 24\n",
    "    N_PIPS           = 5                   # total PIPs  → we'll use 4 for clustering\n",
    "    DIST_MEASURE     = 2\n",
    "    N_CLUSTERS       = 10\n",
    "    CLUSTER_THRESHOLD= 0.30\n",
    "    FAST_EMA, SLOW_EMA = 12, 26\n",
    "\n",
    "    # ─── Load & prep ─────────────────────────────────────────────────────────\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    logp = np.log(df.values.T)              # shape (50, 750)\n",
    "    n_inst, n_t = logp.shape\n",
    "\n",
    "    # train/test split\n",
    "    n_train    = 500\n",
    "    train_data = logp[:, :n_train]\n",
    "    test_data  = logp[:, n_train:]\n",
    "    _, n_test  = test_data.shape\n",
    "\n",
    "    # ─── 1) Build cluster centroids on first N_PIPS–1 PIPs ───────────────────\n",
    "    train_vecs = []\n",
    "    for inst in range(n_inst):\n",
    "        series = train_data[inst]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            window   = series[t-WINDOW_SIZE : t]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < N_PIPS:\n",
    "                continue\n",
    "            # use only first N_PIPS-1 indices as features\n",
    "            feats_idx = pips_idx[:-1]\n",
    "            vals = window[feats_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx - mn) if mx!=mn else np.zeros_like(vals)\n",
    "            train_vecs.append(norm)\n",
    "\n",
    "    kmeans = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers= kmeans.cluster_centers_\n",
    "\n",
    "    # ─── 2) In-test evaluation: cluster on first N_PIPS-1, predict next Nth pip via EMA ─\n",
    "    ema_confusion = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        # build a plain price series for EMA\n",
    "        price_ser = pd.Series(np.exp(series), index=np.arange(n_train, n_train+n_test))\n",
    "        ema_f = price_ser.ewm(span=FAST_EMA, adjust=False).mean()\n",
    "        ema_s = price_ser.ewm(span=SLOW_EMA, adjust=False).mean()\n",
    "\n",
    "        # slide windows\n",
    "        for end in range(WINDOW_SIZE, n_test):\n",
    "            window   = series[end-WINDOW_SIZE : end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            # features = first N_PIPS-1 PIPs\n",
    "            feats_idx = pips_idx[:-1]\n",
    "            feat_vals = window[feats_idx]\n",
    "            mn, mx    = feat_vals.min(), feat_vals.max()\n",
    "            feat_norm = (feat_vals - mn)/(mx - mn) if mx!=mn else np.zeros_like(feat_vals)\n",
    "\n",
    "            # assign cluster\n",
    "            lbl = kmeans.predict([feat_norm])[0]\n",
    "            dist= np.linalg.norm(feat_norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # EMA‐based prediction at bar t–1\n",
    "            t_global = n_train + end - 1\n",
    "            ema_pred = 'Up' if ema_f.loc[t_global] > ema_s.loc[t_global] else 'Down'\n",
    "\n",
    "            # actual move of the Nth pip inside the window\n",
    "            prev_val = window[feats_idx[-1]]\n",
    "            next_val = window[pips_idx[-1]]\n",
    "            actual   = 'Up' if next_val > prev_val else 'Down'\n",
    "\n",
    "            print(f\"Cluster {lbl}, EMA_pred {ema_pred}, Actual {actual}\")\n",
    "            ema_confusion[(ema_pred, actual)] += 1\n",
    "\n",
    "    # ─── 3) Print EMA confusion matrix ─────────────────────────────────────────\n",
    "    print(\"\\nEMA Crossover vs Next-PIP Move\")\n",
    "    total = sum(ema_confusion.values())\n",
    "    for (pred, act), cnt in ema_confusion.items():\n",
    "        print(f\"Pred {pred}, Actual {act}: {cnt}\")\n",
    "    print(f\"Overall accuracy: {(ema_confusion[('Up','Up')] + ema_confusion[('Down','Down')]) / total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"Extract n_pips perceptually important points (PIPs) from a 1D series.\"\"\"\n",
    "    if n_pips < 2: return []\n",
    "    pips_x = [0, len(data)-1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist, max_idx = -1.0, None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1,y1),(x2,y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1+1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1+x2)/2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2-y1)*i - (x2-x1)*data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2-y1, x2-x1)\n",
    "                    dist = num/den if den else 0\n",
    "                else:\n",
    "                    interp = y1 + (y2-y1)*(i-x1)/(x2-x1)\n",
    "                    dist = abs(data[i]-interp)\n",
    "                if dist > max_dist:\n",
    "                    max_dist, max_idx = dist, i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def compute_indicators(price: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a wide DataFrame of technicals from a close‐price series.\n",
    "    Drops any lookbacks >100 and fills NaNs nicely.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=price.index)\n",
    "    # ── SMAs & EMAs ─────────────────────\n",
    "    sma_windows = [10, 12, 20, 25, 26, 50, 100]\n",
    "    ema_windows = [12, 26, 50]\n",
    "    for w in sma_windows:\n",
    "        df[f\"SMA_{w}\"] = price.rolling(w, min_periods=1).mean()\n",
    "    for w in ema_windows:\n",
    "        df[f\"EMA_{w}\"] = price.ewm(span=w, adjust=False).mean()\n",
    "\n",
    "    # ── price–MA diffs & MA–MA diffs ────\n",
    "    for w in [12, 26, 50, 100]:\n",
    "        df[f\"PR_MA_{w}\"] = price - df[f\"SMA_{w}\"]\n",
    "    df[\"MA_DIFF_25_100\"] = df[\"SMA_25\"] - df[\"SMA_100\"]\n",
    "    df[\"MA_DIFF_12_26\"]  = df[\"SMA_12\"] - df[\"SMA_26\"]\n",
    "\n",
    "    # ── RSI ─────────────────────────────\n",
    "    for w in [6, 9, 14, 21]:\n",
    "        delta = price.diff()\n",
    "        up    = delta.clip(lower=0)\n",
    "        down  = -delta.clip(upper=0)\n",
    "        ma_up   = up.rolling(w, min_periods=1).mean()\n",
    "        ma_down = down.rolling(w, min_periods=1).mean()\n",
    "        rs = ma_up / ma_down.replace(0, np.nan)\n",
    "        df[f\"RSI_{w}\"] = 100 - (100/(1+rs))\n",
    "\n",
    "    # ── SLOPE ───────────────────────────\n",
    "    def _slope(y):\n",
    "        t = np.arange(len(y))\n",
    "        m, _ = np.linalg.lstsq(np.vstack([t, np.ones_like(t)]).T, y, rcond=None)[0]\n",
    "        return m\n",
    "    for w in [10, 50, 75, 100]:\n",
    "        df[f\"SLOPE_{w}\"] = price.rolling(w, min_periods=w).apply(_slope, raw=True)\n",
    "\n",
    "    # ── ROC, STD, ZSCORE, PCTL ──────────\n",
    "    for w in [5, 10, 20, 50]:\n",
    "        df[f\"ROC_{w}\"] = price.pct_change(w)\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"STD_{w}\"] = price.rolling(w, min_periods=1).std()\n",
    "    for w in [20, 50]:\n",
    "        mu = price.rolling(w, min_periods=1).mean()\n",
    "        sd = price.rolling(w, min_periods=1).std().replace(0, np.nan)\n",
    "        df[f\"Z_{w}\"] = (price - mu) / sd\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"PCTL_{w}\"] = price.rolling(w, min_periods=1) \\\n",
    "                         .apply(lambda x: (x.iloc[-1]-x.min())/(x.max()-x.min()) \n",
    "                                if x.max()!=x.min() else 0, raw=False)\n",
    "\n",
    "    # ── UP/DOWN STREAK ───────────────────\n",
    "    dif = price.diff()\n",
    "    streak = [0]\n",
    "    for i in range(1, len(price)):\n",
    "        if dif.iat[i] > 0 and dif.iat[i-1] > 0:\n",
    "            streak.append(streak[-1]+1)\n",
    "        elif dif.iat[i] < 0 and dif.iat[i-1] < 0:\n",
    "            streak.append(streak[-1]-1)\n",
    "        else:\n",
    "            streak.append(1 if dif.iat[i]>0 else -1 if dif.iat[i]<0 else 0)\n",
    "    df[\"STREAK\"] = streak\n",
    "\n",
    "    # ── MACD ────────────────────────────\n",
    "    macd_line   = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
    "    macd_signal = macd_line.ewm(span=9, adjust=False).mean()\n",
    "    df[\"MACD\"]      = macd_line\n",
    "    df[\"MACD_SIG\"]  = macd_signal\n",
    "    df[\"MACD_HIST\"] = macd_line - macd_signal\n",
    "\n",
    "    # ── PIVOTS & BROKEN PIVOTS ─────────\n",
    "    for w in [6, 10, 50, 100]:\n",
    "        df[f\"PIVOT_H_{w}\"] = price.rolling(w, min_periods=1).max().shift(1)\n",
    "        df[f\"PIVOT_L_{w}\"] = price.rolling(w, min_periods=1).min().shift(1)\n",
    "        df[f\"BRK_H_{w}\"]   = (price > df[f\"PIVOT_H_{w}\"]).astype(int)\n",
    "        df[f\"BRK_L_{w}\"]   = (price < df[f\"PIVOT_L_{w}\"]).astype(int)\n",
    "\n",
    "    # ── CLEANUP ─────────────────────────\n",
    "    # Backfill/forward‐fill and finally fill zeros for any stragglers\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Config ───────────────────────────────────────────────────────────────\n",
    "    PRICE_FILE        = '../../prices.txt'\n",
    "    WINDOW_SIZE       = 24\n",
    "    N_PIPS            = 5\n",
    "    DIST_MEASURE      = 2\n",
    "    N_CLUSTERS        = 10\n",
    "    CLUSTER_THRESHOLD = 0.30\n",
    "    FAST_EMA, SLOW_EMA= 12, 26\n",
    "    MIN_SAMPLES_LEAF  = 10\n",
    "    MAX_TREE_DEPTH    = 2\n",
    "\n",
    "    # ─── Load raw close data ─────────────────────────────────────────────────\n",
    "    raw = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    raw.columns = [f\"I{i}\" for i in raw.columns]\n",
    "    T, n_inst = raw.shape      # T=750, n_inst=50\n",
    "    price_df   = raw.copy()\n",
    "\n",
    "    # ─── Precompute all indicators per instrument ────────────────────────────\n",
    "    print(\"🔧 Computing technical indicators…\")\n",
    "    feats_by_inst = {}\n",
    "    for inst in price_df.columns:\n",
    "        feats_by_inst[inst] = compute_indicators(price_df[inst])\n",
    "    print(\" → done.\\n\")\n",
    "\n",
    "    # ─── Split into train (0–499) & test (500–749) ──────────────────────────\n",
    "    n_train   = 500\n",
    "    train_df  = np.log(price_df.iloc[:n_train].values).T    # shape (50,500)\n",
    "    test_df   = np.log(price_df.iloc[n_train:].values).T    # shape (50,250)\n",
    "\n",
    "    # ─── 1) Build PIP‐vectors & cluster on train set ─────────────────────────\n",
    "    print(\"🔧 Extracting PIP‐patterns from training set…\")\n",
    "    train_vecs = []\n",
    "    for inst_idx, inst in enumerate(price_df.columns):\n",
    "        series = train_df[inst_idx]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            win = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "            vals = win[pips[:-1]]   # first N_PIPS-1 points\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            train_vecs.append(norm)\n",
    "    print(f\" → collected {len(train_vecs)} windows\\n\")\n",
    "\n",
    "    print(\"🔧 Fitting KMeans…\")\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0)\n",
    "    kmeans.fit(train_vecs)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # quick bar‐chart of cluster sizes\n",
    "    ct = Counter(kmeans.labels_)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(ct.keys(), ct.values(), color='C0', alpha=0.7)\n",
    "    plt.title(\"Train‐set PIP‐windows per cluster\")\n",
    "    plt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ─── 2) Gather per‐cluster training rows for Decision Trees ──────────────\n",
    "    print(\"\\n🔧 Building per‐cluster training datasets…\")\n",
    "    X_by_cluster = defaultdict(list)\n",
    "    y_by_cluster = defaultdict(list)\n",
    "\n",
    "    # precompute EMAs on real price for train\n",
    "    price_train_df = price_df.iloc[:n_train]\n",
    "    ema_f_train = price_train_df.ewm(span=FAST_EMA, adjust=False).mean()\n",
    "    ema_s_train = price_train_df.ewm(span=SLOW_EMA, adjust=False).mean()\n",
    "\n",
    "    for inst_idx, inst_col in enumerate(price_df.columns):\n",
    "        log_series = train_df[inst_idx]\n",
    "        for t in range(WINDOW_SIZE, n_train-1):\n",
    "            win = log_series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            # 1) cluster\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            lbl    = kmeans.predict([norm])[0]\n",
    "            if np.linalg.norm(norm - centers[lbl]) > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # 2) features = precomputed at t-1\n",
    "            feats = feats_by_inst[inst_col].iloc[t-1].values\n",
    "\n",
    "            # 3) label = next‐bar Up/Down\n",
    "            actual = 1 if log_series[t] > log_series[t-1] else 0\n",
    "\n",
    "            X_by_cluster[lbl].append(feats)\n",
    "            y_by_cluster[lbl].append(actual)\n",
    "\n",
    "    # report cluster sizes\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        print(f\" Cluster {cid:>2d}: {len(y_by_cluster[cid])} samples\")\n",
    "\n",
    "    # ─── 3) Train a tiny Decision Tree per cluster ──────────────────────────\n",
    "    print(\"\\n🔧 Training Decision Trees per cluster…\")\n",
    "    models = {}\n",
    "    for cid, Xs in X_by_cluster.items():\n",
    "        Ys = y_by_cluster[cid]\n",
    "        if len(Ys) < MIN_SAMPLES_LEAF*2:\n",
    "            print(f\"  · skipping cluster {cid} (only {len(Ys)} samples)\")\n",
    "            continue\n",
    "        tree = DecisionTreeClassifier(\n",
    "            max_depth=MAX_TREE_DEPTH,\n",
    "            min_samples_leaf=MIN_SAMPLES_LEAF,\n",
    "            random_state=0\n",
    "        )\n",
    "        tree.fit(Xs, Ys)\n",
    "        models[cid] = tree\n",
    "        print(f\"  · cluster {cid}: trained tree ({len(Ys)} samples)\")\n",
    "        print(export_text(tree, feature_names=list(feats_by_inst[inst_col].columns)))\n",
    "\n",
    "    # ─── 4) Inference on the 250‐bar test set ───────────────────────────────\n",
    "    print(\"\\n🔧 Running test‐set inference…\")\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    price_test_df = price_df.iloc[n_train:]\n",
    "    ema_f_test = price_test_df.ewm(span=FAST_EMA, adjust=False).mean()\n",
    "    ema_s_test = price_test_df.ewm(span=SLOW_EMA, adjust=False).mean()\n",
    "\n",
    "    for inst_idx, inst_col in enumerate(price_df.columns):\n",
    "        log_series = test_df[inst_idx]\n",
    "        for t in range(WINDOW_SIZE, n_test-1):\n",
    "            win = log_series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            lbl    = kmeans.predict([norm])[0]\n",
    "            if (np.linalg.norm(norm - centers[lbl]) > CLUSTER_THRESHOLD\n",
    "                    or lbl not in models):\n",
    "                continue\n",
    "\n",
    "            feats = feats_by_inst[inst_col].iloc[n_train + t - 1].values\n",
    "            pred  = models[lbl].predict([feats])[0]\n",
    "            actual= 1 if log_series[t] > log_series[t-1] else 0\n",
    "\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(actual)\n",
    "            print(f\"[{inst_col} @ t={n_train+t}] Cluster {lbl} → \"\n",
    "                  f\"Pred={'Up' if pred else 'Down'} / \"\n",
    "                  f\"Actual={'Up' if actual else 'Down'}\")\n",
    "\n",
    "    # ─── 5) Summarize performance ───────────────────────────────────────────\n",
    "    print(\"\\n🎯 Final Test‐set Confusion Matrix\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Down\",\"Up\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.show()\n",
    "\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"Overall accuracy: {acc:.2%} ({len(y_true)} calls)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"Extract n_pips perceptually important points (PIPs) from a 1D series.\"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist, max_idx = -1.0, None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i \n",
    "                              - (x2 - x1) * data[i] \n",
    "                              + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den else 0\n",
    "                else:\n",
    "                    # vertical distance to the chord\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                if dist > max_dist:\n",
    "                    max_dist, max_idx = dist, i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def compute_indicators(price: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a wide DataFrame of technicals from a close‐price series.\n",
    "    Drops lookbacks > 100, handles NaNs by back/forward‐fill then zero.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=price.index)\n",
    "    # 1) SMAs & EMAs\n",
    "    sma_ws = [10, 12, 20, 25, 26, 50, 100]\n",
    "    ema_ws = [12, 26, 50]\n",
    "    for w in sma_ws:\n",
    "        df[f\"SMA_{w}\"] = price.rolling(w, min_periods=1).mean()\n",
    "    for w in ema_ws:\n",
    "        df[f\"EMA_{w}\"] = price.ewm(span=w, adjust=False).mean()\n",
    "\n",
    "    # 2) Price–MA & MA–MA diffs\n",
    "    for w in [12, 26, 50, 100]:\n",
    "        df[f\"PR_MA_{w}\"] = price - df[f\"SMA_{w}\"]\n",
    "    df[\"MA_DIFF_25_100\"] = df[\"SMA_25\"] - df[\"SMA_100\"]\n",
    "    df[\"MA_DIFF_12_26\"]  = df[\"SMA_12\"] - df[\"SMA_26\"]\n",
    "\n",
    "    # 3) RSI\n",
    "    for w in [6, 9, 14, 21]:\n",
    "        delta = price.diff()\n",
    "        up    = delta.clip(lower=0).rolling(w, min_periods=1).mean()\n",
    "        down  = (-delta.clip(upper=0)).rolling(w, min_periods=1).mean()\n",
    "        rs    = up / down.replace(0, np.nan)\n",
    "        df[f\"RSI_{w}\"] = 100 - 100 / (1 + rs)\n",
    "\n",
    "    # 4) SLOPE (OLS) for windows up to 100\n",
    "    def _slope(arr):\n",
    "        t = np.arange(len(arr))\n",
    "        m, _ = np.linalg.lstsq(np.vstack([t, np.ones_like(t)]).T,\n",
    "                               arr, rcond=None)[0]\n",
    "        return m\n",
    "    for w in [10, 50, 75, 100]:\n",
    "        df[f\"SLOPE_{w}\"] = price.rolling(w, min_periods=w).apply(\n",
    "            _slope, raw=True)\n",
    "\n",
    "    # 5) ROC, STD, Z, PCTL\n",
    "    for w in [5, 10, 20, 50]:\n",
    "        df[f\"ROC_{w}\"] = price.pct_change(w)\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"STD_{w}\"] = price.rolling(w, min_periods=1).std()\n",
    "    for w in [20, 50]:\n",
    "        mu = price.rolling(w, min_periods=1).mean()\n",
    "        sd = price.rolling(w, min_periods=1).std().replace(0, np.nan)\n",
    "        df[f\"Z_{w}\"] = (price - mu) / sd\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"PCTL_{w}\"] = price.rolling(w, min_periods=1) \\\n",
    "            .apply(lambda x: (x.iloc[-1] - x.min()) /\n",
    "                             (x.max() - x.min()) if x.max()!=x.min() else 0,\n",
    "                   raw=False)\n",
    "\n",
    "    # 6) Up/Down streak\n",
    "    dif = price.diff()\n",
    "    streak = [0]\n",
    "    for i in range(1, len(price)):\n",
    "        if dif.iat[i] > 0 and dif.iat[i-1] > 0:\n",
    "            streak.append(streak[-1] + 1)\n",
    "        elif dif.iat[i] < 0 and dif.iat[i-1] < 0:\n",
    "            streak.append(streak[-1] - 1)\n",
    "        else:\n",
    "            streak.append(1 if dif.iat[i] > 0 else -1 if dif.iat[i] < 0 else 0)\n",
    "    df[\"STREAK\"] = streak\n",
    "\n",
    "    # 7) MACD\n",
    "    macd_line   = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
    "    macd_sig    = macd_line.ewm(span=9, adjust=False).mean()\n",
    "    df[\"MACD\"]      = macd_line\n",
    "    df[\"MACD_SIG\"]  = macd_sig\n",
    "    df[\"MACD_HIST\"] = macd_line - macd_sig\n",
    "\n",
    "    # 8) Pivots + broken pivot flags\n",
    "    for w in [6, 10, 50, 100]:\n",
    "        df[f\"PIVOT_H_{w}\"] = price.rolling(w, min_periods=1).max().shift(1)\n",
    "        df[f\"PIVOT_L_{w}\"] = price.rolling(w, min_periods=1).min().shift(1)\n",
    "        df[f\"BRK_H_{w}\"]   = (price > df[f\"PIVOT_H_{w}\"]).astype(int)\n",
    "        df[f\"BRK_L_{w}\"]   = (price < df[f\"PIVOT_L_{w}\"]).astype(int)\n",
    "\n",
    "    # Fill NaNs\n",
    "    df = df.fillna(method=\"bfill\").fillna(method=\"ffill\").fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Parameters ──────────────────────────────────────────────────────────\n",
    "    PRICE_FILE        = '../../prices.txt'\n",
    "    WINDOW_SIZE       = 24\n",
    "    N_PIPS            = 5\n",
    "    DIST_MEASURE      = 2\n",
    "    N_CLUSTERS        = 10\n",
    "    CLUSTER_THRESHOLD = 0.30\n",
    "    FAST_EMA, SLOW_EMA= 12, 26\n",
    "    GBDT_PARAMS       = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 4,\n",
    "        \"subsample\": 0.7,\n",
    "        \"random_state\": 0\n",
    "    }\n",
    "\n",
    "    # ─── Load closes & compute indicators ────────────────────────────────────\n",
    "    raw = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    raw.columns = [f\"I{i}\" for i in raw.columns]\n",
    "    price_df = raw.copy()                     # shape (750, n_inst)\n",
    "    print(\"Computed indicators… this may take 10–20s\")\n",
    "    feats_by_inst = {\n",
    "        inst: compute_indicators(price_df[inst])\n",
    "        for inst in price_df.columns\n",
    "    }\n",
    "    print(\"→ Done.\\n\")\n",
    "\n",
    "    # ─── Log‐prices & train/test split ───────────────────────────────────────\n",
    "    log_vals = np.log(price_df.values)\n",
    "    n_total, n_inst = log_vals.shape\n",
    "    n_train = 500\n",
    "    train_vals = log_vals[:n_train]           # shape (500, inst)\n",
    "    test_vals  = log_vals[n_train:]           # shape (250, inst)\n",
    "\n",
    "    # transpose for easier indexing: shape → (inst, time)\n",
    "    train_T = train_vals.T\n",
    "    test_T  = test_vals.T\n",
    "\n",
    "    # ─── 1) PIP clustering on train windows ─────────────────────────────────\n",
    "    print(\"Clustering PIP-shapes on first 500 bars…\")\n",
    "    train_patterns = []\n",
    "    for ii in range(n_inst):\n",
    "        series = train_T[ii]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            win  = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            train_patterns.append(norm)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0)\n",
    "    kmeans.fit(train_patterns)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(\"→ KMeans fitted on\", len(train_patterns), \"patterns\\n\")\n",
    "\n",
    "    # plot cluster‐sizes\n",
    "    ct = Counter(kmeans.labels_)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(ct.keys(), ct.values(), alpha=0.7)\n",
    "    plt.title(\"Train-set windows per cluster\")\n",
    "    plt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ─── 2) Build global train set (features + distances → next-bar label) ──\n",
    "    print(\"Building global training set…\")\n",
    "    X_train, y_train = [], []\n",
    "    for ii, inst in enumerate(price_df.columns):\n",
    "        series = train_T[ii]\n",
    "        feats_df = feats_by_inst[inst]\n",
    "        for t in range(WINDOW_SIZE, n_train-1):\n",
    "            win  = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            # 1) cluster distances\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            dists  = np.linalg.norm(centers - norm, axis=1)\n",
    "\n",
    "            # 2) indicators at the last in-window bar = t-1\n",
    "            feats = feats_df.iloc[t-1].values\n",
    "\n",
    "            # 3) label = next-bar up/down\n",
    "            label = 1 if series[t] > series[t-1] else 0\n",
    "\n",
    "            # 4) build row\n",
    "            X_train.append(np.concatenate([feats, dists]))\n",
    "            y_train.append(label)\n",
    "\n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    print(f\"→ Training set size: {X_train.shape[0]} rows, {X_train.shape[1]} features\\n\")\n",
    "\n",
    "    # ─── 3) Fit a global GradientBoostingClassifier ─────────────────────────\n",
    "    print(\"Training GradientBoostingClassifier on global features…\")\n",
    "    model = GradientBoostingClassifier(**GBDT_PARAMS)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"→ Done.\\n\")\n",
    "\n",
    "    # feature importances\n",
    "    fi = model.feature_importances_\n",
    "    # split: first p indicators, last N_CLUSTERS distances\n",
    "    n_feats = X_train.shape[1] - N_CLUSTERS\n",
    "    feat_names = list(feats_by_inst[inst].columns) + [f\"DIST_C{c}\" for c in range(N_CLUSTERS)]\n",
    "    imp_idx = np.argsort(fi)[::-1][:20]  # top 20\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([feat_names[i] for i in imp_idx], fi[imp_idx], color='C1')\n",
    "    plt.xticks(rotation=60, ha='right')\n",
    "    plt.title(\"Top-20 Feature Importances\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ─── 4) Build and evaluate on test set ──────────────────────────────────\n",
    "    print(\"Building test set & running inference…\")\n",
    "    X_test, y_test = [], []\n",
    "    for ii, inst in enumerate(price_df.columns):\n",
    "        series = test_T[ii]\n",
    "        feats_df = feats_by_inst[inst]\n",
    "        for t in range(WINDOW_SIZE, n_total-n_train-1):\n",
    "            win  = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            dists  = np.linalg.norm(centers - norm, axis=1)\n",
    "\n",
    "            feats = feats_df.iloc[n_train + t -1].values\n",
    "            label = 1 if series[t] > series[t-1] else 0\n",
    "\n",
    "            X_test.append(np.concatenate([feats, dists]))\n",
    "            y_test.append(label)\n",
    "\n",
    "    X_test = np.vstack(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    print(f\"→ Test set size: {X_test.shape[0]} rows\\n\")\n",
    "\n",
    "    print(\"Running test predictions…\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=None)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Down\",\"Up\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(\"GBDT Global Model: Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    acc = (y_pred == y_test).mean()\n",
    "    print(f\"Overall test accuracy: {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"Extract n_pips perceptually important points (PIPs) from a 1D series.\"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist, max_idx = -1.0, None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1+1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1+x2)/2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2-y1)*i - (x2-x1)*data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2-y1, x2-x1)\n",
    "                    dist = num/den if den else 0\n",
    "                else:\n",
    "                    interp = y1 + (y2-y1)*(i-x1)/(x2-x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                if dist > max_dist:\n",
    "                    max_dist, max_idx = dist, i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def compute_indicators(price: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build technical indicators from close-only series.\n",
    "    All lookbacks ≤100, NaNs backfilled then forward-filled then zeroed.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=price.index)\n",
    "    # 1) SMAs & EMAs\n",
    "    sma_ws = [10, 12, 20, 25, 26, 50, 100]\n",
    "    ema_ws = [12, 26, 50]\n",
    "    for w in sma_ws:\n",
    "        df[f\"SMA_{w}\"] = price.rolling(w, min_periods=1).mean()\n",
    "    for w in ema_ws:\n",
    "        df[f\"EMA_{w}\"] = price.ewm(span=w, adjust=False).mean()\n",
    "\n",
    "    # 2) Price-MA and MA-MA diffs\n",
    "    for w in [12, 26, 50, 100]:\n",
    "        df[f\"PR_MA_{w}\"] = price - df[f\"SMA_{w}\"]\n",
    "    df[\"MA_DIFF_25_100\"] = df[\"SMA_25\"] - df[\"SMA_100\"]\n",
    "    df[\"MA_DIFF_12_26\"]  = df[\"SMA_12\"] - df[\"SMA_26\"]\n",
    "\n",
    "    # 3) RSI\n",
    "    for w in [6, 9, 14, 21]:\n",
    "        delta = price.diff()\n",
    "        up    = delta.clip(lower=0).rolling(w, min_periods=1).mean()\n",
    "        down  = (-delta.clip(upper=0)).rolling(w, min_periods=1).mean()\n",
    "        rs    = up / down.replace(0, np.nan)\n",
    "        df[f\"RSI_{w}\"] = 100 - 100/(1+rs)\n",
    "\n",
    "    # 4) OLS slope\n",
    "    def _slope(arr):\n",
    "        t = np.arange(len(arr))\n",
    "        m, _ = np.linalg.lstsq(np.vstack([t, np.ones_like(t)]).T, arr,\n",
    "                               rcond=None)[0]\n",
    "        return m\n",
    "    for w in [10, 50, 75, 100]:\n",
    "        df[f\"SLOPE_{w}\"] = price.rolling(w, min_periods=w).apply(_slope, raw=True)\n",
    "\n",
    "    # 5) ROC, STD, Z, PCTL\n",
    "    for w in [5, 10, 20, 50]:\n",
    "        df[f\"ROC_{w}\"] = price.pct_change(w)\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"STD_{w}\"] = price.rolling(w, min_periods=1).std()\n",
    "    for w in [20, 50]:\n",
    "        mu = price.rolling(w, min_periods=1).mean()\n",
    "        sd = price.rolling(w, min_periods=1).std().replace(0, np.nan)\n",
    "        df[f\"Z_{w}\"] = (price - mu)/sd\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"PCTL_{w}\"] = price.rolling(w, min_periods=1) \\\n",
    "            .apply(lambda x: (x.iloc[-1]-x.min())/(x.max()-x.min())\n",
    "                   if x.max()!=x.min() else 0, raw=False)\n",
    "\n",
    "    # 6) Up/Down streak\n",
    "    dif = price.diff()\n",
    "    streak = [0]\n",
    "    for i in range(1, len(price)):\n",
    "        if dif.iat[i] > 0 and dif.iat[i-1] > 0:\n",
    "            streak.append(streak[-1]+1)\n",
    "        elif dif.iat[i] < 0 and dif.iat[i-1] < 0:\n",
    "            streak.append(streak[-1]-1)\n",
    "        else:\n",
    "            streak.append(1 if dif.iat[i]>0 else -1 if dif.iat[i]<0 else 0)\n",
    "    df[\"STREAK\"] = streak\n",
    "\n",
    "    # 7) MACD\n",
    "    macd_line   = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
    "    macd_sig    = macd_line.ewm(span=9, adjust=False).mean()\n",
    "    df[\"MACD\"]      = macd_line\n",
    "    df[\"MACD_SIG\"]  = macd_sig\n",
    "    df[\"MACD_HIST\"] = macd_line - macd_sig\n",
    "\n",
    "    # 8) Pivots & broken pivots\n",
    "    for w in [6, 10, 50, 100]:\n",
    "        df[f\"PIVOT_H_{w}\"] = price.rolling(w, min_periods=1).max().shift(1)\n",
    "        df[f\"PIVOT_L_{w}\"] = price.rolling(w, min_periods=1).min().shift(1)\n",
    "        df[f\"BRK_H_{w}\"]   = (price > df[f\"PIVOT_H_{w}\"]).astype(int)\n",
    "        df[f\"BRK_L_{w}\"]   = (price < df[f\"PIVOT_L_{w}\"]).astype(int)\n",
    "\n",
    "    return df.fillna(method=\"bfill\").fillna(method=\"ffill\").fillna(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Parameters ──────────────────────────────────────────────────────────\n",
    "    PRICE_FILE        = '../../prices.txt'\n",
    "    WINDOW_SIZE       = 36\n",
    "    N_PIPS            = 7\n",
    "    DIST_MEASURE      = 2\n",
    "    N_CLUSTERS        = 12\n",
    "    CLUSTER_THRESHOLD = 0.30\n",
    "    GBDT_PARAMS = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 4,\n",
    "        \"subsample\": 0.7,\n",
    "        \"random_state\": 0\n",
    "    }\n",
    "    MIN_SAMPLES = 50   # minimum training rows per cluster to build a model\n",
    "\n",
    "    # ─── 1) Load data & compute indicators ───────────────────────────────────\n",
    "    raw = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    raw.columns = [f\"I{i}\" for i in raw.columns]\n",
    "    price_df = raw.copy()\n",
    "    print(\"💡 Computing indicators for each instrument…\")\n",
    "    feats_by_inst = {inst: compute_indicators(price_df[inst])\n",
    "                     for inst in price_df.columns}\n",
    "    print(\"→ Done.\\n\")\n",
    "\n",
    "    # ─── 2) Log‐prices & split train/test ───────────────────────────────────\n",
    "    log_vals = np.log(price_df.values)\n",
    "    T, n_inst = log_vals.shape\n",
    "    n_train = 500\n",
    "    train_T = log_vals[:n_train].T   # shape (n_inst, 500)\n",
    "    test_T  = log_vals[n_train:].T   # shape (n_inst, 250)\n",
    "\n",
    "    # ─── 3) Cluster PIP‐patterns on training set ───────────────────────────\n",
    "    train_patterns = []\n",
    "    for ii in range(n_inst):\n",
    "        series = train_T[ii]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            win  = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals-mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            train_patterns.append(norm)\n",
    "\n",
    "    print(f\"🔧 Clustering {len(train_patterns)} training windows into {N_CLUSTERS} clusters…\")\n",
    "    kmeans  = KMeans(n_clusters=N_CLUSTERS, random_state=0)\n",
    "    kmeans.fit(train_patterns)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(\"→ KMeans complete.\\n\")\n",
    "\n",
    "    # plot cluster sizes\n",
    "    ct = Counter(kmeans.labels_)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(ct.keys(), ct.values(), alpha=0.7)\n",
    "    plt.title(\"Train‐set windows per cluster\")\n",
    "    plt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ─── 4) Build per‐cluster training sets ────────────────────────────────\n",
    "    X_train_by_c = defaultdict(list)\n",
    "    y_train_by_c = defaultdict(list)\n",
    "\n",
    "    print(\"🔧 Building per-cluster training sets…\")\n",
    "    for ii, inst in enumerate(price_df.columns):\n",
    "        series = train_T[ii]\n",
    "        feats_df = feats_by_inst[inst]\n",
    "        for t in range(WINDOW_SIZE, n_train-1):\n",
    "            win  = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            # assign cluster on first N_PIPS-1 PIPs\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals-mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            cid    = int(kmeans.predict([norm])[0])\n",
    "            if np.linalg.norm(norm - centers[cid]) > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # features at bar t-1\n",
    "            feats = feats_df.iloc[t-1].values\n",
    "\n",
    "            # label next-bar Up/Down\n",
    "            label = 1 if series[t] > series[t-1] else 0\n",
    "\n",
    "            X_train_by_c[cid].append(feats)\n",
    "            y_train_by_c[cid].append(label)\n",
    "\n",
    "    # report cluster training sizes\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        print(f\" Cluster {cid:>2d}: {len(y_train_by_c[cid])} samples\")\n",
    "    print()\n",
    "\n",
    "    # ─── 5) Train one GBDT per cluster ────────────────────────────────────\n",
    "    cluster_models = {}\n",
    "    print(\"🔧 Training one model per cluster…\")\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        Xs = X_train_by_c[cid]\n",
    "        Ys = y_train_by_c[cid]\n",
    "        if len(Ys) < MIN_SAMPLES:\n",
    "            print(f\"  · skipping cluster {cid} ({len(Ys)} < {MIN_SAMPLES})\")\n",
    "            continue\n",
    "        model = GradientBoostingClassifier(**GBDT_PARAMS)\n",
    "        model.fit(Xs, Ys)\n",
    "        cluster_models[cid] = model\n",
    "        print(f\"  · trained cluster {cid} model on {len(Ys)} samples\")\n",
    "\n",
    "    # ─── 6) Inference on test set ─────────────────────────────────────────\n",
    "    print(\"\\n🔧 Running test‐set inference…\")\n",
    "    y_true, y_pred = [], []\n",
    "    per_cluster_perf = defaultdict(lambda: [0,0])  # correct, total\n",
    "\n",
    "    for ii, inst in enumerate(price_df.columns):\n",
    "        series = test_T[ii]\n",
    "        feats_df = feats_by_inst[inst]\n",
    "        for t in range(WINDOW_SIZE, test_T.shape[1]-1):\n",
    "            win  = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "\n",
    "            # cluster assignment\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals-mn)/(mx-mn) if mx!=mn else np.zeros_like(vals)\n",
    "            cid    = int(kmeans.predict([norm])[0])\n",
    "            if (cid not in cluster_models or\n",
    "                np.linalg.norm(norm - centers[cid]) > CLUSTER_THRESHOLD):\n",
    "                continue\n",
    "\n",
    "            feats = feats_df.iloc[n_train + t - 1].values\n",
    "            pred  = cluster_models[cid].predict([feats])[0]\n",
    "            actual= 1 if series[t] > series[t-1] else 0\n",
    "\n",
    "            y_true.append(actual)\n",
    "            y_pred.append(pred)\n",
    "            per_cluster_perf[cid][1] += 1\n",
    "            per_cluster_perf[cid][0] += (pred == actual)\n",
    "\n",
    "            print(f\"[{inst} @ t={n_train + t}] Cluster {cid} → \"\n",
    "                  f\"Pred={'Up' if pred else 'Down'} / \"\n",
    "                  f\"Actual={'Up' if actual else 'Down'}\")\n",
    "\n",
    "    # ─── 7) Summaries & plots ─────────────────────────────────────────────\n",
    "    print(\"\\n🎯 Global confusion matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Down\",\"Up\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.show()\n",
    "    acc = np.mean(np.array(y_true)==np.array(y_pred))\n",
    "    print(f\"Overall accuracy: {acc:.2%} ({len(y_true)} predictions)\\n\")\n",
    "\n",
    "    print(\"Per-cluster accuracy:\")\n",
    "    for cid in sorted(per_cluster_perf):\n",
    "        corr, tot = per_cluster_perf[cid]\n",
    "        print(f\" Cluster {cid}: {corr}/{tot} = {corr/tot:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist, max_idx = -1.0, None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1+1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i - (x2 - x1) * data[i] + x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den else 0\n",
    "                else:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                if dist > max_dist:\n",
    "                    max_dist, max_idx = dist, i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def compute_indicators(price: pd.Series) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(index=price.index)\n",
    "    # SMAs & EMAs\n",
    "    sma_ws = [10, 20, 50, 100]\n",
    "    ema_ws = [12, 26, 50, 200]\n",
    "    for w in sma_ws:\n",
    "        df[f\"SMA_{w}\"] = price.rolling(w, min_periods=1).mean()\n",
    "    for w in ema_ws:\n",
    "        df[f\"EMA_{w}\"] = price.ewm(span=w, adjust=False).mean()\n",
    "    # RSI\n",
    "    for w in [6, 14, 21]:\n",
    "        delta = price.diff()\n",
    "        up = delta.clip(lower=0).rolling(w, min_periods=1).mean()\n",
    "        down = (-delta.clip(upper=0)).rolling(w, min_periods=1).mean()\n",
    "        rs = up / down.replace(0, np.nan)\n",
    "        df[f\"RSI_{w}\"] = 100 - 100/(1+rs)\n",
    "    # ROC, STD, Z\n",
    "    for w in [5, 10, 20, 50]:\n",
    "        df[f\"ROC_{w}\"] = price.pct_change(w)\n",
    "        df[f\"STD_{w}\"] = price.rolling(w, min_periods=1).std()\n",
    "    for w in [20, 50]:\n",
    "        mu = price.rolling(w, min_periods=1).mean()\n",
    "        sd = price.rolling(w, min_periods=1).std().replace(0, np.nan)\n",
    "        df[f\"Z_{w}\"] = (price - mu) / sd\n",
    "    # MACD\n",
    "    df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
    "    df[\"MACD_SIG\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"MACD_HIST\"] = df[\"MACD\"] - df[\"MACD_SIG\"]\n",
    "    return df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    PRICE_FILE = '../../prices.txt'\n",
    "    WINDOW_SIZE = 24\n",
    "    N_PIPS = 5\n",
    "    DIST_MEASURE = 2\n",
    "    N_CLUSTERS = 10\n",
    "    CLUSTER_THRESHOLD = 0.3\n",
    "\n",
    "    raw = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    raw.columns = [f\"I{c}\" for c in raw.columns]\n",
    "    price_df = raw.copy()\n",
    "\n",
    "    # Compute indicators\n",
    "    print(\"Computing indicators for all instruments...\")\n",
    "    feats_by_inst = {inst: compute_indicators(price_df[inst]) for inst in price_df.columns}\n",
    "    print(\"Done computing indicators.\\n\")\n",
    "\n",
    "    # Log-transform and split\n",
    "    log_vals = np.log(price_df.values)\n",
    "    n_train = 500\n",
    "    train_T = log_vals[:n_train].T\n",
    "    test_T = log_vals[n_train:].T\n",
    "    n_inst = train_T.shape[0]\n",
    "\n",
    "    # 1) Cluster PIP patterns in training set\n",
    "    print(\"Clustering PIP patterns on training data...\")\n",
    "    train_patterns = []\n",
    "    for i in range(n_inst):\n",
    "        series = train_T[i]\n",
    "        for t in range(WINDOW_SIZE, n_train):\n",
    "            win = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            train_patterns.append(norm)\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(train_patterns)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(\"Done clustering.\\n\")\n",
    "\n",
    "    # 2) Build per-cluster training sets\n",
    "    X_train_by_c = defaultdict(list)\n",
    "    y_train_by_c = defaultdict(list)\n",
    "    print(\"Building per-cluster training datasets...\")\n",
    "    for i, inst in enumerate(price_df.columns):\n",
    "        series = train_T[i]\n",
    "        feats_df = feats_by_inst[inst]\n",
    "        for t in range(WINDOW_SIZE, n_train-1):\n",
    "            win = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            cid = int(kmeans.predict([norm])[0])\n",
    "            if np.linalg.norm(norm - centers[cid]) > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "            feats = feats_df.iloc[t-1].values\n",
    "            label = 1 if series[t] > series[t-1] else 0\n",
    "            X_train_by_c[cid].append(feats)\n",
    "            y_train_by_c[cid].append(label)\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        print(f\"Cluster {cid}: {len(y_train_by_c[cid])} training samples\")\n",
    "    print()\n",
    "\n",
    "    # 3) Train CatBoost per cluster\n",
    "    print(\"Training CatBoost models per cluster...\")\n",
    "    cluster_models = {}\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        Xs = X_train_by_c[cid]\n",
    "        Ys = y_train_by_c[cid]\n",
    "        if len(Ys) < 100:\n",
    "            print(f\"Skipping cluster {cid}, only {len(Ys)} samples\")\n",
    "            continue\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            learning_rate=0.05,\n",
    "            depth=4,\n",
    "            loss_function='Logloss',\n",
    "            verbose=False\n",
    "        )\n",
    "        model.fit(Xs, Ys)\n",
    "        cluster_models[cid] = model\n",
    "        print(f\"Trained CatBoost for cluster {cid} ({len(Ys)} samples)\")\n",
    "    print()\n",
    "\n",
    "    # 4) Inference on test set\n",
    "    y_true, y_pred = [], []\n",
    "    perf_by_c = defaultdict(lambda: [0,0])  # correct, total\n",
    "    print(\"Running per-cluster inference on test data...\")\n",
    "    for i, inst in enumerate(price_df.columns):\n",
    "        series = test_T[i]\n",
    "        feats_df = feats_by_inst[inst]\n",
    "        for t in range(WINDOW_SIZE, test_T.shape[1]-1):\n",
    "            win = series[t-WINDOW_SIZE:t]\n",
    "            pips = find_pips(win, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips) < N_PIPS:\n",
    "                continue\n",
    "            vals = win[pips[:-1]]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            cid = int(kmeans.predict([norm])[0])\n",
    "            if cid not in cluster_models:\n",
    "                continue\n",
    "            feats = feats_df.iloc[n_train + t - 1].values\n",
    "            pred = cluster_models[cid].predict([feats])[0]\n",
    "            actual = 1 if series[t] > series[t-1] else 0\n",
    "            y_true.append(actual)\n",
    "            y_pred.append(pred)\n",
    "            perf_by_c[cid][1] += 1\n",
    "            perf_by_c[cid][0] += (pred == actual)\n",
    "            print(f\"Inst {inst} t={n_train+t}, Clust {cid}, Pred={'Up' if pred else 'Down'}, Actual={'Up' if actual else 'Down'}\")\n",
    "\n",
    "    # 5) Global confusion\n",
    "    print(\"\\nGlobal confusion matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Down\",\"Up\"])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.show()\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"Overall accuracy: {acc:.2%} ({len(y_true)} predictions)\\n\")\n",
    "\n",
    "    print(\"Per-cluster accuracy:\")\n",
    "    for cid, (corr, tot) in perf_by_c.items():\n",
    "        print(f\"Cluster {cid}: {corr}/{tot} = {corr/tot:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_pips(data: np.ndarray, n_pips: int, dist_measure: int = 2) -> list[int]:\n",
    "    \"\"\"\n",
    "    Extract n_pips perceptually important points (PIPs) from a 1D series.\n",
    "    dist_measure: 1=Euclidean (x-distance),\n",
    "                  2=Perpendicular distance to chord,\n",
    "                  3=Vertical distance to chord\n",
    "    Returns sorted list of indices for PIPs within data.\n",
    "    \"\"\"\n",
    "    if n_pips < 2:\n",
    "        return []\n",
    "    pips_x = [0, len(data) - 1]\n",
    "    for _ in range(2, n_pips):\n",
    "        max_dist = -1.0\n",
    "        max_idx = None\n",
    "        coords = sorted((x, data[x]) for x in pips_x)\n",
    "        for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):\n",
    "            for i in range(x1 + 1, x2):\n",
    "                if dist_measure == 1:\n",
    "                    dist = abs(i - (x1 + x2) / 2)\n",
    "                elif dist_measure == 2:\n",
    "                    num = abs((y2 - y1) * i -\n",
    "                              (x2 - x1) * data[i] +\n",
    "                              x2*y1 - y2*x1)\n",
    "                    den = math.hypot(y2 - y1, x2 - x1)\n",
    "                    dist = num / den if den else 0\n",
    "                elif dist_measure == 3:\n",
    "                    interp = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "                    dist = abs(data[i] - interp)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown dist_measure {dist_measure}\")\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_idx = i\n",
    "        if max_idx is None:\n",
    "            break\n",
    "        pips_x.append(max_idx)\n",
    "    return sorted(pips_x)\n",
    "\n",
    "\n",
    "def extract_pip_patterns(log_data: np.ndarray,\n",
    "                         window_size: int,\n",
    "                         n_pips: int,\n",
    "                         dist_measure: int = 2) -> Counter:\n",
    "    \"\"\"\n",
    "    Count normalized PIP patterns in the data.\n",
    "    \"\"\"\n",
    "    n_inst, n_t = log_data.shape\n",
    "    patterns = Counter()\n",
    "    for inst in range(n_inst):\n",
    "        series = log_data[inst]\n",
    "        for end in range(window_size, n_t + 1):\n",
    "            window = series[end - window_size:end]\n",
    "            pips_idx = find_pips(window, n_pips, dist_measure)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "            patterns[tuple(np.round(norm, 3))] += 1\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def cluster_patterns(patterns: list, n_clusters: int):\n",
    "    data = np.array(patterns)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return kmeans, data, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ─── Configuration ───────────────────────────────────────────────────────\n",
    "    PRICE_FILE        = '../../prices.txt'\n",
    "    WINDOW_SIZE       = 5\n",
    "    N_PIPS            = 3\n",
    "    DIST_MEASURE      = 2\n",
    "    N_CLUSTERS        = 8\n",
    "    CLUSTER_THRESHOLD = 0.35\n",
    "\n",
    "    # ─── Load & pre-process ──────────────────────────────────────────────────\n",
    "    df = pd.read_csv(PRICE_FILE, sep=r\"\\s+\", header=None)\n",
    "    log_prices = np.log(df.values.T)\n",
    "    n_inst, n_t = log_prices.shape\n",
    "\n",
    "    n_train    = 500\n",
    "    train_data = log_prices[:, :n_train]\n",
    "    test_data  = log_prices[:, n_train:]\n",
    "    _, n_test  = test_data.shape\n",
    "\n",
    "    # ─── 1) Train clusters on training data ──────────────────────────────────\n",
    "    train_patterns = extract_pip_patterns(train_data, WINDOW_SIZE, N_PIPS, DIST_MEASURE)\n",
    "    train_vecs     = list(train_patterns.keys())\n",
    "    kmeans, _, _   = cluster_patterns(train_vecs, N_CLUSTERS)\n",
    "    centers        = kmeans.cluster_centers_\n",
    "\n",
    "    # ─── 2) Identify cluster occurrences on test set per instrument ────────\n",
    "    matched_ends = {inst: {cid: [] for cid in range(N_CLUSTERS)} for inst in range(n_inst)}\n",
    "    cluster_counts_test   = Counter()\n",
    "    cluster_patterns_test = {cid: [] for cid in range(N_CLUSTERS)}\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        series = test_data[inst]\n",
    "        for end in range(WINDOW_SIZE, n_test):\n",
    "            window   = series[end - WINDOW_SIZE:end]\n",
    "            pips_idx = find_pips(window, N_PIPS, DIST_MEASURE)\n",
    "            if len(pips_idx) < 2:\n",
    "                continue\n",
    "\n",
    "            vals = window[pips_idx]\n",
    "            mn, mx = vals.min(), vals.max()\n",
    "            norm   = (vals - mn) / (mx - mn) if mx != mn else np.zeros_like(vals)\n",
    "\n",
    "            lbl  = kmeans.predict([norm])[0]\n",
    "            dist = np.linalg.norm(norm - centers[lbl])\n",
    "            if dist > CLUSTER_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            cluster_counts_test[lbl] += 1\n",
    "            cluster_patterns_test[lbl].append(tuple(np.round(norm, 3)))\n",
    "            matched_ends[inst][lbl].append(end)\n",
    "\n",
    "    # ─── 3) Plot price windows grouped by cluster first, then instrument ─────\n",
    "    x_idx = np.arange(n_train, n_train + n_test)\n",
    "    for cid in range(N_CLUSTERS):\n",
    "        for inst in range(n_inst):\n",
    "            ends = matched_ends[inst][cid]\n",
    "            if not ends:\n",
    "                continue\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            prices = np.exp(test_data[inst])\n",
    "            plt.plot(x_idx, prices, linewidth=1.5, label=f'Instrument {inst} Price')\n",
    "            for end in ends:\n",
    "                start = n_train + end - WINDOW_SIZE\n",
    "                stop  = n_train + end\n",
    "                plt.axvspan(start, stop, alpha=0.3,\n",
    "                            label=f'Cluster {cid} Window' if end == ends[0] else None)\n",
    "            plt.title(f'Cluster {cid} - Instrument {inst} Occurrences (n={len(ends)})')\n",
    "            plt.xlabel('Time Index')\n",
    "            plt.ylabel('Price')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # ─── 4) Print test-set cluster frequencies ───────────────────────────────\n",
    "    print(\"\\nTest Set: Cluster ID → Accumulated Frequency\")\n",
    "    for cid, freq in cluster_counts_test.most_common():\n",
    "        print(f\"Cluster {cid}: {freq}\")\n",
    "\n",
    "    # ─── 5) Plot each test-set cluster patterns + centroid ─────────────────\n",
    "    x = np.arange(N_PIPS)\n",
    "    for cid, pats in cluster_patterns_test.items():\n",
    "        if not pats:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for pat in pats:\n",
    "            plt.plot(x, pat, alpha=0.1)\n",
    "        plt.plot(x, centers[cid], linewidth=2, label=f'Centroid {cid}')\n",
    "        plt.title(f'Test Set Cluster {cid} (n={cluster_counts_test[cid]})')\n",
    "        plt.xlabel('PIP Index')\n",
    "        plt.ylabel('Normalized Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
