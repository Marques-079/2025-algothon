{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_proc_pipeline import pipeline_regiem\n",
    "from pre_proc_labelling_long import plot_all_regimes_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 indicates bearmarket, 2 indicates bull market\n",
    "count = 0\n",
    "count_bull = 0\n",
    "count_bear = 0\n",
    "for i in range(50):\n",
    "    labels = plot_all_regimes_long(500, False, i)\n",
    "    for j in labels:\n",
    "        if j == 1:\n",
    "            count += 1\n",
    "        elif j == 0:\n",
    "            count_bear += 1\n",
    "        else:\n",
    "            count_bull +=1\n",
    "        \n",
    "print(f\"Count of label Netural: {count}\")\n",
    "print(f\"Count of label Bear: {count_bear}\")\n",
    "print(f\"Count of label Bull: {count_bull}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline_regiem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pre_proc_labelling_long import plot_all_regimes_long\n",
    "from pre_proc_pipeline import pipeline_regiem\n",
    "\n",
    "df = pipeline_regiem()            \n",
    "\n",
    "label_frames = []\n",
    "for inst, inst_df in df.groupby(level=\"inst\", sort=False):\n",
    "\n",
    "    labels = plot_all_regimes_long(len(inst_df), False, inst)\n",
    "\n",
    "    valid_idx = inst_df.index[: len(labels) ]\n",
    "\n",
    "    s = pd.Series(labels, index=valid_idx, name=\"regime\")\n",
    "\n",
    "    label_frames.append(s)\n",
    "\n",
    "regimes = pd.concat(label_frames) \n",
    "\n",
    "df2 = df.copy()\n",
    "df2[\"regime\"] = regimes         \n",
    "df2[\"target\"] = (df2.groupby(level=\"inst\")[\"regime\"].shift(-1))\n",
    "\n",
    "df2 = df2.dropna(subset=[\"target\"])\n",
    "\n",
    "X = df2.drop(columns=[\"regime\",\"target\"])\n",
    "y = df2[\"target\"].astype(int)\n",
    "\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "#Mapping 2.0 -> 1.0 for clarity\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # learns μ/σ on train\n",
    "X_test_scaled  = scaler.transform(X_test)       # applies same transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset Context:\n",
    "\n",
    "X_train: (25000, 59) X_test: (6950, 59)\n",
    "59 features,\n",
    "Labels: 0 -> Bear, 1 -> Bull\n",
    "Derived from only close price data over 50 instruments, we train on 500 timesteps of data per instrument\n",
    "Models Aim: We want to maximise consistency in regiem identifcation, regiems often last around 150 time steps long, but can range from 30 to 500.\n",
    "\n",
    "Acess data by (all data is shuffled, segregated by bound 500):\n",
    "X_train_scaled : Training set first 500 timesteps of each instrument\n",
    "y_train : Labelled data for X_train_scaled already aligned\n",
    "\n",
    "\n",
    "X_test_scaled : Test set last 250 timesteps of each instrument\n",
    "y_test : Labelled data for X_test_scaled already aligned\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dvalid = lgb.Dataset(X_test,  label=y_test, reference=dtrain)\n",
    "\n",
    "#Training parameters\n",
    "params = {\n",
    "    \"objective\":       \"binary\",\n",
    "    \"metric\":          [\"binary_logloss\",\"binary_error\"],\n",
    "    \"learning_rate\":   0.05,\n",
    "    \"num_leaves\":      31,\n",
    "    \"min_data_in_leaf\":20,\n",
    "    \"feature_fraction\":0.8,\n",
    "    \"bagging_fraction\":0.8,\n",
    "    \"bagging_freq\":     5,\n",
    "    \"verbose\":         -1,\n",
    "}\n",
    "\n",
    "# 2) your tqdm wrapper stays the same\n",
    "num_round = 500\n",
    "pbar = tqdm(total=num_round, desc=\"LightGBM boosting rounds\")\n",
    "def lgb_tqdm_callback(env):\n",
    "    pbar.update(1)\n",
    "    if env.iteration + 1 == num_round:\n",
    "        pbar.close()\n",
    "\n",
    "# 3) call train WITHOUT verbose_eval or early_stopping_rounds\n",
    "bst = lgb.train(\n",
    "    params,\n",
    "    train_set=dtrain,\n",
    "    num_boost_round=num_round,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    valid_names=[\"train\",\"valid\"],\n",
    "    callbacks=[\n",
    "        # run early‐stop after 50 rounds of no improvement\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        # log train/valid metrics every 10 iters (optional)\n",
    "        log_evaluation(period=10),\n",
    "        # your custom tqdm progress bar\n",
    "        lgb_tqdm_callback,\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred_prob = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred      = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test  F1     :\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "print(\"XGBoost accuracy:\", xgb.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cb = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='Accuracy',\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "cb.fit(X_train_scaled, y_train, eval_set=(X_test_scaled, y_test), use_best_model=True)\n",
    "print(\"CatBoost accuracy:\", cb.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.05, random_state=42)),\n",
    "    ('rf',  RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)),\n",
    "    ('cb',  CatBoostClassifier(iterations=300, depth=5, learning_rate=0.05, verbose=0, random_seed=42))\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack.fit(X_train_scaled, y_train)\n",
    "print(\"Stacked model accuracy:\", stack.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "#Mapping 2.0 -> 1.0 for clarity\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # learns μ/σ on train\n",
    "X_test_scaled  = scaler.transform(X_test)       # applies same transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn-crfsuite\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Split X, y per instrument into train/test lists\n",
    "train_dfs, train_lbls = [], []\n",
    "test_dfs,  test_lbls  = [], []\n",
    "\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    idx = grp.index\n",
    "    train_dfs.append(X.loc[idx[:500]])\n",
    "    train_lbls.append([str(v) for v in y.loc[idx[:500]].values])\n",
    "    test_dfs .append(X.loc[idx[500:]])\n",
    "    test_lbls .append([str(v) for v in y.loc[idx[500:]].values])\n",
    "\n",
    "# 2) Global scaling\n",
    "scaler = StandardScaler()\n",
    "all_X = pd.concat(train_dfs + test_dfs)\n",
    "_ = scaler.fit_transform(all_X)\n",
    "for i in range(len(train_dfs)):\n",
    "    train_dfs[i] = pd.DataFrame(\n",
    "        scaler.transform(train_dfs[i]),\n",
    "        index=train_dfs[i].index,\n",
    "        columns=train_dfs[i].columns\n",
    "    )\n",
    "for i in range(len(test_dfs)):\n",
    "    test_dfs[i] = pd.DataFrame(\n",
    "        scaler.transform(test_dfs[i]),\n",
    "        index=test_dfs[i].index,\n",
    "        columns=test_dfs[i].columns\n",
    "    )\n",
    "\n",
    "# 3) Convert each row to a feature dict\n",
    "def df_to_feats(df):\n",
    "    return [\n",
    "        {f\"f{j}\": float(val) for j, val in enumerate(row)}\n",
    "        for row in df.values\n",
    "    ]\n",
    "\n",
    "X_seq_train = [df_to_feats(df) for df in train_dfs]\n",
    "X_seq_test  = [df_to_feats(df) for df in test_dfs]\n",
    "\n",
    "# 4) Train CRF\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=200,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_seq_train, train_lbls)\n",
    "\n",
    "# 5) Predict & evaluate\n",
    "pred_seq = crf.predict(X_seq_test)\n",
    "y_pred   = np.concatenate([[int(lbl) for lbl in seq] for seq in pred_seq])\n",
    "y_true   = np.concatenate([[int(lbl) for lbl in seq] for seq in test_lbls])\n",
    "\n",
    "accuracy = (y_pred == y_true).mean()\n",
    "print(f\"CRF test accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Split & scale exactly as you already have ───────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# Map labels 2→1, else 0\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Build sliding windows ───────────────────────────────────────────────\n",
    "L = 50  # window length (you can tune this)\n",
    "\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = inst_ids == inst\n",
    "        arr = X_arr[mask]       # (T_inst, n_features)\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(\n",
    "    X_train_scaled, y_train.values, X_train.index\n",
    ")\n",
    "X_seq_test, y_seq_test = make_windows(\n",
    "    X_test_scaled,  y_test.values,  X_test.index\n",
    ")\n",
    "\n",
    "print(\"Seq shapes →\", X_seq_train.shape, y_seq_train.shape)\n",
    "\n",
    "# ─── 2) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_train, y_seq_train),\n",
    "    batch_size=batch_size, shuffle=True,  num_workers=0\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    RegimeDataset(X_seq_test,  y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "# ─── 3) Pure‐LSTM model ─────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = drop if num_layers>1 else 0.0\n",
    "        )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, L, F)\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        # h_n: (num_layers, batch, hidden_dim)\n",
    "        h_last = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = LSTMRegime(n_features=X_seq_train.shape[2]).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 4) Train & evaluate with tqdm ─────────────────────────────────────────\n",
    "num_epochs = 6\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — train —\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_bar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = crit(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * xb.size(0)\n",
    "        train_bar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # — eval —\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    eval_bar = tqdm(test_loader, desc=f\"Epoch {epoch}/{num_epochs} [Eval ]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in eval_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            acc_batch = (preds == yb).float().mean().item()\n",
    "            eval_bar.set_postfix(acc=f\"{acc_batch:.4f}\")\n",
    "\n",
    "    epoch_acc = correct / len(test_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train loss: {avg_loss:.4f},  Test acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Split & scale ───────────────────────────────────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# map labels → 0/1\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Sliding windows ──────────────────────────────────────────────────────\n",
    "L = 50  # sequence length\n",
    "\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = (inst_ids == inst)\n",
    "        arr = X_arr[mask]\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i:i+L])\n",
    "            ys.append(labs[i+L-1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(X_train_scaled, y_train.values, X_train.index)\n",
    "X_seq_test,  y_seq_test  = make_windows(X_test_scaled,  y_test.values,  X_test.index)\n",
    "\n",
    "# ─── 2) Dataset & DataLoader ────────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 256\n",
    "# using num_workers=0 to avoid multiprocessing pickling issues\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ─── 3) Vanilla RNN model ───────────────────────────────────────────────────\n",
    "class RNNRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            nonlinearity = 'tanh',\n",
    "            dropout      = drop if num_layers>1 else 0.0\n",
    "        )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, L, F)\n",
    "        out, h_n = self.rnn(x)\n",
    "        # h_n: (num_layers, batch, hidden_dim)\n",
    "        h_last = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = RNNRegime(n_features=X_seq_train.shape[2]).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 4) Train & evaluate ───────────────────────────────────────────────────\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — training —\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_bar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = crit(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * xb.size(0)\n",
    "        train_bar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # — evaluation —\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    eval_bar = tqdm(test_loader, desc=f\"Epoch {epoch}/{num_epochs} [Eval ]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in eval_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            batch_acc = (preds == yb).float().mean().item()\n",
    "            eval_bar.set_postfix(acc=f\"{batch_acc:.4f}\")\n",
    "\n",
    "    epoch_acc = correct / len(test_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train loss: {avg_loss:.4f},  Test acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 0) Installs (if you haven’t already) ───────────────────────────────────\n",
    "# In a notebook cell:\n",
    "# %pip install --upgrade pip\n",
    "# %pip install scikit-learn sktime numba\n",
    "\n",
    "# ─── 1) Imports & assume X, y are already in memory ─────────────────────────\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sktime.transformations.panel.rocket import MiniRocket\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# ─── 2) Split into train/test by inst, then scale ───────────────────────────\n",
    "train_parts, train_labels = [], []\n",
    "test_parts,  test_labels  = [], []\n",
    "\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    idx = grp.index\n",
    "    train_parts.append(X.loc[idx[:500]])\n",
    "    train_labels.append(y.loc[idx[:500]])\n",
    "    test_parts .append(X.loc[idx[500:]])\n",
    "    test_labels .append(y.loc[idx[500:]])\n",
    "\n",
    "X_train = pd.concat(train_parts)\n",
    "y_train = (pd.concat(train_labels)==2).astype(int).values\n",
    "X_test  = pd.concat(test_parts)\n",
    "y_test  = (pd.concat(test_labels)==2).astype(int).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 3) Build sliding‐window arrays ─────────────────────────────────────────\n",
    "L = 50\n",
    "def make_windows(X_arr, y_arr, idx):\n",
    "    Xs, ys = [], []\n",
    "    insts = idx.get_level_values(0)\n",
    "    for inst in np.unique(insts):\n",
    "        mask = insts == inst\n",
    "        arr  = X_arr[mask]\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(X_train_scaled, y_train, X_train.index)\n",
    "X_seq_test,  y_seq_test  = make_windows(X_test_scaled,  y_test,  X_test.index)\n",
    "\n",
    "print(\"Windowed shapes:\", X_seq_train.shape, y_seq_train.shape)\n",
    "\n",
    "# ─── 4) Convert to nested DataFrame for sktime ──────────────────────────────\n",
    "def to_nested(arr3d):\n",
    "    n, L, F = arr3d.shape\n",
    "    return pd.DataFrame({\n",
    "        f\"f{i}\": [pd.Series(arr3d[j,:,i]) for j in range(n)]\n",
    "        for i in range(F)\n",
    "    })\n",
    "\n",
    "X_train_nested = to_nested(X_seq_train)\n",
    "X_test_nested  = to_nested(X_seq_test)\n",
    "print(\"Nested shape:\", X_train_nested.shape)\n",
    "\n",
    "# ─── 5) Fit MiniRocket & transform ──────────────────────────────────────────\n",
    "print(\"Fitting MiniRocket…\")\n",
    "mrocket = MiniRocket(random_state=42, num_kernels=500, n_jobs=1)\n",
    "t0 = time.time()\n",
    "mrocket.fit(X_train_nested)\n",
    "print(f\" → done in {time.time()-t0:.1f}s, kernels={mrocket.num_kernels}\")\n",
    "\n",
    "print(\"Transforming train…\")\n",
    "t1 = time.time()\n",
    "X_train_feat = mrocket.transform(X_train_nested)\n",
    "print(f\"  Train → {X_train_feat.shape} in {time.time()-t1:.1f}s\")\n",
    "\n",
    "print(\"Transforming test…\")\n",
    "t2 = time.time()\n",
    "X_test_feat  = mrocket.transform(X_test_nested)\n",
    "print(f\"  Test  → {X_test_feat.shape} in {time.time()-t2:.1f}s\")\n",
    "\n",
    "# ─── 6) Train & evaluate an online logistic (SGDClassifier) ───────────────\n",
    "print(\"Training SGDClassifier…\")\n",
    "clf = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    penalty=\"l2\",\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "t0 = time.time()\n",
    "clf.fit(X_train_feat, y_seq_train)\n",
    "print(f\" → trained in {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(\"Final test accuracy:\")\n",
    "print(f\"  MiniRocket + SGDClassifier → {clf.score(X_test_feat, y_seq_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ─── 1) ASSUME YOU ALREADY HAVE:\n",
    "#    X_train_scaled, X_test_scaled: numpy arrays of shape (N_train, 59) and (N_test, 59)\n",
    "#    y_train, y_test: arrays or Series of 0/1 labels of length N_train and N_test\n",
    "\n",
    "# ─── 2) DYNAMIC RESHAPING ──────────────────────────────────────────────────────\n",
    "n_features      = X_train_scaled.shape[1]\n",
    "train_timesteps = 500\n",
    "\n",
    "# how many instruments in train?\n",
    "n_instruments = X_train_scaled.shape[0] // train_timesteps\n",
    "\n",
    "# infer test_timesteps so total_test_rows is divisible by n_instruments\n",
    "total_test_rows = X_test_scaled.shape[0]\n",
    "test_timesteps  = total_test_rows // n_instruments\n",
    "\n",
    "# drop any extra rows if not perfectly divisible\n",
    "needed = n_instruments * test_timesteps\n",
    "if total_test_rows != needed:\n",
    "    drop = total_test_rows - needed\n",
    "    print(f\"Dropping {drop} extra test rows → {total_test_rows} → {needed}\")\n",
    "    X_test_scaled = X_test_scaled[:needed]\n",
    "\n",
    "# convert y to numpy\n",
    "y_train_arr = y_train.values if hasattr(y_train, \"values\") else y_train\n",
    "y_test_arr  = y_test .values if hasattr(y_test,  \"values\") else y_test\n",
    "\n",
    "# drop the same rows in y_test\n",
    "if total_test_rows != needed:\n",
    "    y_test_arr = y_test_arr[:needed]\n",
    "\n",
    "# reshape into (n_instruments, timesteps, n_features) / (n_instruments, timesteps)\n",
    "X_train_seq = X_train_scaled.reshape(n_instruments, train_timesteps, n_features)\n",
    "X_test_seq  = X_test_scaled .reshape(n_instruments, test_timesteps,  n_features)\n",
    "\n",
    "y_train_seq = y_train_arr.reshape(n_instruments, train_timesteps)\n",
    "y_test_seq  = y_test_arr .reshape(n_instruments, test_timesteps)\n",
    "\n",
    "print(\"X_train_seq:\", X_train_seq.shape)\n",
    "print(\"X_test_seq: \", X_test_seq.shape)\n",
    "print(\"y_train_seq:\", y_train_seq.shape)\n",
    "print(\"y_test_seq: \", y_test_seq.shape)\n",
    "\n",
    "\n",
    "# ─── 3) DATASET & DATALOADER ─────────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X: (n_seq, seq_len, n_features), y: (n_seq, seq_len)\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size  = 8\n",
    "train_ds    = RegimeDataset(X_train_seq, y_train_seq)\n",
    "test_ds     = RegimeDataset(X_test_seq,  y_test_seq)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "# ─── 4) MODEL DEFINITION ─────────────────────────────────────────────────────\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, rnn_type=\"LSTM\", hidden1=64, hidden2=32, dropout=0.2):\n",
    "        super().__init__()\n",
    "        rnn_cls = nn.LSTM if rnn_type == \"LSTM\" else nn.GRU\n",
    "\n",
    "        # first bidirectional layer\n",
    "        self.rnn1 = rnn_cls(\n",
    "            input_size=n_features, hidden_size=hidden1,\n",
    "            num_layers=1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.do1  = nn.Dropout(dropout)\n",
    "\n",
    "        # second bidirectional layer\n",
    "        self.rnn2 = rnn_cls(\n",
    "            input_size=hidden1*2, hidden_size=hidden2,\n",
    "            num_layers=1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.do2  = nn.Dropout(dropout)\n",
    "\n",
    "        # per-timestep binary output\n",
    "        self.fc   = nn.Linear(hidden2*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, n_features)\n",
    "        out, _ = self.rnn1(x)      # → (batch, seq_len, hidden1*2)\n",
    "        out     = self.do1(out)\n",
    "        out, _ = self.rnn2(out)    # → (batch, seq_len, hidden2*2)\n",
    "        out     = self.do2(out)\n",
    "        logits  = self.fc(out)     # → (batch, seq_len, 1)\n",
    "        return torch.sigmoid(logits).squeeze(-1)  # → (batch, seq_len)\n",
    "\n",
    "\n",
    "# instantiate model — choose \"LSTM\" or \"GRU\"\n",
    "model = BiRNN(rnn_type=\"LSTM\")\n",
    "# model = BiRNN(rnn_type=\"GRU\")\n",
    "\n",
    "\n",
    "# ─── 5) TRAINING SETUP ────────────────────────────────────────────────────────\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X_batch)             # (batch, seq_len)\n",
    "        loss   = criterion(y_pred, y_batch) # BCELoss over all timesteps\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation accuracy\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = (model(X_batch) > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total   += y_batch.numel()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ─── 6) FINAL EVALUATION ─────────────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        preds = (model(X_batch) > 0.5).float()\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total   += y_batch.numel()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Split & scale exactly as you already have ───────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# Map labels 2→1, else 0\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Build sliding windows ───────────────────────────────────────────────\n",
    "L = 60  # window length (you can tune this)\n",
    "\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = inst_ids == inst\n",
    "        arr = X_arr[mask]       # (T_inst, n_features)\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(\n",
    "    X_train_scaled, y_train.values, X_train.index\n",
    ")\n",
    "X_seq_test, y_seq_test = make_windows(\n",
    "    X_test_scaled,  y_test.values,  X_test.index\n",
    ")\n",
    "\n",
    "print(\"Seq shapes →\", X_seq_train.shape, y_seq_train.shape)\n",
    "\n",
    "# ─── 2) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_train, y_seq_train),\n",
    "    batch_size=batch_size, shuffle=True,  num_workers=0\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    RegimeDataset(X_seq_test,  y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "# ─── 3) Pure‐LSTM model ─────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = drop if num_layers>1 else 0.0\n",
    "        )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, L, F)\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        # h_n: (num_layers, batch, hidden_dim)\n",
    "        h_last = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = LSTMRegime(n_features=X_seq_train.shape[2]).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 4) Train & evaluate with tqdm ─────────────────────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — train —\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_bar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = crit(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * xb.size(0)\n",
    "        train_bar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # — eval —\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    eval_bar = tqdm(test_loader, desc=f\"Epoch {epoch}/{num_epochs} [Eval ]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in eval_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            acc_batch = (preds == yb).float().mean().item()\n",
    "            eval_bar.set_postfix(acc=f\"{acc_batch:.4f}\")\n",
    "\n",
    "    epoch_acc = correct / len(test_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train loss: {avg_loss:.4f},  Test acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# You’ll need: pip install torch-crf\n",
    "from torchcrf import CRF\n",
    "\n",
    "# ─── 1) ASSUME YOU ALREADY HAVE:\n",
    "#    X_train_scaled, X_test_scaled: numpy arrays of shape (N_train, 59) & (N_test, 59)\n",
    "#    y_train, y_test: arrays or pandas Series of 0/1 labels of length N_train & N_test\n",
    "\n",
    "# ─── 2) DYNAMIC RESHAPING INTO SEQUENCES ──────────────────────────────────────\n",
    "n_features      = X_train_scaled.shape[1]\n",
    "train_timesteps = 500\n",
    "\n",
    "# compute how many instruments in train\n",
    "n_instruments   = X_train_scaled.shape[0] // train_timesteps\n",
    "\n",
    "# infer test_timesteps so total_test_rows is divisible by n_instruments\n",
    "total_test_rows = X_test_scaled.shape[0]\n",
    "test_timesteps  = total_test_rows // n_instruments\n",
    "\n",
    "# drop any leftover rows\n",
    "needed = n_instruments * test_timesteps\n",
    "if total_test_rows != needed:\n",
    "    print(f\"Dropping {total_test_rows - needed} extra test rows\")\n",
    "    X_test_scaled = X_test_scaled[:needed]\n",
    "\n",
    "# convert y to numpy and drop same extras\n",
    "y_train_arr = y_train.values if hasattr(y_train, \"values\") else y_train\n",
    "y_test_arr  = y_test .values if hasattr(y_test,  \"values\") else y_test\n",
    "if total_test_rows != needed:\n",
    "    y_test_arr = y_test_arr[:needed]\n",
    "\n",
    "# reshape to (n_instruments, timesteps, features) and (n_instruments, timesteps)\n",
    "X_train_seq = X_train_scaled.reshape(n_instruments, train_timesteps, n_features)\n",
    "X_test_seq  = X_test_scaled .reshape(n_instruments, test_timesteps,  n_features)\n",
    "y_train_seq = y_train_arr.reshape(  n_instruments, train_timesteps)\n",
    "y_test_seq  = y_test_arr.reshape(   n_instruments, test_timesteps)\n",
    "\n",
    "print(\"X_train_seq:\", X_train_seq.shape)  # → (n_instruments, 500, 59)\n",
    "print(\"X_test_seq: \", X_test_seq.shape)   # → (n_instruments, test_timesteps, 59)\n",
    "\n",
    "# ─── 3) DATASET & DATALOADER ─────────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X: (n_seq, seq_len, n_features), y: (n_seq, seq_len)\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        # CRF expects LongTensor labels\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size   = 8\n",
    "train_loader = DataLoader(RegimeDataset(X_train_seq, y_train_seq),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(RegimeDataset(X_test_seq,  y_test_seq),\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "\n",
    "# ─── 4) BI-LSTM + CRF MODEL ──────────────────────────────────────────────────\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 lstm_hidden=64,\n",
    "                 dropout=0.2,\n",
    "                 num_tags=2  # Bear, Bull\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size=in_features,\n",
    "                            hidden_size=lstm_hidden,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # project LSTM outputs to emission scores for each tag\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden * 2, num_tags)\n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, in_features)\n",
    "        lstm_out, _ = self.lstm(x)           # → (batch, seq_len, hidden*2)\n",
    "        feats       = self.hidden2tag(self.dropout(lstm_out))\n",
    "        # decode returns list of tag sequences\n",
    "        return self.crf.decode(feats)\n",
    "\n",
    "    def neg_log_likelihood(self, x, tags):\n",
    "        # tags: (batch, seq_len) with values in {0,1}\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        feats       = self.hidden2tag(self.dropout(lstm_out))\n",
    "        # crf returns log likelihood of the gold tags; we minimize -ll\n",
    "        return -self.crf(feats, tags)\n",
    "\n",
    "\n",
    "# instantiate\n",
    "model = BiLSTM_CRF(in_features=n_features).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# ─── 5) TRAIN/VALID LOOP ─────────────────────────────────────────────────────\n",
    "device    = next(model.parameters()).device\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.neg_log_likelihood(Xb, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb)  # list of lists, length=batch\n",
    "            # convert to tensor\n",
    "            preds_tensor = torch.stack([torch.tensor(p, device=device) for p in preds])\n",
    "            correct += (preds_tensor == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} — Train NLL: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ─── 6) FINAL EVALUATION ─────────────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        preds_tensor = torch.stack([torch.tensor(p, device=device) for p in preds])\n",
    "        correct += (preds_tensor == yb).sum().item()\n",
    "        total   += yb.numel()\n",
    "print(f\"\\nTest Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Your existing data split & scaling ──────────────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# Map labels 2→1, else 0\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Build sliding windows ───────────────────────────────────────────────\n",
    "L = 60  # window length\n",
    "\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = inst_ids == inst\n",
    "        arr = X_arr[mask]\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(\n",
    "    X_train_scaled, y_train.values, X_train.index\n",
    ")\n",
    "X_seq_test, y_seq_test = make_windows(\n",
    "    X_test_scaled,  y_test.values,  X_test.index\n",
    ")\n",
    "\n",
    "# ─── 2) Dataset & DataLoader wrapper ────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "# ─── 3) LSTM model definition ────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = drop if num_layers>1 else 0.0\n",
    "        )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "# ─── 4) Hyperparameter grid ──────────────────────────────────────────────────\n",
    "param_grid = {\n",
    "    \"hidden_dim\":  [64, 128],\n",
    "    \"num_layers\":  [1, 2],\n",
    "    \"dropout\":     [0.2, 0.4],\n",
    "    \"lr\":          [1e-3, 5e-4],\n",
    "    \"batch_size\":  [128, 256]\n",
    "}\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "# ─── 5) Training settings with time limit ───────────────────────────────────\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "time_limit = 30 * 60  # 30 minutes\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, params in enumerate(grid, 1):\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed > time_limit:\n",
    "        print(f\"Time limit reached ({elapsed:.0f}s), stopping grid search.\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\n[{i}/{len(grid)}] Testing params: {params}\")\n",
    "    # DataLoaders for this batch size\n",
    "    bs = params[\"batch_size\"]\n",
    "    train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                              batch_size=bs, shuffle=True)\n",
    "    test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                              batch_size=bs, shuffle=False)\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = LSTMRegime(\n",
    "        n_features = X_seq_train.shape[2],\n",
    "        hidden_dim = params[\"hidden_dim\"],\n",
    "        num_layers = params[\"num_layers\"],\n",
    "        drop       = params[\"dropout\"]\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss  = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        print(f\"  Epoch {epoch}/{num_epochs} — loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    test_acc = correct / total\n",
    "    print(f\"  → Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    results.append({**params, \"test_acc\": test_acc})\n",
    "\n",
    "# ─── 6) Best hyperparameters ────────────────────────────────────────────────\n",
    "best = max(results, key=lambda x: x[\"test_acc\"])\n",
    "print(\"\\nBest hyperparameters and accuracy:\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Data split & scaling ─────────────────────────────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts); y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts);  y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# Map labels 2→1, else 0\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Sliding windows (L=60) ───────────────────────────────────────────────\n",
    "L = 60\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = inst_ids == inst\n",
    "        arr, labs = X_arr[mask], y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(X_train_scaled, y_train.values, X_train.index)\n",
    "X_seq_test,  y_seq_test  = make_windows(X_test_scaled,  y_test.values,  X_test.index)\n",
    "\n",
    "# ─── 2) Dataset & DataLoader ─────────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 256  # from optimal grid search\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 3) Model definition ─────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = 0.0  # dropout only applies if num_layers > 1\n",
    "        )\n",
    "        self.drop = nn.Dropout(0.2)  # optimal dropout\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, L, features)\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last = h_n[-1]                    # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "# ─── 4) Training setup ───────────────────────────────────────────────────────\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = LSTMRegime(\n",
    "    n_features = X_seq_train.shape[2],\n",
    "    hidden_dim = 64,   # optimal\n",
    "    num_layers = 1,    # optimal\n",
    "    drop       = 0.2   # optimal\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  # optimal LR\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — train —\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # — evaluate —\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(test_loader, desc=f\"Epoch {epoch}/{num_epochs} [Eval ]\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred_labels = (model(xb) > 0.5).float()\n",
    "            correct     += (pred_labels == yb).sum().item()\n",
    "            total       += yb.numel()\n",
    "    test_acc = correct / total\n",
    "\n",
    "    print(f\"\\nEpoch {epoch:02d} — Train Loss: {train_loss:.4f}, Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "# ─── 5) Final evaluation ─────────────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct     += (pred_labels == yb).sum().item()\n",
    "        total       += yb.numel()\n",
    "print(f\"Final Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Split & scale exactly as you already have ───────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# Map labels 2→1, else 0\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Build sliding windows ───────────────────────────────────────────────\n",
    "L = 60  # window length (you can tune this)\n",
    "\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = inst_ids == inst\n",
    "        arr = X_arr[mask]       # (T_inst, n_features)\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(\n",
    "    X_train_scaled, y_train.values, X_train.index\n",
    ")\n",
    "X_seq_test, y_seq_test = make_windows(\n",
    "    X_test_scaled,  y_test.values,  X_test.index\n",
    ")\n",
    "\n",
    "print(\"Seq shapes →\", X_seq_train.shape, y_seq_train.shape)\n",
    "\n",
    "# ─── 2) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_train, y_seq_train),\n",
    "    batch_size=batch_size, shuffle=True,  num_workers=0\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    RegimeDataset(X_seq_test,  y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "# ─── 3) Pure‐LSTM model ─────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = drop if num_layers>1 else 0.0\n",
    "        )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, L, F)\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        # h_n: (num_layers, batch, hidden_dim)\n",
    "        h_last = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = LSTMRegime(n_features=X_seq_train.shape[2]).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 4) Train & evaluate with tqdm ─────────────────────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — train —\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_bar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = crit(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * xb.size(0)\n",
    "        train_bar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # — eval —\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    eval_bar = tqdm(test_loader, desc=f\"Epoch {epoch}/{num_epochs} [Eval ]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in eval_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            acc_batch = (preds == yb).float().mean().item()\n",
    "            eval_bar.set_postfix(acc=f\"{acc_batch:.4f}\")\n",
    "\n",
    "    epoch_acc = correct / len(test_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train loss: {avg_loss:.4f},  Test acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Evaluation helper\n",
    "def eval_accuracy(model, X_seq, y_seq, batch_size=256):\n",
    "    ds   = RegimeDataset(X_seq, y_seq)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    return correct / total\n",
    "\n",
    "# 2) Compute baseline accuracy\n",
    "baseline_acc = eval_accuracy(model, X_seq_test, y_seq_test)\n",
    "print(f\"Baseline Test Accuracy: {baseline_acc:.4f}\")\n",
    "\n",
    "# 3) Ablate each feature\n",
    "F = X_seq_test.shape[2]\n",
    "importances = []\n",
    "for f in tqdm(range(F), desc=\"Ablating features\"):\n",
    "    X_abl = X_seq_test.copy()\n",
    "    X_abl[..., f] = 0.0\n",
    "    acc = eval_accuracy(model, X_abl, y_seq_test)\n",
    "    importances.append(baseline_acc - acc)\n",
    "\n",
    "# 4) Build & display a sorted DataFrame\n",
    "df_imp = pd.DataFrame({\n",
    "    \"feature_idx\": np.arange(F),\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ─── assume df is your original DataFrame with MultiIndex (inst, time),\n",
    "#     and its columns are exactly the features in X_seq_* order ──────────────\n",
    "#     e.g. df.columns = [\"close\", \"log_price\", \"roll_pct_20\", …, \"vol_ratio_30\"]\n",
    "\n",
    "# 1) grab the feature names\n",
    "feature_names = list(df.columns)   # length should be F=59\n",
    "\n",
    "# 2) (re-)compute importances if you don’t already have them:\n",
    "def eval_accuracy(model, X_seq, y_seq, batch_size=256):\n",
    "    ds     = RegimeDataset(X_seq, y_seq)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    return correct / total\n",
    "\n",
    "baseline_acc = eval_accuracy(model, X_seq_test, y_seq_test)\n",
    "\n",
    "importances = []\n",
    "F = X_seq_test.shape[2]\n",
    "for f in tqdm(range(F), desc=\"Ablating features\"):\n",
    "    X_abl = X_seq_test.copy()\n",
    "    X_abl[..., f] = 0.0\n",
    "    acc = eval_accuracy(model, X_abl, y_seq_test)\n",
    "    importances.append(baseline_acc - acc)\n",
    "\n",
    "# 3) build a DataFrame and map names\n",
    "df_imp = pd.DataFrame({\n",
    "    \"feature_idx\": np.arange(F),\n",
    "    \"importance\": importances\n",
    "})\n",
    "df_imp[\"feature_name\"] = df_imp[\"feature_idx\"].map(lambda i: feature_names[i])\n",
    "\n",
    "# 4) sort by importance descending\n",
    "df_imp = df_imp.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5) show\n",
    "print(df_imp[[\"feature_idx\",\"feature_name\",\"importance\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Identify negatively‐important features\n",
    "neg_feats = df_imp.loc[df_imp[\"importance\"] < 0, \"feature_name\"].tolist()\n",
    "print(\"Removing these negatively‐important features:\", neg_feats)\n",
    "\n",
    "# 2) Drop them from your original DataFrame\n",
    "df_pruned = df.drop(columns=neg_feats)\n",
    "\n",
    "# 3) (Optional) Verify\n",
    "print(\"Original df columns:\", len(df.columns))\n",
    "print(\"Pruned df columns:  \", len(df_pruned.columns))\n",
    "print(\"Remaining features:\", df_pruned.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Split & scale exactly as you already have ───────────────────────────\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "# Map labels 2→1, else 0\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ─── 1) Build sliding windows ───────────────────────────────────────────────\n",
    "L = 60  # window length\n",
    "\n",
    "def make_windows(X_arr, y_arr, idx_index):\n",
    "    Xs, ys = [], []\n",
    "    inst_ids = idx_index.get_level_values(0)\n",
    "    for inst in np.unique(inst_ids):\n",
    "        mask = inst_ids == inst\n",
    "        arr  = X_arr[mask]       # (T_inst, n_features)\n",
    "        labs = y_arr[mask]\n",
    "        for i in range(len(arr) - L):\n",
    "            Xs.append(arr[i : i + L])\n",
    "            ys.append(labs[i + L - 1])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "X_seq_train, y_seq_train = make_windows(\n",
    "    X_train_scaled, y_train.values, X_train.index\n",
    ")\n",
    "X_seq_test, y_seq_test = make_windows(\n",
    "    X_test_scaled,  y_test.values,  X_test.index\n",
    ")\n",
    "\n",
    "print(\"Seq shapes →\", X_seq_train.shape, y_seq_train.shape)\n",
    "\n",
    "# ─── 2) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()  # (N, L, F)\n",
    "        self.y = torch.from_numpy(y).float()  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_train, y_seq_train),\n",
    "    batch_size=batch_size, shuffle=True,  num_workers=0\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    RegimeDataset(X_seq_test,  y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# ─── 3) Pure‐LSTM model ─────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = drop if num_layers>1 else 0.0\n",
    "        )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, L, F)\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = LSTMRegime(n_features=X_seq_train.shape[2]).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 4) Train ───────────────────────────────────────────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = crit(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} Train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ─── 5) Evaluation helper ───────────────────────────────────────────────────\n",
    "def eval_accuracy(model, X_seq, y_seq, batch_size=256):\n",
    "    ds     = RegimeDataset(X_seq, y_seq)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    return correct / total\n",
    "\n",
    "# ─── 6) Baseline full‐59 evaluation ─────────────────────────────────────────\n",
    "base_acc = eval_accuracy(model, X_seq_test, y_seq_test)\n",
    "print(f\"Baseline Test Accuracy (59 features): {base_acc:.4f}\")\n",
    "\n",
    "# ─── 7) Ablate the 15 negative features at inference ───────────────────────\n",
    "negative_feats = [\n",
    "    'donch_pct_50','macd_line','streak_down','velocity','adx_14',\n",
    "    'log_price','close','price_minus_sma_50','sma_12_26_diff',\n",
    "    'lr_std_100','bb_width_30','percent_b_100','std_20','std_10','lr_std_30'\n",
    "]\n",
    "all_features = list(X.columns)  # original 59 feature names\n",
    "neg_idx = [all_features.index(f) for f in negative_feats]\n",
    "\n",
    "# zero‐out those dims in the test windows\n",
    "X_seq_test_ablate = X_seq_test.copy()\n",
    "X_seq_test_ablate[..., neg_idx] = 0.0\n",
    "\n",
    "ablate_acc = eval_accuracy(model, X_seq_test_ablate, y_seq_test)\n",
    "print(f\"Ablated Test Accuracy (zeroed 15 negatives): {ablate_acc:.4f}\")\n",
    "print(f\"Δ Accuracy: {ablate_acc - base_acc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Assume you have:\n",
    "#    X: DataFrame with MultiIndex (inst, time) and 59 feature columns\n",
    "#    y: Series  with the same MultiIndex, values in {1,2}\n",
    "\n",
    "# ─── 1) Build per‐instrument arrays of exactly 750 timesteps ──────────────\n",
    "raw_X_parts = []\n",
    "raw_y_parts = []\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    arr  = grp.iloc[:750].values                # shape (750, 59)\n",
    "    labs = y.loc[grp.index[:750]].values        # shape (750,)\n",
    "    raw_X_parts.append(arr)\n",
    "    raw_y_parts.append(labs)\n",
    "\n",
    "X_all = np.stack(raw_X_parts)  # (n_inst, 750, 59)\n",
    "y_all = np.stack(raw_y_parts)  # (n_inst, 750)\n",
    "\n",
    "# ─── 2) Remap labels 1→0, 2→1 ─────────────────────────────────────────────\n",
    "y_all = (y_all == 2).astype(np.float32)  # now in {0.0,1.0}\n",
    "\n",
    "n_inst, T, F = X_all.shape\n",
    "\n",
    "# ─── 3) Fit scaler on train portion (first 450 steps) and transform all ───\n",
    "scaler = StandardScaler()\n",
    "flat_train = X_all[:, :450, :].reshape(-1, F)  # (n_inst*450, 59)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "X_all_scaled = scaler.transform(X_all.reshape(-1, F)).reshape(n_inst, T, F)\n",
    "\n",
    "# ─── 4) Create sliding windows (L=60) and split by end‐index ─────────────\n",
    "L = 60\n",
    "Xw_train, yw_train = [], []\n",
    "Xw_val,   yw_val   = [], []\n",
    "Xw_test,  yw_test  = [], []\n",
    "\n",
    "for inst_idx in range(n_inst):\n",
    "    series = X_all_scaled[inst_idx]  # (750,59)\n",
    "    labels = y_all[inst_idx]         # (750,)\n",
    "    for i in range(T - L + 1):       # 0 .. 690\n",
    "        window = series[i : i+L]     # (60,59)\n",
    "        lab    = labels[i + L - 1]   # float 0.0/1.0\n",
    "        end_t  = i + L - 1\n",
    "        if end_t < 450:\n",
    "            Xw_train.append(window); yw_train.append(lab)\n",
    "        elif end_t < 600:\n",
    "            Xw_val.append(window);   yw_val.append(lab)\n",
    "        else:\n",
    "            Xw_test.append(window);  yw_test.append(lab)\n",
    "\n",
    "X_seq_train = np.stack(Xw_train)\n",
    "y_seq_train = np.array(yw_train, dtype=np.float32)\n",
    "X_seq_val   = np.stack(Xw_val)\n",
    "y_seq_val   = np.array(yw_val,   dtype=np.float32)\n",
    "X_seq_test  = np.stack(Xw_test)\n",
    "y_seq_test  = np.array(yw_test,  dtype=np.float32)\n",
    "\n",
    "print(\"Train windows:\", X_seq_train.shape, y_seq_train.shape)\n",
    "print(\" Val windows:\", X_seq_val.shape,   y_seq_val.shape)\n",
    "print(\"Test windows:\", X_seq_test.shape,  y_seq_test.shape)\n",
    "\n",
    "# ─── 5) Dataset + DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)       # (N,60,59)\n",
    "        self.y = torch.from_numpy(y)       # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(RegimeDataset(X_seq_val,   y_seq_val),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 6) Define your LSTM regime‐classifier ─────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size  = n_features,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers  = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout     = (dropout if num_layers > 1 else 0.0)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last        = h_n[-1]               # (batch, hidden_dim)\n",
    "        h_last        = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegime(n_features=F).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 7) Train on first 450, validate on next 150 ──────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — training —\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{num_epochs}\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = crit(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # — validation (frozen) —\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb   = xb.to(device), yb.to(device)\n",
    "            plabels  = (model(xb) > 0.5).float()\n",
    "            correct += (plabels == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ─── 8) Final test evaluation ───────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb   = xb.to(device), yb.to(device)\n",
    "        plabels  = (model(xb) > 0.5).float()\n",
    "        correct += (plabels == yb).sum().item()\n",
    "        total   += yb.numel()\n",
    "test_acc = correct / total\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Assume you have:\n",
    "#    X: DataFrame with MultiIndex (inst, time) and 59 feature columns\n",
    "#    y: Series  with the same MultiIndex, values in {1,2}\n",
    "\n",
    "# ─── 1) Build per‐instrument arrays of exactly 750 timesteps ──────────────\n",
    "raw_X_parts = []\n",
    "raw_y_parts = []\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    arr  = grp.iloc[:750].values                # (750,59)\n",
    "    labs = y.loc[grp.index[:750]].values        # (750,)\n",
    "    raw_X_parts.append(arr)\n",
    "    raw_y_parts.append(labs)\n",
    "\n",
    "X_all = np.stack(raw_X_parts)  # (n_inst, 750, 59)\n",
    "y_all = np.stack(raw_y_parts)  # (n_inst, 750)\n",
    "\n",
    "n_inst, T, F = X_all.shape\n",
    "\n",
    "# ─── 2) Remap labels 1→0, 2→1 and cast to float32 ────────────────────────\n",
    "y_all = (y_all == 2).astype(np.float32)\n",
    "\n",
    "# ─── 3) Fit scaler on train portion (first 450 steps) ────────────────────\n",
    "scaler = StandardScaler()\n",
    "flat_train = X_all[:, :450, :].reshape(-1, F)   # (n_inst*450, 59)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "# apply to all data and cast to float32\n",
    "X_all_scaled = scaler.transform(X_all.reshape(-1, F)) \\\n",
    "                   .reshape(n_inst, T, F) \\\n",
    "                   .astype(np.float32)\n",
    "\n",
    "# ─── 4) Create sliding windows (L=60) and split by last index ────────────\n",
    "L = 60\n",
    "Xw_train, yw_train = [], []\n",
    "Xw_val,   yw_val   = [], []\n",
    "Xw_test,  yw_test  = [], []\n",
    "\n",
    "for inst_idx in range(n_inst):\n",
    "    series = X_all_scaled[inst_idx]  # (750,59), dtype=float32\n",
    "    labels = y_all[inst_idx]         # (750,), dtype=float32\n",
    "    for i in range(T - L + 1):\n",
    "        window = series[i : i + L]       # (60,59)\n",
    "        lab    = labels[i + L - 1]       # scalar float32\n",
    "        end_t  = i + L - 1\n",
    "        if end_t < 450:\n",
    "            Xw_train.append(window); yw_train.append(lab)\n",
    "        elif end_t < 600:\n",
    "            Xw_val.append(window);   yw_val.append(lab)\n",
    "        else:\n",
    "            Xw_test.append(window);  yw_test.append(lab)\n",
    "\n",
    "# stack and ensure float32\n",
    "X_seq_train = np.stack(Xw_train).astype(np.float32)\n",
    "y_seq_train = np.array(yw_train, dtype=np.float32)\n",
    "X_seq_val   = np.stack(Xw_val).astype(np.float32)\n",
    "y_seq_val   = np.array(yw_val,   dtype=np.float32)\n",
    "X_seq_test  = np.stack(Xw_test).astype(np.float32)\n",
    "y_seq_test  = np.array(yw_test,  dtype=np.float32)\n",
    "\n",
    "print(\"Train windows:\", X_seq_train.shape, y_seq_train.shape)\n",
    "print(\" Val windows:\", X_seq_val.shape,   y_seq_val.shape)\n",
    "print(\"Test windows:\", X_seq_test.shape,  y_seq_test.shape)\n",
    "\n",
    "# ─── 5) Dataset + DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # both X and y are np.float32\n",
    "        self.X = torch.from_numpy(X)       # yields FloatTensor\n",
    "        self.y = torch.from_numpy(y)       # FloatTensor\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(RegimeDataset(X_seq_val,   y_seq_val),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 6) LSTM model ─────────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = (dropout if num_layers>1 else 0.0)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is FloatTensor\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last        = h_n[-1]           # (batch, hidden_dim)\n",
    "        h_last        = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegime(n_features=F).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 7) Train on first 450, validate on next 150 ──────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — training —\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{num_epochs}\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = crit(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # — validation (frozen) —\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb  = xb.to(device), yb.to(device)\n",
    "            plabels = (model(xb) > 0.5).float()\n",
    "            correct += (plabels == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ─── 8) Final test evaluation ───────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb  = xb.to(device), yb.to(device)\n",
    "        plabels = (model(xb) > 0.5).float()\n",
    "        correct += (plabels == yb).sum().item()\n",
    "        total   += yb.numel()\n",
    "test_acc = correct / total\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Assume:\n",
    "#     X: DataFrame with MultiIndex (inst, time) and 59 feature columns\n",
    "#     y: Series  with same MultiIndex, values in {1,2}\n",
    "\n",
    "# ─── 1) Build per‐instrument arrays (750 timesteps each) ───────────────────\n",
    "raw_X_parts = []\n",
    "raw_y_parts = []\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    # take exactly 750 rows\n",
    "    arr  = grp.iloc[:750].values                 # shape (750, 59)\n",
    "    labs = (y.loc[grp.index[:750]] == 2).astype(np.float32).values  # map 1→0,2→1\n",
    "    raw_X_parts.append(arr)\n",
    "    raw_y_parts.append(labs)\n",
    "\n",
    "X_all = np.stack(raw_X_parts).astype(np.float32)  # (n_inst, 750, 59)\n",
    "y_all = np.stack(raw_y_parts).astype(np.float32)  # (n_inst, 750)\n",
    "\n",
    "n_inst, T, F = X_all.shape\n",
    "\n",
    "# ─── 2) Fit scaler on train portion (first 600 steps) and transform all ───\n",
    "scaler = StandardScaler()\n",
    "flat_train = X_all[:, :600, :].reshape(-1, F)    # (n_inst*600, 59)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "X_all_scaled = scaler.transform(X_all.reshape(-1, F)) \\\n",
    "                   .reshape(n_inst, T, F) \\\n",
    "                   .astype(np.float32)\n",
    "\n",
    "# ─── 3) Create sliding windows (L=60) and split into train/test ───────────\n",
    "L = 60\n",
    "Xw_train, yw_train = [], []\n",
    "Xw_test,  yw_test  = [], []\n",
    "\n",
    "for inst_idx in range(n_inst):\n",
    "    series = X_all_scaled[inst_idx]  # (750,59)\n",
    "    labels = y_all[inst_idx]         # (750,)\n",
    "    for i in range(T - L + 1):       # i = 0 ... 690\n",
    "        window = series[i : i + L]   # (60,59)\n",
    "        lab    = labels[i + L - 1]   # float 0.0 or 1.0\n",
    "        end_t  = i + L - 1           # index of that label\n",
    "        if end_t < 600:\n",
    "            Xw_train.append(window); yw_train.append(lab)\n",
    "        else:\n",
    "            Xw_test.append(window);  yw_test.append(lab)\n",
    "\n",
    "# Stack into arrays\n",
    "X_seq_train = np.stack(Xw_train)             # (n_train_windows, 60, 59)\n",
    "y_seq_train = np.array(yw_train, dtype=np.float32)\n",
    "X_seq_test  = np.stack(Xw_test)              # (n_test_windows, 60, 59)\n",
    "y_seq_test  = np.array(yw_test,  dtype=np.float32)\n",
    "\n",
    "print(\"Train windows:\", X_seq_train.shape, y_seq_train.shape)\n",
    "print(\"Test  windows:\", X_seq_test.shape,  y_seq_test.shape)\n",
    "\n",
    "# ─── 4) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)       # FloatTensor\n",
    "        self.y = torch.from_numpy(y)       # FloatTensor\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 5) Define LSTM model ───────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = (dropout if num_layers>1 else 0.0)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last        = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last        = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegime(n_features=F).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# ─── 6) Train on first 600, then test on next 150 ─────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ─── 7) Evaluate on test set ───────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb     = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct    += (pred_labels == yb).sum().item()\n",
    "        total      += yb.numel()\n",
    "test_acc = correct / total\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ─── 8) Ablation at inference ──────────────────────────────────────────────\n",
    "negative_feats = [\n",
    "    'donch_pct_50','macd_line','streak_down','velocity','adx_14',\n",
    "    'log_price','close','price_minus_sma_50','sma_12_26_diff',\n",
    "    'lr_std_100','bb_width_30','percent_b_100','std_20','std_10','lr_std_30'\n",
    "]\n",
    "all_feats = list(X.columns)\n",
    "neg_idx   = [all_feats.index(f) for f in negative_feats]\n",
    "\n",
    "X_seq_test_ablate = X_seq_test.copy()\n",
    "X_seq_test_ablate[..., neg_idx] = 0.0\n",
    "\n",
    "ablate_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_test_ablate, y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "correct_ab, total_ab = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in ablate_loader:\n",
    "        xb, yb     = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct_ab += (pred_labels == yb).sum().item()\n",
    "        total_ab   += yb.numel()\n",
    "ablate_acc = correct_ab / total_ab\n",
    "\n",
    "print(f\"Ablated Test Accuracy: {ablate_acc:.4f}\")\n",
    "print(f\"Δ Accuracy: {ablate_acc - test_acc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Assume:\n",
    "#     X: DataFrame with MultiIndex (inst, time) and 59 feature columns\n",
    "#     y: Series  with same MultiIndex, values in {1,2}\n",
    "\n",
    "# ─── 1) Build per‐instrument arrays (750 timesteps each) ───────────────────\n",
    "raw_X_parts = []\n",
    "raw_y_parts = []\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    # take exactly 750 rows\n",
    "    arr  = grp.iloc[:750].values                 # shape (750, 59)\n",
    "    labs = (y.loc[grp.index[:750]] == 2).astype(np.float32).values  # map 1→0,2→1\n",
    "    raw_X_parts.append(arr)\n",
    "    raw_y_parts.append(labs)\n",
    "\n",
    "X_all = np.stack(raw_X_parts).astype(np.float32)  # (n_inst, 750, 59)\n",
    "y_all = np.stack(raw_y_parts).astype(np.float32)  # (n_inst, 750)\n",
    "\n",
    "n_inst, T, F = X_all.shape\n",
    "\n",
    "# ─── Set split index ────────────────────────────────────────────────────────\n",
    "train_steps = 700   # use first 700 for training, last 50 for testing\n",
    "\n",
    "# ─── 2) Fit scaler on train portion (first train_steps) and transform all ──\n",
    "scaler = StandardScaler()\n",
    "flat_train = X_all[:, :train_steps, :].reshape(-1, F)    # (n_inst*700, 59)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "X_all_scaled = scaler.transform(X_all.reshape(-1, F)) \\\n",
    "                   .reshape(n_inst, T, F) \\\n",
    "                   .astype(np.float32)\n",
    "\n",
    "# ─── 3) Create sliding windows (L=60) and split into train/test ───────────\n",
    "L = 60\n",
    "Xw_train, yw_train = [], []\n",
    "Xw_test,  yw_test  = [], []\n",
    "\n",
    "for inst_idx in range(n_inst):\n",
    "    series = X_all_scaled[inst_idx]\n",
    "    labels = y_all[inst_idx]\n",
    "    for i in range(T - L + 1):\n",
    "        window = series[i : i + L]\n",
    "        lab    = labels[i + L - 1]\n",
    "        end_t  = i + L - 1\n",
    "\n",
    "        if end_t < train_steps:\n",
    "            Xw_train.append(window); yw_train.append(lab)\n",
    "        else:\n",
    "            Xw_test.append(window);  yw_test.append(lab)\n",
    "\n",
    "# — sanity check —\n",
    "print(f\"T={T}, L={L}, train_steps={train_steps}\")\n",
    "print(f\"→ collected {len(Xw_train)} train windows\")\n",
    "print(f\"→ collected {len(Xw_test)} test  windows\")\n",
    "\n",
    "\n",
    "# ─── 4) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)       # FloatTensor\n",
    "        self.y = torch.from_numpy(y)       # FloatTensor\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 5) Define LSTM model ───────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = (dropout if num_layers>1 else 0.0)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last        = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last        = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegime(n_features=F).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# ─── 6) Train on first 600, then test on next 150 ─────────────────────────\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ─── 7) Evaluate on test set ───────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb     = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct    += (pred_labels == yb).sum().item()\n",
    "        total      += yb.numel()\n",
    "test_acc = correct / total\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ─── 8) Ablation at inference ──────────────────────────────────────────────\n",
    "negative_feats = [\n",
    "    'donch_pct_50','macd_line','streak_down','velocity','adx_14',\n",
    "    'log_price','close','price_minus_sma_50','sma_12_26_diff',\n",
    "    'lr_std_100','bb_width_30','percent_b_100','std_20','std_10','lr_std_30'\n",
    "]\n",
    "all_feats = list(X.columns)\n",
    "neg_idx   = [all_feats.index(f) for f in negative_feats]\n",
    "\n",
    "X_seq_test_ablate = X_seq_test.copy()\n",
    "X_seq_test_ablate[..., neg_idx] = 0.0\n",
    "\n",
    "ablate_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_test_ablate, y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "correct_ab, total_ab = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in ablate_loader:\n",
    "        xb, yb     = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct_ab += (pred_labels == yb).sum().item()\n",
    "        total_ab   += yb.numel()\n",
    "ablate_acc = correct_ab / total_ab\n",
    "\n",
    "print(f\"Ablated Test Accuracy: {ablate_acc:.4f}\")\n",
    "print(f\"Δ Accuracy: {ablate_acc - test_acc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 0) Assume:\n",
    "#     X: DataFrame with MultiIndex (inst, time) and 59 feature columns\n",
    "#     y: Series  with same MultiIndex, values in {1,2}\n",
    "\n",
    "# ─── 1) Build per‐instrument arrays (750 timesteps each) ───────────────────\n",
    "raw_X_parts = []\n",
    "raw_y_parts = []\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    # take exactly 750 rows\n",
    "    arr  = grp.iloc[:750].values                 # shape (750, 59)\n",
    "    labs = (y.loc[grp.index[:750]] == 2).astype(np.float32).values  # map 1→0,2→1\n",
    "    raw_X_parts.append(arr)\n",
    "    raw_y_parts.append(labs)\n",
    "\n",
    "X_all = np.stack(raw_X_parts).astype(np.float32)  # (n_inst, 750, 59)\n",
    "y_all = np.stack(raw_y_parts).astype(np.float32)  # (n_inst, 750)\n",
    "\n",
    "n_inst, T, F = X_all.shape\n",
    "\n",
    "# ─── Set split index ────────────────────────────────────────────────────────\n",
    "train_steps = 700   # use first 700 for training, last 50 for testing\n",
    "\n",
    "# ─── 2) Fit scaler on train portion (first train_steps) and transform all ──\n",
    "scaler = StandardScaler()\n",
    "flat_train = X_all[:, :train_steps, :].reshape(-1, F)    # (n_inst*700, 59)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "X_all_scaled = scaler.transform(X_all.reshape(-1, F)) \\\n",
    "                   .reshape(n_inst, T, F) \\\n",
    "                   .astype(np.float32)\n",
    "\n",
    "# ─── 3) Create sliding windows (L=60) and split into train/test ───────────\n",
    "L = 60\n",
    "Xw_train, yw_train = [], []\n",
    "Xw_test,  yw_test  = [], []\n",
    "\n",
    "for inst_idx in range(n_inst):\n",
    "    series = X_all_scaled[inst_idx]\n",
    "    labels = y_all[inst_idx]\n",
    "    for i in range(T - L + 1):\n",
    "        window = series[i : i + L]\n",
    "        lab    = labels[i + L - 1]\n",
    "        end_t  = i + L - 1\n",
    "\n",
    "        if end_t < train_steps:\n",
    "            Xw_train.append(window); yw_train.append(lab)\n",
    "        else:\n",
    "            Xw_test.append(window);  yw_test.append(lab)\n",
    "\n",
    "# — sanity check —\n",
    "print(f\"T={T}, L={L}, train_steps={train_steps}\")\n",
    "print(f\"→ collected {len(Xw_train)} train windows\")\n",
    "print(f\"→ collected {len(Xw_test)} test  windows\")\n",
    "\n",
    "\n",
    "# ─── 4) Dataset & DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)       # FloatTensor\n",
    "        self.y = torch.from_numpy(y)       # FloatTensor\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 5) Define LSTM model ───────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = (dropout if num_layers>1 else 0.0)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last        = h_n[-1]            # (batch, hidden_dim)\n",
    "        h_last        = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegime(n_features=F).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# ─── 6) Train on first 600, then test on next 150 ─────────────────────────\n",
    "num_epochs = 30\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ─── 7) Evaluate on test set ───────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb     = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct    += (pred_labels == yb).sum().item()\n",
    "        total      += yb.numel()\n",
    "test_acc = correct / total\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ─── 8) Ablation at inference ──────────────────────────────────────────────\n",
    "negative_feats = [\n",
    "    'donch_pct_50','macd_line','streak_down','velocity','adx_14',\n",
    "    'log_price','close','price_minus_sma_50','sma_12_26_diff',\n",
    "    'lr_std_100','bb_width_30','percent_b_100','std_20','std_10','lr_std_30'\n",
    "]\n",
    "all_feats = list(X.columns)\n",
    "neg_idx   = [all_feats.index(f) for f in negative_feats]\n",
    "\n",
    "X_seq_test_ablate = X_seq_test.copy()\n",
    "X_seq_test_ablate[..., neg_idx] = 0.0\n",
    "\n",
    "ablate_loader = DataLoader(\n",
    "    RegimeDataset(X_seq_test_ablate, y_seq_test),\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "correct_ab, total_ab = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in ablate_loader:\n",
    "        xb, yb     = xb.to(device), yb.to(device)\n",
    "        pred_labels = (model(xb) > 0.5).float()\n",
    "        correct_ab += (pred_labels == yb).sum().item()\n",
    "        total_ab   += yb.numel()\n",
    "ablate_acc = correct_ab / total_ab\n",
    "\n",
    "print(f\"Ablated Test Accuracy: {ablate_acc:.4f}\")\n",
    "print(f\"Δ Accuracy: {ablate_acc - test_acc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
