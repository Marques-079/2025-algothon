{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ── 1) Hyperparameters ─────────────────────────────────────────────────────────\n",
    "TRAIN_LEN   = 400    # train on steps [0..399]\n",
    "BATCH_SIZE  = 10\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "LR          = 1e-3\n",
    "NUM_EPOCHS  = 15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ── 2) Load, clean & shift labels ───────────────────────────────────────────────\n",
    "df = pd.read_csv(\"features_all_models4.csv\")\n",
    "#  a) drop first 100 null‐warmups\n",
    "df = df.groupby(\"inst\", group_keys=False) \\\n",
    "       .apply(lambda g: g.iloc[100:]) \\\n",
    "       .reset_index(drop=True)\n",
    "#  b) shift labels *one day ahead* per instrument\n",
    "df[\"true_regime\"] = df.groupby(\"inst\")[\"true_regime\"].shift(-1)\n",
    "#  c) drop the last row of each instrument (now NaN label)\n",
    "df = df.dropna(subset=[\"true_regime\"]).reset_index(drop=True)\n",
    "df[\"true_regime\"] = df[\"true_regime\"].astype(int)\n",
    "\n",
    "price_df = pd.read_csv(\"prices.txt\", sep=r\"\\s+\", header=None)\n",
    "\n",
    "# ── 3) Determine sequence length & build arrays ────────────────────────────────\n",
    "seq_lens = df.groupby(\"inst\").size()\n",
    "SEQ_LEN  = int(seq_lens.max())\n",
    "print(\"Detected sequence length per instrument (post-shift):\", SEQ_LEN)\n",
    "\n",
    "n_inst    = df[\"inst\"].nunique()\n",
    "feat_cols = [c for c in df.columns if c not in (\"inst\",\"time\",\"true_regime\")]\n",
    "\n",
    "# initialize\n",
    "X = np.zeros((n_inst, SEQ_LEN, len(feat_cols)), dtype=np.float32)\n",
    "Y = np.zeros((n_inst, SEQ_LEN),               dtype=np.int64)\n",
    "\n",
    "# fill per-instrument\n",
    "for inst in range(n_inst):\n",
    "    sub = df[df[\"inst\"]==inst].reset_index(drop=True)\n",
    "    assert len(sub)==SEQ_LEN\n",
    "    X[inst] = sub[feat_cols].values\n",
    "    Y[inst] = sub[\"true_regime\"].values\n",
    "\n",
    "NUM_TAGS = int(Y.max()) + 1\n",
    "\n",
    "# ── 4) Split into train vs. test windows ──────────────────────────────────────\n",
    "X_train = torch.tensor(X[:, :TRAIN_LEN, :])\n",
    "Y_train = torch.tensor(Y[:, :TRAIN_LEN])\n",
    "X_test  = torch.tensor(X[:, TRAIN_LEN:, :])\n",
    "Y_test  = Y[:, TRAIN_LEN:]           # numpy for metrics & plotting\n",
    "LEN_TEST = SEQ_LEN - TRAIN_LEN\n",
    "\n",
    "# ── 5) Dataset & DataLoader ───────────────────────────────────────────────────\n",
    "class SeqTagDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X; self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(SeqTagDataset(X_train, Y_train),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ── 6) BiLSTM tagger ───────────────────────────────────────────────────────────\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, num_layers, num_tags, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(feat_dim, hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_tags)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)     # (B, T, 2H)\n",
    "        return self.fc(out)       # (B, T, num_tags)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = BiLSTMTagger(\n",
    "    feat_dim   = X.shape[2],\n",
    "    hidden_dim = HIDDEN_SIZE,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    num_tags   = NUM_TAGS,\n",
    "    dropout    = DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ── 7) Train ───────────────────────────────────────────────────────────────────\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for feats, tags in train_loader:\n",
    "        feats, tags = feats.to(device), tags.to(device)\n",
    "        logits      = model(feats)              # (B, T, C)\n",
    "        loss        = criterion(\n",
    "            logits.view(-1, NUM_TAGS),         # (B*T, C)\n",
    "            tags.view(-1)                      # (B*T,)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch:02d} — Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ── 8) Inference + metrics + two‐panel plotting ───────────────────────────────\n",
    "def get_segments(reg):\n",
    "    changes = np.flatnonzero(reg[1:] != reg[:-1])\n",
    "    starts  = np.concatenate(([0], changes+1))\n",
    "    ends    = np.concatenate((changes, [len(reg)-1]))\n",
    "    return list(zip(starts, ends, reg[starts]))\n",
    "\n",
    "true_cmap = ListedColormap([\"#ff0000\",\"#808080\",\"#00ff00\"])\n",
    "pred_cmap = ListedColormap([\"#cc0000\",\"#444444\",\"#00cc00\"])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_test = model(X_test.to(device))      # (50, LEN_TEST, C)\n",
    "    preds_test  = logits_test.argmax(dim=2).cpu().numpy()\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    true_seq = Y_test[inst]\n",
    "    pred_seq = preds_test[inst]\n",
    "    price    = price_df.iloc[100+TRAIN_LEN:100+TRAIN_LEN+LEN_TEST, inst].values\n",
    "\n",
    "    acc = (pred_seq == true_seq).mean()\n",
    "    print(f\"Inst {inst:02d} Test acc: {acc:.3f}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,6))\n",
    "\n",
    "    # TRUE regimes\n",
    "    for s,e,lbl in get_segments(true_seq):\n",
    "        ax1.axvspan(s, e, color=true_cmap(lbl), alpha=0.5, linewidth=0)\n",
    "    ax1.plot(price, 'k-', label='Price')\n",
    "    ax1.set_title(f\"Inst {inst} — TRUE regimes (t={TRAIN_LEN}→end)\")\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # PREDICTED regimes\n",
    "    for s,e,lbl in get_segments(pred_seq):\n",
    "        ax2.axvspan(s, e, color=pred_cmap(lbl), alpha=0.5, linewidth=0)\n",
    "    ax2.plot(price, 'k-', label='Price')\n",
    "    ax2.set_title(f\"Inst {inst} — PREDICTED regimes (t={TRAIN_LEN}→end)\")\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ── 1) Hyperparameters ─────────────────────────────────────────────────────────\n",
    "TRAIN_LEN   = 400    # train on steps [0..399]\n",
    "BATCH_SIZE  = 10\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "LR          = 1e-3\n",
    "NUM_EPOCHS  = 15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ── 2) Load, clean & shift labels ───────────────────────────────────────────────\n",
    "df = pd.read_csv(\"features_all_modelsFINAL.csv\")\n",
    "#  a) drop first 100 null‐warmups\n",
    "df = df.groupby(\"inst\", group_keys=False) \\\n",
    "       .apply(lambda g: g.iloc[100:]) \\\n",
    "       .reset_index(drop=True)\n",
    "#  b) shift labels *one day ahead* per instrument\n",
    "df[\"true_regime\"] = df.groupby(\"inst\")[\"true_regime\"].shift(-1)\n",
    "#  c) drop the last row of each instrument (now NaN label)\n",
    "df = df.dropna(subset=[\"true_regime\"]).reset_index(drop=True)\n",
    "df[\"true_regime\"] = df[\"true_regime\"].astype(int)\n",
    "\n",
    "price_df = pd.read_csv(\"prices.txt\", sep=r\"\\s+\", header=None)\n",
    "\n",
    "# ── 3) Determine sequence length & build arrays ────────────────────────────────\n",
    "seq_lens = df.groupby(\"inst\").size()\n",
    "SEQ_LEN  = int(seq_lens.max())\n",
    "print(\"Detected sequence length per instrument (post-shift):\", SEQ_LEN)\n",
    "\n",
    "n_inst    = df[\"inst\"].nunique()\n",
    "feat_cols = [c for c in df.columns if c not in (\"inst\",\"time\",\"true_regime\")]\n",
    "\n",
    "# initialize\n",
    "X = np.zeros((n_inst, SEQ_LEN, len(feat_cols)), dtype=np.float32)\n",
    "Y = np.zeros((n_inst, SEQ_LEN),               dtype=np.int64)\n",
    "\n",
    "# fill per-instrument\n",
    "for inst in range(n_inst):\n",
    "    sub = df[df[\"inst\"]==inst].reset_index(drop=True)\n",
    "    assert len(sub)==SEQ_LEN\n",
    "    X[inst] = sub[feat_cols].values\n",
    "    Y[inst] = sub[\"true_regime\"].values\n",
    "\n",
    "NUM_TAGS = int(Y.max()) + 1\n",
    "\n",
    "# ── 4) Split into train vs. test windows ──────────────────────────────────────\n",
    "X_train = torch.tensor(X[:, :TRAIN_LEN, :])\n",
    "Y_train = torch.tensor(Y[:, :TRAIN_LEN])\n",
    "X_test  = torch.tensor(X[:, TRAIN_LEN:, :])\n",
    "Y_test  = Y[:, TRAIN_LEN:]           # numpy for metrics & plotting\n",
    "LEN_TEST = SEQ_LEN - TRAIN_LEN\n",
    "\n",
    "# ── 5) Dataset & DataLoader ───────────────────────────────────────────────────\n",
    "class SeqTagDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X; self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(SeqTagDataset(X_train, Y_train),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ── 6) BiLSTM tagger ───────────────────────────────────────────────────────────\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, num_layers, num_tags, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(feat_dim, hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_tags)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)     # (B, T, 2H)\n",
    "        return self.fc(out)       # (B, T, num_tags)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = BiLSTMTagger(\n",
    "    feat_dim   = X.shape[2],\n",
    "    hidden_dim = HIDDEN_SIZE,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    num_tags   = NUM_TAGS,\n",
    "    dropout    = DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ── 7) Train ───────────────────────────────────────────────────────────────────\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for feats, tags in train_loader:\n",
    "        feats, tags = feats.to(device), tags.to(device)\n",
    "        logits      = model(feats)              # (B, T, C)\n",
    "        loss        = criterion(\n",
    "            logits.view(-1, NUM_TAGS),         # (B*T, C)\n",
    "            tags.view(-1)                      # (B*T,)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch:02d} — Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ── 8) Inference + metrics + two‐panel plotting ───────────────────────────────\n",
    "def get_segments(reg):\n",
    "    changes = np.flatnonzero(reg[1:] != reg[:-1])\n",
    "    starts  = np.concatenate(([0], changes+1))\n",
    "    ends    = np.concatenate((changes, [len(reg)-1]))\n",
    "    return list(zip(starts, ends, reg[starts]))\n",
    "\n",
    "true_cmap = ListedColormap([\"#ff0000\",\"#808080\",\"#00ff00\"])\n",
    "pred_cmap = ListedColormap([\"#cc0000\",\"#444444\",\"#00cc00\"])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_test = model(X_test.to(device))      # (50, LEN_TEST, C)\n",
    "    preds_test  = logits_test.argmax(dim=2).cpu().numpy()\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    true_seq = Y_test[inst]\n",
    "    pred_seq = preds_test[inst]\n",
    "    price    = price_df.iloc[100+TRAIN_LEN:100+TRAIN_LEN+LEN_TEST, inst].values\n",
    "\n",
    "    acc = (pred_seq == true_seq).mean()\n",
    "    print(f\"Inst {inst:02d} Test acc: {acc:.3f}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,6))\n",
    "\n",
    "    # TRUE regimes\n",
    "    for s,e,lbl in get_segments(true_seq):\n",
    "        ax1.axvspan(s, e, color=true_cmap(lbl), alpha=0.5, linewidth=0)\n",
    "    ax1.plot(price, 'k-', label='Price')\n",
    "    ax1.set_title(f\"Inst {inst} — TRUE regimes (t={TRAIN_LEN}→end)\")\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # PREDICTED regimes\n",
    "    for s,e,lbl in get_segments(pred_seq):\n",
    "        ax2.axvspan(s, e, color=pred_cmap(lbl), alpha=0.5, linewidth=0)\n",
    "    ax2.plot(price, 'k-', label='Price')\n",
    "    ax2.set_title(f\"Inst {inst} — PREDICTED regimes (t={TRAIN_LEN}→end)\")\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Save\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# after training, run:\n",
    "save_path = \"bilstm_regime_model.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"✅ Model weights saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
