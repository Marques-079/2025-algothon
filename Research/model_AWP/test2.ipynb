{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from AWP_inference import load_model, predict_regimes\n",
    "\n",
    "# — assume you have a function like this somewhere —\n",
    "#    it takes raw prices shape (n_inst,100) and returns\n",
    "#    features shape (n_inst, 20, D)\n",
    "def compute_feature_window(price_window: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    price_window: np.ndarray, shape (n_inst, 100)\n",
    "    returns: np.ndarray, shape (n_inst, 20, D)\n",
    "    \"\"\"\n",
    "    # your code here: reimplement the same MA/EMA/slope/vol/etc logic\n",
    "    # but only on these 100 points. You must return a 3D array.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def online_inference(price_matrix: np.ndarray,\n",
    "                     checkpoint: str=\"bilstm_tagger.pth\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    price_matrix: np.ndarray, shape (n_inst, T)\n",
    "    checkpoint:   path to your trained PyTorch .pth file\n",
    "    returns:      np.ndarray, shape (n_inst,), the regime at time T for each instrument\n",
    "    \"\"\"\n",
    "    n_inst, T = price_matrix.shape\n",
    "\n",
    "    # 1) need at least 100 points\n",
    "    if T < 100:\n",
    "        return np.full((n_inst,), -999, dtype=int)\n",
    "\n",
    "    # 2) slice out the last-100 window\n",
    "    last100 = price_matrix[:, -100:]   # (n_inst,100)\n",
    "\n",
    "    # 3) build your feature‐sequence for each instrument\n",
    "    #    must produce shape (n_inst,20,D)\n",
    "    feat_seqs = compute_feature_window(last100)\n",
    "\n",
    "    # 4) load your frozen LSTM once\n",
    "    model, device = load_model(checkpoint)\n",
    "\n",
    "    # 5) run it per instrument, take the last step’s label\n",
    "    preds = np.zeros((n_inst,), dtype=int)\n",
    "    for i in range(n_inst):\n",
    "        seq_preds = predict_regimes(model, device, feat_seqs[i])\n",
    "        preds[i]  = seq_preds[-1]\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWP_pipeline.py\n",
    "# Builds features CSV and provides streaming warmup + inference pipeline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from precision_labeller import plot_all_regimes_long\n",
    "from AWP_inference import load_model, predict_regimes\n",
    "\n",
    "\n",
    "# ── Part B: Streaming warmup + efficient inference pipeline ─────────────────────────\n",
    "class StreamingRegimePredictor:\n",
    "    \"\"\"\n",
    "    Stream in price data for multiple instruments, warm up caches,\n",
    "    and emit a 1D numpy array of regime predictions (one per instrument)\n",
    "    once both price (100) and feature (20) caches are full.  Until then,\n",
    "    returns an array filled with -999.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_name: str, features_csv: str):\n",
    "        # load frozen LSTM tagger\n",
    "        self.model, self.device = load_model(checkpoint_name)\n",
    "        # load static features (post-100 warm-up) for all instruments\n",
    "        df = pd.read_csv(features_csv)\n",
    "        df = df.groupby(\"inst\", group_keys=False).apply(lambda g: g).reset_index(drop=True)\n",
    "        # pivot into array shape (n_inst, seq_len, D)\n",
    "        feat_cols = [c for c in df.columns if c not in (\"inst\",\"time\",\"true_regime\")]\n",
    "        grouped = df.groupby(\"inst\")\n",
    "        self.static_feats = np.stack([grouped.get_group(i)[feat_cols].values\n",
    "                                      for i in sorted(grouped.groups)])\n",
    "        # cache placeholders\n",
    "        self.price_cache = None     # shape: (n_inst, 100)\n",
    "        self.feature_cache = None   # shape: (n_inst, 20, D)\n",
    "        self.n_inst, self.seq_len, self.D = self.static_feats.shape\n",
    "\n",
    "    def step(self, price_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Ingest the full historical price_matrix of shape (n_inst, T).\n",
    "        If T < 120, returns array([-999]). Once T >= 120:\n",
    "         - initialize or roll price_cache to keep last 100 prices\n",
    "         - initialize or roll feature_cache to keep last 20 feature rows\n",
    "         - after updating feature_cache, run inference on each instrument\n",
    "\n",
    "        Returns:\n",
    "          preds: np.ndarray of shape (n_inst,) of current regime labels,\n",
    "                 or np.full((n_inst,), -999) if not warmed up.\n",
    "        \"\"\"\n",
    "        if price_matrix.ndim != 2:\n",
    "            raise ValueError(\"price_matrix must be 2D: (n_inst, T)\")\n",
    "        n_inst, T = price_matrix.shape\n",
    "        # not enough history\n",
    "        if T < 120:\n",
    "            return np.full((n_inst,), -999, dtype=int)\n",
    "        # initialize or update price cache\n",
    "        last_prices = price_matrix[:, -100:]\n",
    "        if self.price_cache is None:\n",
    "            self.price_cache = last_prices.copy()\n",
    "        else:\n",
    "            # roll then append\n",
    "            self.price_cache = np.roll(self.price_cache, -1, axis=1)\n",
    "            self.price_cache[:, -1] = last_prices[:, -1]\n",
    "        # determine static feature index for this new timestep\n",
    "        static_idx = T - 100 - 1  # zero-based index into static_feats axis1\n",
    "        if not (0 <= static_idx < self.seq_len):\n",
    "            # out of bounds static features\n",
    "            return np.full((n_inst,), -999, dtype=int)\n",
    "        # get new feature row for all instruments\n",
    "        new_feat = self.static_feats[:, static_idx, :]  # (n_inst, D)\n",
    "        # initialize or update feature cache\n",
    "        if self.feature_cache is None:\n",
    "            # first fill: take the first 20 rows of static_feats\n",
    "            self.feature_cache = self.static_feats[:, :20, :].copy()\n",
    "        else:\n",
    "            self.feature_cache = np.roll(self.feature_cache, -1, axis=1)\n",
    "            self.feature_cache[:, -1, :] = new_feat\n",
    "        # now caches are warm: run inference\n",
    "        preds = np.zeros(self.n_inst, dtype=int)\n",
    "        for i in range(self.n_inst):\n",
    "            feats_i = self.feature_cache[i]  # shape (20, D)\n",
    "            preds[i] = predict_regimes(self.model, self.device, feats_i)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from AWP_inference import load_model, predict_regimes\n",
    "\n",
    "# ─── 0) LOAD MODEL ──────────────────────────────────────────────────────────────\n",
    "model, device = load_model(\"bilstm_tagger.pth\")\n",
    "\n",
    "# ─── 1) LOAD PRECOMPUTED FEATURES ──────────────────────────────────────────────\n",
    "# (must match the CSV you used in offline training)\n",
    "df_feat = pd.read_csv(\"features_all_models4.csv\")\n",
    "df_feat = (\n",
    "    df_feat\n",
    "    .groupby(\"inst\", group_keys=False)\n",
    "    .apply(lambda g: g.iloc[100:])   # drop the same 100 warm-ups\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "feat_cols   = [c for c in df_feat.columns if c not in (\"inst\",\"time\",\"true_regime\")]\n",
    "grouped     = df_feat.groupby(\"inst\")\n",
    "static_feats = np.stack([grouped.get_group(i)[feat_cols].values\n",
    "                         for i in sorted(grouped.groups)])\n",
    "# static_feats.shape == (n_inst, T_feat, D)\n",
    "n_inst, T_feat, D = static_feats.shape\n",
    "\n",
    "# ─── 2) DEFINE STEP FUNCTION ───────────────────────────────────────────────────\n",
    "def step_from_prices(prices_2d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    prices_2d: shape (n_inst, T)\n",
    "    returns:   shape (n_inst,) with the predicted regime at time T for each instrument\n",
    "               or -999 if T < 100\n",
    "    \"\"\"\n",
    "    n_inst_, T = prices_2d.shape\n",
    "    assert n_inst_ == n_inst, f\"expected {n_inst} instruments\"\n",
    "    # need at least 100 points to index static_feats\n",
    "    if T < 100:\n",
    "        return np.full(n_inst, -999, dtype=int)\n",
    "    # static index aligned so that static_feats[:,0] is features at t=100\n",
    "    static_idx = T - 100 - 1\n",
    "    if static_idx < 19 or static_idx >= T_feat:\n",
    "        # not enough to fill 20-step LSTM window\n",
    "        return np.full(n_inst, -999, dtype=int)\n",
    "    # grab the 20×D window for each instrument\n",
    "    feat_window = static_feats[:, static_idx-19:static_idx+1, :]  # shape (n_inst,20,D)\n",
    "\n",
    "    # run inference per instrument\n",
    "    preds = np.zeros(n_inst, dtype=int)\n",
    "    for i in range(n_inst):\n",
    "        seq_preds = predict_regimes(model, device, feat_window[i])  # length 20\n",
    "        preds[i]  = seq_preds[-1]  # take only the final label\n",
    "    return preds\n",
    "\n",
    "# ─── 3) SIMULATE STREAMING ────────────────────────────────────────────────────\n",
    "# read raw prices and transpose → (n_inst, T_raw)\n",
    "price_df = pd.read_csv(\"prices.txt\", sep=r\"\\s+\", header=None)\n",
    "prices   = price_df.values.T\n",
    "n_inst_, T_raw = prices.shape\n",
    "\n",
    "print(f\"Streaming from t=1 to t={T_raw}  (n_inst={n_inst})\")\n",
    "for t in range(1, T_raw+1):\n",
    "    preds = step_from_prices(prices[:, :t])\n",
    "    print(f\"t={t:3d}: {preds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from AWP_pipeline import compute_features_for_latest  # adjust import as needed\n",
    "\n",
    "# 1) Load the price data: rows = timesteps, columns = instruments\n",
    "df = pd.read_csv('prices.txt', sep=r'\\s+', header=None)\n",
    "\n",
    "# 2) Convert to a NumPy array of shape (n_inst, T)\n",
    "#    df.values is (T, n_inst) so we transpose\n",
    "prices = df.values.T\n",
    "\n",
    "n_inst, T = prices.shape\n",
    "\n",
    "# 3) Slide a 100-bar window (the \"trailing 99 + current bar\") from t=99 up to T-1\n",
    "for t in range(99, T):\n",
    "    # extract a window of shape (n_inst, 100)\n",
    "    window = prices[:, t-99:t+1]\n",
    "    \n",
    "    # compute the 9 regimes per instrument at this timestep\n",
    "    feats = compute_features_for_latest(window)  # returns (n_inst, 9)\n",
    "    \n",
    "    # print them; you could also store them in a list or DataFrame\n",
    "    print(f\"t={t:4d} →\")\n",
    "    for inst_idx, inst_feats in enumerate(feats):\n",
    "        print(f\"  Inst {inst_idx:2d}: {inst_feats.tolist()}\")\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from precision_labeller import plot_all_regimes_long\n",
    "\n",
    "\n",
    "def build_feature_matrix_from_array(\n",
    "    prices_array: np.ndarray,\n",
    "    output_csv: str = \"features_all_models_from_array.csv\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build a feature matrix for multiple regime models from a 2D numpy array of prices.\n",
    "\n",
    "    Args:\n",
    "        prices_array: 2D array of shape (T, n_inst), where each column is one instrument's price series.\n",
    "        output_csv: Path to write the resulting CSV file.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if prices_array.ndim != 2:\n",
    "        raise ValueError(\"prices_array must be a 2D array of shape (T, n_inst)\")\n",
    "\n",
    "    T, n_inst = prices_array.shape\n",
    "    all_rows = []\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        close = pd.Series(prices_array[:, inst])\n",
    "        high = close.copy()\n",
    "        low = close.copy()\n",
    "\n",
    "        # Ground truth regimes for full series\n",
    "        true_regs = plot_all_regimes_long(end_point=T, plot_graph=False, inst=inst)\n",
    "        true_regs = pd.Series(true_regs, name=\"true_regime\")\n",
    "\n",
    "        # Prepare DataFrame for this instrument\n",
    "        features = pd.DataFrame(index=np.arange(T))\n",
    "        logp = np.log(close)\n",
    "\n",
    "        # MA-based regime\n",
    "        ma_s = logp.rolling(window=5, min_periods=1).mean()\n",
    "        ma_l = logp.rolling(window=70, min_periods=1).mean()\n",
    "        features[\"ma_reg\"] = np.where(ma_l > ma_s, 0, 2)\n",
    "\n",
    "        # EMA-based regime\n",
    "        ema_s = logp.ewm(span=5, adjust=False).mean()\n",
    "        ema_l = logp.ewm(span=50, adjust=False).mean()\n",
    "        features[\"ema_reg\"] = np.where(ema_s > ema_l, 2, 0)\n",
    "\n",
    "        # Slope/Vol regime (only computed where full window available)\n",
    "        sv_df = compute_slope_vol(close, slope_win=30, vol_win=100).dropna()\n",
    "        idx = sv_df.index  # valid window endpoints\n",
    "        slope = sv_df[\"slope\"]\n",
    "        vol = sv_df[\"vol\"]\n",
    "        median_vol = vol.median()\n",
    "        # Initialize column with NaN so early indexes remain NaN\n",
    "        features[\"slope_vol_reg\"] = np.nan\n",
    "        features.loc[idx, \"slope_vol_reg\"] = np.where(\n",
    "            (slope > 0) & (vol < median_vol), 2, 0\n",
    "        )\n",
    "\n",
    "        # MACD regime\n",
    "        ema_s2 = logp.ewm(span=50, adjust=False).mean()\n",
    "        ema_l2 = logp.ewm(span=90, adjust=False).mean()\n",
    "        macd = ema_s2 - ema_l2\n",
    "        signal = macd.ewm(span=40, adjust=False).mean()\n",
    "        features[\"macd_reg\"] = np.where(macd > signal, 2, 0)\n",
    "\n",
    "        # Kalman regime\n",
    "        x_est = np.zeros(T)\n",
    "        P = np.zeros(T)\n",
    "        x_est[0], P[0] = logp.iloc[0], 1.0\n",
    "        for t in range(1, T):\n",
    "            x_pred = x_est[t - 1]\n",
    "            P_pred = P[t - 1] + 0.01\n",
    "            K = P_pred / (P_pred + 10.0)\n",
    "            x_est[t] = x_pred + K * (logp.iloc[t] - x_pred)\n",
    "            P[t] = (1 - K) * P_pred\n",
    "        features[\"kalman_reg\"] = np.where(\n",
    "            logp > x_est, 2, 0\n",
    "        )\n",
    "\n",
    "        # Fibonacci regime\n",
    "        high_win = close.rolling(window=50, min_periods=50).max()\n",
    "        low_win = close.rolling(window=50, min_periods=50).min()\n",
    "        fib_range = high_win - low_win\n",
    "        lower = low_win + 0.618 * fib_range\n",
    "        upper = low_win + 0.786 * fib_range\n",
    "        features[\"fib_reg\"] = np.where(\n",
    "            close > upper, 2,\n",
    "            np.where(close < lower, 0, 1)\n",
    "        )\n",
    "\n",
    "        # PSAR regime\n",
    "        psar = np.zeros(T)\n",
    "        trend_up = True\n",
    "        af, max_step = 0.01, 0.10\n",
    "        ep = high.iloc[0]\n",
    "        psar[0] = low.iloc[0]\n",
    "        for t in range(1, T):\n",
    "            prev = psar[t - 1]\n",
    "            psar[t] = prev + af * (ep - prev)\n",
    "            if trend_up:\n",
    "                if low.iloc[t] < psar[t]:\n",
    "                    trend_up = False\n",
    "                    psar[t], ep, af = ep, low.iloc[t], 0.01\n",
    "                elif high.iloc[t] > ep:\n",
    "                    ep = high.iloc[t]\n",
    "                    af = min(af + 0.01, max_step)\n",
    "            else:\n",
    "                if high.iloc[t] > psar[t]:\n",
    "                    trend_up = True\n",
    "                    psar[t], ep, af = ep, high.iloc[t], 0.01\n",
    "                elif low.iloc[t] < ep:\n",
    "                    ep = low.iloc[t]\n",
    "                    af = min(af + 0.01, max_step)\n",
    "        features[\"psar_reg\"] = np.where(\n",
    "            close > psar, 2, 0\n",
    "        )\n",
    "\n",
    "        # Z-score regime\n",
    "        ma90 = close.rolling(window=90, min_periods=90).mean()\n",
    "        sd90 = close.rolling(window=90, min_periods=90).std()\n",
    "        z = (close - ma90) / sd90\n",
    "        features[\"zscore_reg\"] = np.where(\n",
    "            z > 0.5, 2,\n",
    "            np.where(z < -0.5, 0, 1)\n",
    "        )\n",
    "\n",
    "        # Weighted-return regime\n",
    "        r = close.pct_change()\n",
    "        weights = np.arange(1, 46) ** 0.5\n",
    "        weights /= weights.sum()\n",
    "        wr = r.rolling(window=45, min_periods=45).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "        features[\"wret_reg\"] = np.where(\n",
    "            wr > 0, 2,\n",
    "            np.where(wr < 0, 0, 1)\n",
    "        )\n",
    "\n",
    "        # Final columns\n",
    "        features[\"true_regime\"] = true_regs\n",
    "        features[\"inst\"] = inst\n",
    "        features[\"time\"] = features.index\n",
    "\n",
    "        all_rows.append(features.reset_index(drop=True))\n",
    "\n",
    "    final_df = pd.concat(all_rows, ignore_index=True)\n",
    "    final_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Features written to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_prices_to_array(price_file: str, delim_whitespace: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a text file where each column is one instrument and each row is a timestep,\n",
    "    and returns a 2D NumPy array of shape (T, n_inst).\n",
    "\n",
    "    Args:\n",
    "        price_file: Path to your prices.txt file.\n",
    "        delim_whitespace: If True, splits on any whitespace (default). If False,\n",
    "                         you can pass a specific delimiter via np.loadtxt's delimiter kwarg.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (T, n_inst), where T is the number of timesteps.\n",
    "    \"\"\"\n",
    "    if delim_whitespace:\n",
    "        data = np.loadtxt(price_file)\n",
    "    else:\n",
    "        # example: use comma-delimited CSV\n",
    "        data = np.loadtxt(price_file, delimiter=',')\n",
    "    return data\n",
    "\n",
    "\n",
    "prices_array = load_prices_to_array(\"prices.txt\")\n",
    "print(prices_array.shape)  # (number_of_timesteps, number_of_instruments)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# prices_array: numpy array of shape (T, n_inst)\n",
    "build_feature_matrix_from_array(prices_array, output_csv=\"features10.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regime_inference.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ───────────────────────── helpers ───────────────────────────────────────\n",
    "def _ols_slope(y: np.ndarray) -> float:\n",
    "    t = np.arange(len(y))\n",
    "    X = np.vstack([t, np.ones_like(t)]).T\n",
    "    m, _ = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── updated helper ─────────────────────────────────────────────────────\n",
    "def _slope_vol_reg(close: np.ndarray,\n",
    "                   idx: int,\n",
    "                   slope_win: int = 30,\n",
    "                   vol_win: int   = 100) -> float | int:\n",
    "    logp = np.log(close)\n",
    "\n",
    "    slope_series = (\n",
    "        pd.Series(logp)\n",
    "          .rolling(slope_win, min_periods=slope_win)\n",
    "          .apply(lambda arr: _ols_slope(arr), raw=True)\n",
    "    )\n",
    "    rtn = pd.Series(logp).diff()\n",
    "    vol_series = rtn.rolling(vol_win, min_periods=vol_win).std()\n",
    "\n",
    "    slope = slope_series.iloc[idx]\n",
    "    vol   = vol_series.iloc[idx]\n",
    "\n",
    "    if np.isnan(slope) or np.isnan(vol):\n",
    "        return np.nan\n",
    "\n",
    "    # 100-bar rolling *median* (causal, matches training pipeline)\n",
    "    median_vol = (\n",
    "        vol_series\n",
    "          .rolling(window=100, min_periods=100)\n",
    "          .median()\n",
    "          .iloc[idx]\n",
    "    )\n",
    "\n",
    "    return 2 if (slope > 0 and vol < median_vol) else 0\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────── pipeline (no drop_last) ──────────────────────────\n",
    "def compute_regime_features_window(prices_window: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices_window : np.ndarray\n",
    "        Shape (50, 100).  Each row is one instrument’s 100-bar history\n",
    "        ending at the timestep for which we want predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Shape (50, 9).  Columns in training order:\n",
    "        [ma, ema, slope_vol, macd, kalman, fib, psar, zscore, wret]\n",
    "    \"\"\"\n",
    "    n_inst, win_len = prices_window.shape\n",
    "    idx = win_len - 1                     # evaluate at the latest bar\n",
    "\n",
    "    out = np.full((n_inst, 9), np.nan)\n",
    "    sqrt_weights = np.arange(1, 46, dtype=float) ** 0.5\n",
    "    sqrt_weights /= sqrt_weights.sum()\n",
    "\n",
    "    for i in range(n_inst):\n",
    "        close = prices_window[i]\n",
    "        logp  = np.log(close)\n",
    "\n",
    "        # MA regime\n",
    "        ma_s = pd.Series(logp).rolling(5).mean().iloc[idx]\n",
    "        ma_l = pd.Series(logp).rolling(70).mean().iloc[idx]\n",
    "        ma_reg = 0 if ma_l > ma_s else 2\n",
    "\n",
    "        # EMA regime\n",
    "        ema_s = pd.Series(logp).ewm(span=5,  adjust=False).mean().iloc[idx]\n",
    "        ema_l = pd.Series(logp).ewm(span=50, adjust=False).mean().iloc[idx]\n",
    "        ema_reg = 2 if ema_s > ema_l else 0\n",
    "\n",
    "        # Slope/Vol regime\n",
    "        sv_reg = _slope_vol_reg(close, idx)\n",
    "\n",
    "        # MACD regime\n",
    "        macd_line = (\n",
    "            pd.Series(logp).ewm(50, adjust=False).mean()\n",
    "            - pd.Series(logp).ewm(90, adjust=False).mean()\n",
    "        )\n",
    "        signal_line = macd_line.ewm(span=40, adjust=False).mean()\n",
    "        macd_reg = 2 if macd_line.iloc[idx] > signal_line.iloc[idx] else 0\n",
    "\n",
    "        # Kalman trend regime\n",
    "        proc_var, meas_var = 0.01, 10.0\n",
    "        x_est = np.zeros(win_len)\n",
    "        P     = np.zeros(win_len)\n",
    "        x_est[0], P[0] = logp[0], 1.0\n",
    "        for t in range(1, win_len):\n",
    "            x_pred = x_est[t - 1]\n",
    "            P_pred = P[t - 1] + proc_var\n",
    "            K      = P_pred / (P_pred + meas_var)\n",
    "            x_est[t] = x_pred + K * (logp[t] - x_pred)\n",
    "            P[t]     = (1 - K) * P_pred\n",
    "        kalman_reg = 2 if logp[idx] > x_est[idx] else 0\n",
    "\n",
    "        # Fibonacci regime\n",
    "        if idx >= 50:\n",
    "            win50 = close[idx - 49 : idx + 1]\n",
    "            hi, lo = win50.max(), win50.min()\n",
    "            rng = hi - lo\n",
    "            upper, lower = lo + 0.786 * rng, lo + 0.618 * rng\n",
    "            fib_reg = 2 if close[idx] > upper else 0 if close[idx] < lower else 1\n",
    "        else:\n",
    "            fib_reg = np.nan\n",
    "\n",
    "        # PSAR regime\n",
    "        psar = np.empty(win_len)\n",
    "        trend_up, af, max_af = True, 0.01, 0.10\n",
    "        ep = close[0]\n",
    "        psar[0] = close[0]\n",
    "        for t in range(1, win_len):\n",
    "            psar[t] = psar[t - 1] + af * (ep - psar[t - 1])\n",
    "            if trend_up:\n",
    "                if close[t] < psar[t]:\n",
    "                    trend_up, psar[t], ep, af = False, ep, close[t], 0.01\n",
    "                elif close[t] > ep:\n",
    "                    ep, af = close[t], min(af + 0.01, max_af)\n",
    "            else:\n",
    "                if close[t] > psar[t]:\n",
    "                    trend_up, psar[t], ep, af = True, ep, close[t], 0.01\n",
    "                elif close[t] < ep:\n",
    "                    ep, af = close[t], min(af + 0.01, max_af)\n",
    "        psar_reg = 2 if close[idx] > psar[idx] else 0\n",
    "\n",
    "        # Z-score regime\n",
    "        ma90 = pd.Series(close).rolling(90).mean().iloc[idx]\n",
    "        sd90 = pd.Series(close).rolling(90).std().iloc[idx]\n",
    "        if np.isnan(ma90) or np.isnan(sd90):\n",
    "            zscore_reg = np.nan\n",
    "        else:\n",
    "            z = (close[idx] - ma90) / sd90\n",
    "            zscore_reg = 2 if z > 0.5 else 0 if z < -0.5 else 1\n",
    "\n",
    "        # Weighted-return regime\n",
    "        if idx >= 45:\n",
    "            r = pd.Series(close).pct_change().iloc[idx - 44 : idx + 1].values\n",
    "            wr = np.dot(r, sqrt_weights)\n",
    "            wret_reg = 2 if wr > 0 else 0 if wr < 0 else 1\n",
    "        else:\n",
    "            wret_reg = np.nan\n",
    "\n",
    "        out[i] = [\n",
    "            ma_reg, ema_reg, sv_reg, macd_reg, kalman_reg,\n",
    "            fib_reg, psar_reg, zscore_reg, wret_reg,\n",
    "        ]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ──────────────────── I/O wrappers for prices.txt ───────────────────────\n",
    "def _extract_window(price_file: str,\n",
    "                    timestep: int,\n",
    "                    win_len: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Slice the latest `win_len` bars (inclusive) ending at `timestep` from the\n",
    "    price file and transpose to (n_inst, win_len).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(price_file, sep=r\"\\s+\", header=None)\n",
    "    n_rows, n_inst = df.shape\n",
    "\n",
    "    if not (0 <= timestep < n_rows):\n",
    "        raise ValueError(f\"timestep {timestep} out of range (0 … {n_rows-1})\")\n",
    "    if timestep < win_len - 1:\n",
    "        raise ValueError(\"Not enough history to build a 100-bar window.\")\n",
    "\n",
    "    slice_df = df.iloc[timestep - win_len + 1 : timestep + 1, :]\n",
    "    return slice_df.to_numpy().T            # (n_inst, win_len)\n",
    "\n",
    "\n",
    "def infer_from_file(price_file: str,\n",
    "                    timestep: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    High-level convenience wrapper:\n",
    "    1. read prices.txt\n",
    "    2. build the (50,100) window ending at `timestep`\n",
    "    3. run the regime-feature pipeline\n",
    "    \"\"\"\n",
    "    window = _extract_window(price_file, timestep, win_len=102)\n",
    "    return compute_regime_features_window(window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_prices_to_array(price_file: str, delim_whitespace: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a text file where each column is one instrument and each row is a timestep,\n",
    "    and returns a 2D NumPy array of shape (T, n_inst).\n",
    "\n",
    "    Args:\n",
    "        price_file: Path to your prices.txt file.\n",
    "        delim_whitespace: If True, splits on any whitespace (default). If False,\n",
    "                         you can pass a specific delimiter via np.loadtxt's delimiter kwarg.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (T, n_inst), where T is the number of timesteps.\n",
    "    \"\"\"\n",
    "    if delim_whitespace:\n",
    "        data = np.loadtxt(price_file)\n",
    "    else:\n",
    "        # example: use comma-delimited CSV\n",
    "        data = np.loadtxt(price_file, delimiter=',')\n",
    "    return data\n",
    "\n",
    "\n",
    "prices_array = load_prices_to_array(\"prices.txt\")\n",
    "print(prices_array.shape)  # (number_of_timesteps, number_of_instruments)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# prices_array: numpy array of shape (T, n_inst)\n",
    "from AWP_pipeline import build_feature_matrix_from_array\n",
    "build_feature_matrix_from_array(prices_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_prices_to_array(\n",
    "    price_file: str,\n",
    "    endpoint: int,\n",
    "    delim_whitespace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a text file where each column is one instrument and each row is a timestep,\n",
    "    then returns a 2D NumPy array of shape (endpoint+1, n_inst),\n",
    "    i.e. all rows from 0 up to and including `endpoint`.\n",
    "\n",
    "    Args:\n",
    "        price_file: Path to your prices.txt file.\n",
    "        endpoint:   Zero-based index of the last row to include.\n",
    "        delim_whitespace: If True, splits on any whitespace (default).\n",
    "    \"\"\"\n",
    "    if delim_whitespace:\n",
    "        data = np.loadtxt(price_file)\n",
    "    else:\n",
    "        data = np.loadtxt(price_file, delimiter=',')\n",
    "    # slice to keep only timesteps 0..endpoint\n",
    "    if endpoint < 0 or endpoint >= data.shape[0]:\n",
    "        raise IndexError(f\"endpoint {endpoint} is out of bounds for data with {data.shape[0]} rows\")\n",
    "    return data[: endpoint + 1, :]\n",
    "\n",
    "# usage:\n",
    "prices_array = load_prices_to_array(\"prices.txt\", endpoint=100)\n",
    "print\n",
    "print(prices_array.shape)  # (750, number_of_instruments)\n",
    "\n",
    "# then call your feature‐matrix builder as before:\n",
    "build_feature_matrix_from_array(prices_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from precision_labeller import plot_all_regimes_long\n",
    "\n",
    "\n",
    "def build_feature_matrix_from_array(\n",
    "    prices_array: np.ndarray\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compute regime features at the latest timestep for each instrument,\n",
    "    drop 'inst' and 'time', stack each feature row into a 2D array, and print it.\n",
    "\n",
    "    Args:\n",
    "        prices_array: 2D array of shape (T, n_inst), where each column is one instrument's price series.\n",
    "    \"\"\"\n",
    "    if prices_array.ndim != 2:\n",
    "        raise ValueError(\"prices_array must be a 2D array of shape (T, n_inst)\")\n",
    "\n",
    "    T, n_inst = prices_array.shape\n",
    "    latest_idx = T - 1\n",
    "    feature_rows = []\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        close = pd.Series(prices_array[:, inst])\n",
    "        high = close.copy()\n",
    "        low = close.copy()\n",
    "        logp = np.log(close)\n",
    "\n",
    "        # True regime label\n",
    "        true_regs = pd.Series(\n",
    "            plot_all_regimes_long(end_point=T, plot_graph=False, inst=inst)\n",
    "        )\n",
    "        true_label = true_regs.iloc[-1]\n",
    "\n",
    "        # Compute indicators at latest index\n",
    "        ma_s = logp.rolling(5, min_periods=1).mean().iloc[latest_idx]\n",
    "        ma_l = logp.rolling(70, min_periods=1).mean().iloc[latest_idx]\n",
    "        ema_s = logp.ewm(span=5, adjust=False).mean().iloc[latest_idx]\n",
    "        ema_l = logp.ewm(span=50, adjust=False).mean().iloc[latest_idx]\n",
    "\n",
    "                        # Slope/Vol regime (exactly at latest timestep)\n",
    "        sv_df_full = compute_slope_vol(close, slope_win=30, vol_win=100)\n",
    "        if latest_idx in sv_df_full.index:\n",
    "            slope_val = sv_df_full.loc[latest_idx, 'slope']\n",
    "            vol_val = sv_df_full.loc[latest_idx, 'vol']\n",
    "            # compute median on all vol values where not NaN\n",
    "            median_vol = sv_df_full['vol'].dropna().median()\n",
    "            slope_vol_reg = 2 if (slope_val > 0 and vol_val < median_vol) else 0\n",
    "        else:\n",
    "            slope_vol_reg = np.nan\n",
    "\n",
    "        # MACD\n",
    "        ema_s2_series = logp.ewm(span=50, adjust=False).mean()\n",
    "        ema_l2_series = logp.ewm(span=90, adjust=False).mean()\n",
    "        macd_series = ema_s2_series - ema_l2_series\n",
    "        signal_series = macd_series.ewm(span=40, adjust=False).mean()\n",
    "        macd_val = macd_series.iloc[latest_idx]\n",
    "        signal_val = signal_series.iloc[latest_idx]\n",
    "        macd_reg = 2 if macd_val > signal_val else 0\n",
    "\n",
    "        # Kalman\n",
    "        x_est = np.zeros(T)\n",
    "        P = np.zeros(T)\n",
    "        x_est[0], P[0] = logp.iloc[0], 1.0\n",
    "        for t in range(1, T):\n",
    "            P_pred = P[t-1] + 0.01\n",
    "            K = P_pred / (P_pred + 10.0)\n",
    "            x_est[t] = x_est[t-1] + K * (logp.iloc[t] - x_est[t-1])\n",
    "            P[t] = (1 - K) * P_pred\n",
    "        kalman_reg = 2 if logp.iloc[latest_idx] > x_est[latest_idx] else 0\n",
    "\n",
    "        # Fibonacci\n",
    "        high_win = close.rolling(50, min_periods=50).max().iloc[latest_idx]\n",
    "        low_win = close.rolling(50, min_periods=50).min().iloc[latest_idx]\n",
    "        fib_range = high_win - low_win\n",
    "        lower = low_win + 0.618 * fib_range\n",
    "        upper = low_win + 0.786 * fib_range\n",
    "        fib_reg = 2 if close.iloc[latest_idx] > upper else (0 if close.iloc[latest_idx] < lower else 1)\n",
    "\n",
    "        # PSAR\n",
    "        psar = np.zeros(T)\n",
    "        trend_up, af, max_step = True, 0.01, 0.10\n",
    "        ep = high.iloc[0]\n",
    "        for t in range(1, T):\n",
    "            prev = psar[t-1]\n",
    "            psar[t] = prev + af * (ep - prev)\n",
    "            if trend_up:\n",
    "                if low.iloc[t] < psar[t]: trend_up, psar[t], ep, af = False, ep, low.iloc[t], 0.01\n",
    "                elif high.iloc[t] > ep: ep, af = high.iloc[t], min(af+0.01, max_step)\n",
    "            else:\n",
    "                if high.iloc[t] > psar[t]: trend_up, psar[t], ep, af = True, ep, high.iloc[t], 0.01\n",
    "                elif low.iloc[t] < ep: ep, af = low.iloc[t], min(af+0.01, max_step)\n",
    "        psar_reg = 2 if close.iloc[latest_idx] > psar[latest_idx] else 0\n",
    "\n",
    "        # Z-score\n",
    "        ma90 = close.rolling(90, min_periods=90).mean().iloc[latest_idx]\n",
    "        sd90 = close.rolling(90, min_periods=90).std().iloc[latest_idx]\n",
    "        z = (close.iloc[latest_idx] - ma90) / sd90\n",
    "        zscore_reg = 2 if z > 0.5 else (0 if z < -0.5 else 1)\n",
    "\n",
    "        # Weighted return\n",
    "        r = close.pct_change()\n",
    "        weights = np.arange(1,46)**0.5; weights /= weights.sum()\n",
    "        wr = r.rolling(45, min_periods=45).apply(lambda x: np.dot(x, weights), raw=True).iloc[latest_idx]\n",
    "        wret_reg = 2 if wr > 0 else (0 if wr < 0 else 1)\n",
    "\n",
    "        # MA regime\n",
    "        ma_reg = 0 if ma_l > ma_s else 2\n",
    "        # EMA regime\n",
    "        ema_reg = 2 if ema_s > ema_l else 0\n",
    "\n",
    "        # Assemble feature row (drop inst, time)\n",
    "        feature_row = [\n",
    "            ma_reg, ema_reg, slope_vol_reg, macd_reg,\n",
    "            kalman_reg, fib_reg, psar_reg, zscore_reg,\n",
    "            wret_reg, true_label\n",
    "        ]\n",
    "        feature_rows.append(feature_row)\n",
    "\n",
    "    # Stack into 2D array and print\n",
    "    feature_matrix = np.vstack(feature_rows)\n",
    "    print(feature_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_prices_to_array(\n",
    "    price_file: str,\n",
    "    endpoint: int,\n",
    "    delim_whitespace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a text file where each column is one instrument and each row is a timestep,\n",
    "    then returns a 2D NumPy array of shape (endpoint+1, n_inst),\n",
    "    i.e. all rows from 0 up to and including `endpoint`.\n",
    "\n",
    "    Args:\n",
    "        price_file: Path to your prices.txt file.\n",
    "        endpoint:   Zero-based index of the last row to include.\n",
    "        delim_whitespace: If True, splits on any whitespace (default).\n",
    "    \"\"\"\n",
    "    if delim_whitespace:\n",
    "        data = np.loadtxt(price_file)\n",
    "    else:\n",
    "        data = np.loadtxt(price_file, delimiter=',')\n",
    "    # slice to keep only timesteps 0..endpoint\n",
    "    if endpoint < 0 or endpoint >= data.shape[0]:\n",
    "        raise IndexError(f\"endpoint {endpoint} is out of bounds for data with {data.shape[0]} rows\")\n",
    "    return data[: endpoint + 1, :]\n",
    "\n",
    "# usage:\n",
    "prices_array = load_prices_to_array(\"prices.txt\", endpoint=100)\n",
    "print(prices_array)\n",
    "print(prices_array.shape)  # (750, number_of_instruments)\n",
    "\n",
    "# then call your feature‐matrix builder as before:\n",
    "build_feature_matrix_from_array(prices_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regime_inference.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ───────────────────────── helpers ───────────────────────────────────────\n",
    "def _ols_slope(y: np.ndarray) -> float:\n",
    "    t = np.arange(len(y))\n",
    "    X = np.vstack([t, np.ones_like(t)]).T\n",
    "    m, _ = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── updated helper ─────────────────────────────────────────────────────\n",
    "def _slope_vol_reg(close: np.ndarray,\n",
    "                   idx: int,\n",
    "                   slope_win: int = 30,\n",
    "                   vol_win: int   = 100) -> float | int:\n",
    "    logp = np.log(close)\n",
    "\n",
    "    slope_series = (\n",
    "        pd.Series(logp)\n",
    "          .rolling(slope_win, min_periods=slope_win)\n",
    "          .apply(lambda arr: _ols_slope(arr), raw=True)\n",
    "    )\n",
    "    rtn = pd.Series(logp).diff()\n",
    "    vol_series = rtn.rolling(vol_win, min_periods=vol_win).std()\n",
    "\n",
    "    slope = slope_series.iloc[idx]\n",
    "    vol   = vol_series.iloc[idx]\n",
    "\n",
    "    if np.isnan(slope) or np.isnan(vol):\n",
    "        return np.nan\n",
    "\n",
    "    # 100-bar rolling *median* (causal, matches training pipeline)\n",
    "    median_vol = (\n",
    "        vol_series\n",
    "          .rolling(window=100, min_periods=100)\n",
    "          .median()\n",
    "          .iloc[idx]\n",
    "    )\n",
    "\n",
    "    return 2 if (slope > 0 and vol < median_vol) else 0\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────── pipeline (no drop_last) ──────────────────────────\n",
    "def compute_regime_features_window(prices_window: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices_window : np.ndarray\n",
    "        Shape (50, 100).  Each row is one instrument’s 100-bar history\n",
    "        ending at the timestep for which we want predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Shape (50, 9).  Columns in training order:\n",
    "        [ma, ema, slope_vol, macd, kalman, fib, psar, zscore, wret]\n",
    "    \"\"\"\n",
    "    n_inst, win_len = prices_window.shape\n",
    "    idx = win_len - 1                     # evaluate at the latest bar\n",
    "\n",
    "    out = np.full((n_inst, 9), np.nan)\n",
    "    sqrt_weights = np.arange(1, 46, dtype=float) ** 0.5\n",
    "    sqrt_weights /= sqrt_weights.sum()\n",
    "\n",
    "    for i in range(n_inst):\n",
    "        close = prices_window[i]\n",
    "        logp  = np.log(close)\n",
    "\n",
    "        # MA regime\n",
    "        ma_s = pd.Series(logp).rolling(5).mean().iloc[idx]\n",
    "        ma_l = pd.Series(logp).rolling(70).mean().iloc[idx]\n",
    "        ma_reg = 0 if ma_l > ma_s else 2\n",
    "\n",
    "        # EMA regime\n",
    "        ema_s = pd.Series(logp).ewm(span=5,  adjust=False).mean().iloc[idx]\n",
    "        ema_l = pd.Series(logp).ewm(span=50, adjust=False).mean().iloc[idx]\n",
    "        ema_reg = 2 if ema_s > ema_l else 0\n",
    "\n",
    "        # Slope/Vol regime\n",
    "        sv_reg = _slope_vol_reg(close, idx)\n",
    "\n",
    "        # MACD regime\n",
    "        macd_line = (\n",
    "            pd.Series(logp).ewm(50, adjust=False).mean()\n",
    "            - pd.Series(logp).ewm(90, adjust=False).mean()\n",
    "        )\n",
    "        signal_line = macd_line.ewm(span=40, adjust=False).mean()\n",
    "        macd_reg = 2 if macd_line.iloc[idx] > signal_line.iloc[idx] else 0\n",
    "\n",
    "        # Kalman trend regime\n",
    "        proc_var, meas_var = 0.01, 10.0\n",
    "        x_est = np.zeros(win_len)\n",
    "        P     = np.zeros(win_len)\n",
    "        x_est[0], P[0] = logp[0], 1.0\n",
    "        for t in range(1, win_len):\n",
    "            x_pred = x_est[t - 1]\n",
    "            P_pred = P[t - 1] + proc_var\n",
    "            K      = P_pred / (P_pred + meas_var)\n",
    "            x_est[t] = x_pred + K * (logp[t] - x_pred)\n",
    "            P[t]     = (1 - K) * P_pred\n",
    "        kalman_reg = 2 if logp[idx] > x_est[idx] else 0\n",
    "\n",
    "        # Fibonacci regime\n",
    "        if idx >= 50:\n",
    "            win50 = close[idx - 49 : idx + 1]\n",
    "            hi, lo = win50.max(), win50.min()\n",
    "            rng = hi - lo\n",
    "            upper, lower = lo + 0.786 * rng, lo + 0.618 * rng\n",
    "            fib_reg = 2 if close[idx] > upper else 0 if close[idx] < lower else 1\n",
    "        else:\n",
    "            fib_reg = np.nan\n",
    "\n",
    "        # PSAR regime\n",
    "        psar = np.empty(win_len)\n",
    "        trend_up, af, max_af = True, 0.01, 0.10\n",
    "        ep = close[0]\n",
    "        psar[0] = close[0]\n",
    "        for t in range(1, win_len):\n",
    "            psar[t] = psar[t - 1] + af * (ep - psar[t - 1])\n",
    "            if trend_up:\n",
    "                if close[t] < psar[t]:\n",
    "                    trend_up, psar[t], ep, af = False, ep, close[t], 0.01\n",
    "                elif close[t] > ep:\n",
    "                    ep, af = close[t], min(af + 0.01, max_af)\n",
    "            else:\n",
    "                if close[t] > psar[t]:\n",
    "                    trend_up, psar[t], ep, af = True, ep, close[t], 0.01\n",
    "                elif close[t] < ep:\n",
    "                    ep, af = close[t], min(af + 0.01, max_af)\n",
    "        psar_reg = 2 if close[idx] > psar[idx] else 0\n",
    "\n",
    "        # Z-score regime\n",
    "        ma90 = pd.Series(close).rolling(90).mean().iloc[idx]\n",
    "        sd90 = pd.Series(close).rolling(90).std().iloc[idx]\n",
    "        if np.isnan(ma90) or np.isnan(sd90):\n",
    "            zscore_reg = np.nan\n",
    "        else:\n",
    "            z = (close[idx] - ma90) / sd90\n",
    "            zscore_reg = 2 if z > 0.5 else 0 if z < -0.5 else 1\n",
    "\n",
    "        # Weighted-return regime\n",
    "        if idx >= 45:\n",
    "            r = pd.Series(close).pct_change().iloc[idx - 44 : idx + 1].values\n",
    "            wr = np.dot(r, sqrt_weights)\n",
    "            wret_reg = 2 if wr > 0 else 0 if wr < 0 else 1\n",
    "        else:\n",
    "            wret_reg = np.nan\n",
    "\n",
    "        out[i] = [\n",
    "            ma_reg, ema_reg, sv_reg, macd_reg, kalman_reg,\n",
    "            fib_reg, psar_reg, zscore_reg, wret_reg,\n",
    "        ]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ──────────────────── I/O wrappers for prices.txt ───────────────────────\n",
    "def _extract_window(price_file: str,\n",
    "                    timestep: int,\n",
    "                    win_len: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Slice the latest `win_len` bars (inclusive) ending at `timestep` from the\n",
    "    price file and transpose to (n_inst, win_len).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(price_file, sep=r\"\\s+\", header=None)\n",
    "    n_rows, n_inst = df.shape\n",
    "\n",
    "    if not (0 <= timestep < n_rows):\n",
    "        raise ValueError(f\"timestep {timestep} out of range (0 … {n_rows-1})\")\n",
    "    if timestep < win_len - 1:\n",
    "        raise ValueError(\"Not enough history to build a 100-bar window.\")\n",
    "\n",
    "    slice_df = df.iloc[timestep - win_len + 1 : timestep + 1, :]\n",
    "    return slice_df.to_numpy().T            # (n_inst, win_len)\n",
    "\n",
    "\n",
    "def infer_from_file(price_file: str,\n",
    "                    timestep: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    High-level convenience wrapper:\n",
    "    1. read prices.txt\n",
    "    2. build the (50,100) window ending at `timestep`\n",
    "    3. run the regime-feature pipeline\n",
    "    \"\"\"\n",
    "    window = _extract_window(price_file, timestep, win_len=100)\n",
    "    return compute_regime_features_window(window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_prices_to_array(\n",
    "    price_file: str,\n",
    "    endpoint: int,\n",
    "    delim_whitespace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a text file where each column is one instrument and each row is a timestep,\n",
    "    then returns a 2D NumPy array of shape (endpoint+1, n_inst),\n",
    "    i.e. all rows from 0 up to and including `endpoint`.\n",
    "\n",
    "    Args:\n",
    "        price_file: Path to your prices.txt file.\n",
    "        endpoint:   Zero-based index of the last row to include.\n",
    "        delim_whitespace: If True, splits on any whitespace (default).\n",
    "    \"\"\"\n",
    "    if delim_whitespace:\n",
    "        data = np.loadtxt(price_file)\n",
    "    else:\n",
    "        data = np.loadtxt(price_file, delimiter=',')\n",
    "    # slice to keep only timesteps 0..endpoint\n",
    "    if endpoint < 0 or endpoint >= data.shape[0]:\n",
    "        raise IndexError(f\"endpoint {endpoint} is out of bounds for data with {data.shape[0]} rows\")\n",
    "    return data[: endpoint + 1, :]\n",
    "\n",
    "# usage:\n",
    "prices_array = load_prices_to_array(\"prices.txt\", endpoint=300)\n",
    "print(prices_array)\n",
    "print(prices_array.shape)  # (750, number_of_instruments)\n",
    "\n",
    "# then call your feature‐matrix builder as before:\n",
    "build_feature_matrix_from_array(prices_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
