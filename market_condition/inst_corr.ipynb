{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import percentileofscore, pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller, coint\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from matplotlib.cm import coolwarm, get_cmap\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPrices():\n",
    "    fn=\"../prices.txt\"\n",
    "    global nt, nInst\n",
    "    df=pd.read_csv(fn, sep=r'\\s+', header=None, index_col=None)\n",
    "    (nt,nInst) = df.shape\n",
    "    return nt, nInst, (df.values).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "(nt, nInst, prcAll) = loadPrices()\n",
    "prcTest = prcAll[:, :450]\n",
    "prcCheck = prcAll[:, 450:600]\n",
    "prcEval = prcAll[:, 600:]\n",
    "prcEvalPrev = prcAll\n",
    "prcCheckPrev = prcAll[:, :600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: What Pearson Correlation on Log Returns Really Means\n",
    "# You're asking: \"When Asset A moves up or down more than usual, does Asset B also tend to move in the same direction, at the same time, and by how much?\"\n",
    "\n",
    "# By using log returns instead of prices: You're removing trend and scale, and looking purely at the rhythm of movement between two instruments.\n",
    "def spearman_correlation_from_prices(prices_a, prices_b, use_log_returns=True):\n",
    "    \"\"\"\n",
    "    Computes Spearman correlation between two price series.\n",
    "\n",
    "    Parameters:\n",
    "        prices_a (np.ndarray): Price series A (1D)\n",
    "        prices_b (np.ndarray): Price series B (1D)\n",
    "        use_log_returns (bool): If True, use log returns; else use percent returns\n",
    "\n",
    "    Returns:\n",
    "        corr (float): Spearman correlation coefficient\n",
    "        pval (float): p-value associated with the correlation\n",
    "    \"\"\"\n",
    "    # Convert to returns\n",
    "    if use_log_returns:\n",
    "        returns_a = np.diff(np.log(prices_a))\n",
    "        returns_b = np.diff(np.log(prices_b))\n",
    "    else:\n",
    "        returns_a = np.diff(prices_a) / prices_a[:-1]\n",
    "        returns_b = np.diff(prices_b) / prices_b[:-1]\n",
    "\n",
    "    # Align lengths in case of mismatch\n",
    "    min_len = min(len(returns_a), len(returns_b))\n",
    "    returns_a = returns_a[:min_len]\n",
    "    returns_b = returns_b[:min_len]\n",
    "\n",
    "    # Compute Spearman correlation\n",
    "    corr, pval = spearmanr(returns_a, returns_b)\n",
    "    return corr, pval\n",
    "def drop_nan_aligned(a, b):\n",
    "    \"\"\"\n",
    "    Drop all elements where either `a` or `b` has NaN, keeping time alignment.\n",
    "\n",
    "    Parameters:\n",
    "        a (np.ndarray): 1D array\n",
    "        b (np.ndarray): 1D array\n",
    "\n",
    "    Returns:\n",
    "        a_clean, b_clean: cleaned arrays of same length\n",
    "    \"\"\"\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "\n",
    "    # Build mask where neither a nor b is NaN\n",
    "    mask = ~np.isnan(a) & ~np.isnan(b)\n",
    "\n",
    "    return a[mask], b[mask]\n",
    "\n",
    "def compute_correlation_matrix(prc_matrix, \n",
    "                               smooth=False, \n",
    "                               smooth_window=5, \n",
    "                               use_log_returns=True, \n",
    "                               method=\"pearson\"):\n",
    "    \"\"\"\n",
    "    Computes correlation and p-value matrices from returns (log or percentage),\n",
    "    with optional smoothing and method choice between Pearson and Spearman.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (n_instruments, n_days) price matrix\n",
    "        smooth (bool): Whether to smooth returns before correlation\n",
    "        smooth_window (int): Window size for smoothing\n",
    "        use_log_returns (bool): If True, use log returns; else use percentage returns\n",
    "        method (str): \"pearson\" (default) or \"spearman\"\n",
    "\n",
    "    Returns:\n",
    "        corr_matrix (ndarray): Correlation coefficients matrix\n",
    "        pval_matrix (ndarray): P-value matrix\n",
    "    \"\"\"\n",
    "    n = prc_matrix.shape[0]\n",
    "\n",
    "    # Calculate returns\n",
    "    if use_log_returns:\n",
    "        returns = np.diff(np.log(prc_matrix), axis=1)\n",
    "    else:\n",
    "        returns = np.diff(prc_matrix, axis=1) / prc_matrix[:, :-1]\n",
    "\n",
    "    # Optional smoothing\n",
    "    if smooth:\n",
    "        kernel = np.ones(smooth_window) / smooth_window\n",
    "        returns = np.array([\n",
    "            np.convolve(r, kernel, mode='valid')\n",
    "            for r in returns\n",
    "        ])\n",
    "\n",
    "    # Prepare matrices\n",
    "    corr_matrix = np.zeros((n, n))\n",
    "    pval_matrix = np.zeros((n, n))\n",
    "\n",
    "    # Compute correlations\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                corr_matrix[i, j] = 1.0\n",
    "                pval_matrix[i, j] = 0.0\n",
    "            else:\n",
    "                r1, r2 = drop_nan_aligned(returns[i], returns[j])\n",
    "                if method == \"pearson\":\n",
    "                    r, p = pearsonr(r1, r2)\n",
    "                elif method == \"spearman\":\n",
    "                    r, p = spearmanr(r1, r2)\n",
    "                else:\n",
    "                    raise ValueError(\"Method must be 'pearson' or 'spearman'\")\n",
    "                corr_matrix[i, j] = r\n",
    "                pval_matrix[i, j] = p\n",
    "\n",
    "    return corr_matrix, pval_matrix\n",
    "\n",
    "def plot_correlation_heatmap(corr_matrix, title=\"Correlation Matrix Heatmap\"):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        cmap='coolwarm',       # red = negative, blue = positive\n",
    "        vmin=-1, vmax=1,       # range of correlation values\n",
    "        center=0,\n",
    "        square=True,\n",
    "        cbar_kws={'shrink': .6},\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_fdr_significant_correlation_heatmap(corr_matrix, pval_matrix, alpha=0.05, title=\"FDR-Significant Correlation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the correlation matrix, masking correlations that are not\n",
    "    statistically significant after FDR correction.\n",
    "\n",
    "    Parameters:\n",
    "        corr_matrix (ndarray): Correlation coefficient matrix (n x n)\n",
    "        pval_matrix (ndarray): Corresponding p-value matrix (n x n)\n",
    "        alpha (float): FDR-adjusted significance threshold (default = 0.05)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    # Flatten and correct p-values using FDR (Benjamini-Hochberg)\n",
    "    flat_pvals = pval_matrix.flatten()\n",
    "    rejected, corrected_pvals = fdrcorrection(flat_pvals, alpha=alpha)\n",
    "\n",
    "    # Reshape rejection mask to original matrix shape\n",
    "    fdr_mask = ~rejected.reshape(pval_matrix.shape)  # Mask = not significant\n",
    "    np.fill_diagonal(fdr_mask, True)  # Always mask diagonal\n",
    "\n",
    "    # Plot heatmap with non-significant values masked\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=fdr_mask,\n",
    "        cmap='coolwarm',\n",
    "        vmin=-1, vmax=1,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        cbar_kws={'shrink': .6},\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    plt.title(f\"{title} (α = {alpha})\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def adjust_pvals_fdr(pval_matrix, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Applies FDR correction to a p-value matrix and returns a boolean mask of significance.\n",
    "    \"\"\"\n",
    "    pvals_flat = pval_matrix.flatten()\n",
    "    rejected, pvals_corrected = fdrcorrection(pvals_flat, alpha=alpha)\n",
    "    return rejected.reshape(pval_matrix.shape)\n",
    "\n",
    "def plot_significant_correlation_heatmap(corr_matrix, pval_matrix, alpha=0.05, title=\"Significant Correlation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the correlation matrix, masking non-significant correlations.\n",
    "\n",
    "    Parameters:\n",
    "        corr_matrix (ndarray): Correlation coefficient matrix\n",
    "        pval_matrix (ndarray): Corresponding p-value matrix\n",
    "        alpha (float): Significance level (default = 0.05)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Create a mask for non-significant values (p > alpha)\n",
    "    mask = (pval_matrix > alpha)\n",
    "\n",
    "    # Optionally mask the diagonal too (self-correlations always = 1)\n",
    "    np.fill_diagonal(mask, True)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        cmap='coolwarm',\n",
    "        vmin=-1, vmax=1,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        cbar_kws={'shrink': .6},\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    plt.title(f\"{title} (α = {alpha})\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def get_top_correlated_pairs(corr_matrix, top_n=5):\n",
    "    \"\"\"\n",
    "    Returns the top N most strongly positively correlated instrument index pairs\n",
    "    from a correlation matrix (excluding diagonal/self-correlations).\n",
    "\n",
    "    Parameters:\n",
    "        corr_matrix (ndarray): A symmetric NxN correlation matrix\n",
    "        top_n (int): Number of top pairs to return\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (instrument_i, instrument_j, correlation)\n",
    "    \"\"\"\n",
    "    # Get indices of upper triangle, excluding diagonal\n",
    "    n = corr_matrix.shape[0]\n",
    "    upper_tri_indices = np.triu_indices(n, k=1)\n",
    "\n",
    "    # Extract the upper triangle values and corresponding index pairs\n",
    "    corr_values = corr_matrix[upper_tri_indices]\n",
    "    index_pairs = list(zip(upper_tri_indices[0], upper_tri_indices[1]))\n",
    "\n",
    "    # Sort by correlation values descending\n",
    "    sorted_pairs = sorted(zip(index_pairs, corr_values), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top N\n",
    "    return [(i, j, round(val, 4)) for ((i, j), val) in sorted_pairs[:top_n]]\n",
    "\n",
    "def lead_lag_correlation_heatmap(returns, target_idx=0, max_lag=10, method='pearson'):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of lagged correlations between a target instrument and all others.\n",
    "\n",
    "    Parameters:\n",
    "        returns (ndarray): (n_instruments, n_days) return matrix (log or %)\n",
    "        target_idx (int): Index of the instrument to compare against\n",
    "        max_lag (int): Max number of lags to check (both positive and negative)\n",
    "        method (str): 'pearson' or 'spearman'\n",
    "    \"\"\"\n",
    "    n_instr, n_days = returns.shape\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "    target = (returns[target_idx] - np.mean(returns[target_idx])) / np.std(returns[target_idx])\n",
    "\n",
    "    corr_matrix = np.zeros((n_instr, len(lags)))\n",
    "\n",
    "    for i in range(n_instr):\n",
    "        if i == target_idx:\n",
    "            corr_matrix[i, :] = np.nan  # skip self\n",
    "            continue\n",
    "        series = (returns[i] - np.mean(returns[i])) / np.std(returns[i])\n",
    "        for j, lag in enumerate(lags):\n",
    "            if lag < 0:\n",
    "                x = series[-lag:]\n",
    "                y = target[:lag]\n",
    "            elif lag > 0:\n",
    "                x = series[:-lag]\n",
    "                y = target[lag:]\n",
    "            else:\n",
    "                x = series\n",
    "                y = target\n",
    "\n",
    "            if method == 'pearson':\n",
    "                corr = np.corrcoef(x, y)[0, 1]\n",
    "            elif method == 'spearman':\n",
    "                from scipy.stats import spearmanr\n",
    "                corr, _ = spearmanr(x, y)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported method\")\n",
    "            corr_matrix[i, j] = corr\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0,\n",
    "                xticklabels=list(lags), yticklabels=[f\"Instr {i}\" for i in range(n_instr)])\n",
    "    plt.title(f\"Lag Correlation with Instrument {target_idx}\", fontsize=16)\n",
    "    plt.xlabel(\"Lag (days)\")\n",
    "    plt.ylabel(\"Other Instruments\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def lead_lag_correlation_heatmap_with_mask(\n",
    "    returns,\n",
    "    target_idx=0,\n",
    "    max_lag=10,\n",
    "    method='pearson',\n",
    "    corr_threshold=0.0,\n",
    "    tickers=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes and plots lead-lag correlation heatmap between a target instrument and all others,\n",
    "    with masking based on a correlation threshold.\n",
    "\n",
    "    Parameters:\n",
    "        returns (ndarray): (n_instruments, n_days) return matrix\n",
    "        target_idx (int): Index of the target instrument\n",
    "        max_lag (int): Max lag (both positive and negative)\n",
    "        method (str): 'pearson' or 'spearman'\n",
    "        corr_threshold (float): Minimum |correlation| to show\n",
    "        tickers (list of str, optional): Ticker labels for instruments\n",
    "\n",
    "    Returns:\n",
    "        corr_matrix (ndarray): Full lag correlation matrix\n",
    "        mask (ndarray): Boolean mask (True = hidden)\n",
    "    \"\"\"\n",
    "    n_instr, n_days = returns.shape\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "    target = (returns[target_idx] - np.mean(returns[target_idx])) / np.std(returns[target_idx])\n",
    "\n",
    "    corr_matrix = np.zeros((n_instr, len(lags)))\n",
    "\n",
    "    for i in range(n_instr):\n",
    "        if i == target_idx:\n",
    "            corr_matrix[i, :] = np.nan\n",
    "            continue\n",
    "\n",
    "        series = (returns[i] - np.mean(returns[i])) / np.std(returns[i])\n",
    "\n",
    "        for j, lag in enumerate(lags):\n",
    "            if lag < 0:\n",
    "                x = series[-lag:]\n",
    "                y = target[:lag]\n",
    "            elif lag > 0:\n",
    "                x = series[:-lag]\n",
    "                y = target[lag:]\n",
    "            else:\n",
    "                x = series\n",
    "                y = target\n",
    "\n",
    "def lead_lag_correlation_heatmap_with_mask(\n",
    "    returns,\n",
    "    target_idx=0,\n",
    "    max_lag=10,\n",
    "    method='pearson',\n",
    "    corr_threshold=0.0,\n",
    "    tickers=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes and plots lead-lag correlation heatmap between a target instrument and all others,\n",
    "    with masking based on a correlation threshold and proper NaN handling.\n",
    "\n",
    "    Parameters:\n",
    "        returns (ndarray): (n_instruments, n_days) return matrix\n",
    "        target_idx (int): Index of the target instrument\n",
    "        max_lag (int): Max lag (both positive and negative)\n",
    "        method (str): 'pearson' or 'spearman'\n",
    "        corr_threshold (float): Minimum |correlation| to show\n",
    "        tickers (list of str, optional): Ticker labels for instruments\n",
    "\n",
    "    Returns:\n",
    "        corr_matrix (ndarray): Full lag correlation matrix\n",
    "        mask (ndarray): Boolean mask (True = hidden)\n",
    "    \"\"\"\n",
    "    n_instr, n_days = returns.shape\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "    corr_matrix = np.full((n_instr, len(lags)), np.nan)\n",
    "\n",
    "    # Normalize target series\n",
    "    target = returns[target_idx]\n",
    "    target = (target - np.nanmean(target)) / np.nanstd(target)\n",
    "\n",
    "    for i in range(n_instr):\n",
    "        if i == target_idx:\n",
    "            continue\n",
    "\n",
    "        series = returns[i]\n",
    "        series = (series - np.nanmean(series)) / np.nanstd(series)\n",
    "\n",
    "        for j, lag in enumerate(lags):\n",
    "            if lag < 0:\n",
    "                x = series[-lag:]\n",
    "                y = target[:lag]\n",
    "            elif lag > 0:\n",
    "                x = series[:-lag]\n",
    "                y = target[lag:]\n",
    "            else:\n",
    "                x = series\n",
    "                y = target\n",
    "\n",
    "            # Drop NaNs pairwise\n",
    "            valid_mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "            x_clean = x[valid_mask]\n",
    "            y_clean = y[valid_mask]\n",
    "\n",
    "            if len(x_clean) < 3:  # Not enough data to correlate\n",
    "                continue\n",
    "\n",
    "            if method == 'pearson':\n",
    "                corr = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "            elif method == 'spearman':\n",
    "                corr, _ = spearmanr(x_clean, y_clean)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported method: use 'pearson' or 'spearman'\")\n",
    "\n",
    "            corr_matrix[i, j] = corr\n",
    "\n",
    "    # Mask values below threshold or NaNs\n",
    "    mask = (np.abs(corr_matrix) < corr_threshold) | np.isnan(corr_matrix)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        mask=mask,\n",
    "        xticklabels=lags,\n",
    "        yticklabels=tickers if tickers else range(n_instr),\n",
    "        cbar_kws={\"label\": f\"{method.title()} Correlation\"},\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f\"Lead-Lag {method.title()} Correlation to Instrument {target_idx}\")\n",
    "    plt.xlabel(\"Lag (Positive = Target Leads)\")\n",
    "    plt.ylabel(\"Instrument\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return corr_matrix, mask\n",
    "\n",
    "def resample_closing_prices(prc_array, n=7):\n",
    "    \"\"\"\n",
    "    Resample daily closing prices into n-day closing prices by taking the last price in each n-day window.\n",
    "\n",
    "    Parameters:\n",
    "        prc_array (np.ndarray): shape (n_instruments, n_days)\n",
    "        n (int): grouping size, e.g. 7 for weekly\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: shape (n_instruments, new_days) with resampled closing prices\n",
    "    \"\"\"\n",
    "    n_instruments, n_days = prc_array.shape\n",
    "    n_groups = n_days // n  # integer number of groups\n",
    "\n",
    "    # Trim extra days at the end that don't fit into full groups\n",
    "    trimmed = prc_array[:, :n_groups * n]\n",
    "\n",
    "    # Reshape: (n_instruments, n_groups, n)\n",
    "    reshaped = trimmed.reshape(n_instruments, n_groups, n)\n",
    "\n",
    "    # Take the last price in each group (simulate close of each period)\n",
    "    resampled = reshaped[:, :, -1]\n",
    "\n",
    "    return resampled\n",
    "\n",
    "def average_windowed_correlation(prc_matrix, \n",
    "                                  window_size=7,\n",
    "                                  method='pearson', \n",
    "                                  use_log_returns=True):\n",
    "    \"\"\"\n",
    "    Computes an average correlation matrix by converting daily prices to windowed data \n",
    "    with multiple offsets (0 to window_size - 1).\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (n_instruments, n_days) daily price matrix\n",
    "        window_size (int): number of days in each window (e.g. 7 for weekly, 30 for monthly)\n",
    "        method (str): 'pearson' or 'spearman'\n",
    "        use_log_returns (bool): Use log returns (True) or percent returns (False)\n",
    "\n",
    "    Returns:\n",
    "        avg_corr_matrix (ndarray): Averaged correlation matrix (n_instruments x n_instruments)\n",
    "    \"\"\"\n",
    "    n_instruments, n_days = prc_matrix.shape\n",
    "    all_corrs = []\n",
    "\n",
    "    for offset in range(window_size):\n",
    "        # Step 1: Resample using window size and offset\n",
    "        idx = np.arange(offset, n_days, window_size)\n",
    "        if len(idx) < 3:\n",
    "            continue  # skip if not enough data points\n",
    "\n",
    "        windowed_prices = prc_matrix[:, idx]\n",
    "\n",
    "        # Step 2: Compute returns\n",
    "        if use_log_returns:\n",
    "            returns = np.diff(np.log(windowed_prices), axis=1)\n",
    "        else:\n",
    "            returns = np.diff(windowed_prices, axis=1) / windowed_prices[:, :-1]\n",
    "\n",
    "        # Step 3: Compute correlation matrix\n",
    "        corr_matrix = np.zeros((n_instruments, n_instruments))\n",
    "        for i in range(n_instruments):\n",
    "            for j in range(n_instruments):\n",
    "                if i == j:\n",
    "                    corr_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    if method == 'pearson':\n",
    "                        r, _ = pearsonr(returns[i], returns[j])\n",
    "                    elif method == 'spearman':\n",
    "                        r, _ = spearmanr(returns[i], returns[j])\n",
    "                    else:\n",
    "                        raise ValueError(\"method must be 'pearson' or 'spearman'\")\n",
    "                    corr_matrix[i, j] = r\n",
    "\n",
    "        all_corrs.append(corr_matrix)\n",
    "\n",
    "    avg_corr_matrix = np.mean(all_corrs, axis=0)\n",
    "    return avg_corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING SAME TECHNIQUES TO NVIDIA AND AMD\n",
    "import yfinance as yf\n",
    "\n",
    "def fetch_closing_prices(tickers, days=450):\n",
    "    \"\"\"\n",
    "    Fetch closing prices for given tickers over the last N trading days.\n",
    "    \n",
    "    Parameters:\n",
    "        tickers (list of str): e.g., ['NVDA', 'AMD']\n",
    "        days (int): number of trading days (~450 = ~2 calendar years)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: (days, tickers) with closing prices\n",
    "    \"\"\"\n",
    "    data = yf.download(tickers, period=f\"{int(days * 1.5)}d\")['Close']  # fetch ~1.5x to ensure enough trading days\n",
    "    data = data.dropna()\n",
    "    return data.head(days)\n",
    "\n",
    "def fetch_weekly_closing_prices(tickers, weeks=64):\n",
    "    \"\"\"\n",
    "    Fetch weekly closing prices for given tickers over the last N weeks.\n",
    "    \n",
    "    Parameters:\n",
    "        tickers (list of str): e.g., ['NVDA', 'AMD']\n",
    "        weeks (int): number of weeks to fetch (default ~64 weeks ≈ 1.25 years)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: (weeks, tickers) with weekly closing prices (Friday close)\n",
    "    \"\"\"\n",
    "    days_to_fetch = int(weeks * 7 * 1.5)  # fetch more calendar days to ensure enough weeks\n",
    "    data = yf.download(tickers, period=f\"{days_to_fetch}d\")['Close']\n",
    "    \n",
    "    # Resample to weekly frequency - take last closing price of each week (Friday)\n",
    "    weekly_data = data.resample('W-FRI').last()\n",
    "    \n",
    "    # Take only the most recent N weeks\n",
    "    return weekly_data.head(weeks)\n",
    "\n",
    "def add_noise_to_matrix(matrix, noise_std=0.01, seed=None):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to each time series in a 2D matrix.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (ndarray): shape (n_series, n_timepoints), each row is a time series\n",
    "        noise_std (float): Standard deviation of the noise\n",
    "        seed (int or None): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        noisy_matrix (ndarray): same shape as input, with added noise\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_std, size=matrix.shape)\n",
    "    return matrix + noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING STARTING DATA!\n",
    "\n",
    "tickers = [\n",
    "    'NVDA', 'AMD',             # Core\n",
    "    'PG', \n",
    "    # 'LOT.AX', 'MAQ.AX'\n",
    "    # 'JNJ', 'KO', 'MCD', 'WMT', 'BRK-B', 'XOM', 'CVX', 'LMT', 'TGT',  # Unrelated US stocks\n",
    "    # 'MAQ.AX', 'LOT.AX', 'XRO.AX',  # Australian stocksl\n",
    "    # 'CCJ', 'UEC', 'URNM' # Uranium-related\n",
    "]\n",
    "\n",
    "days = 600\n",
    "close_df = fetch_closing_prices(tickers, days=days)\n",
    "close_weekly_df = fetch_weekly_closing_prices(tickers, weeks=days//7)\n",
    "prc_matrix = close_df.T.to_numpy()\n",
    "prc_matrix_noise = add_noise_to_matrix(prc_matrix, noise_std=0.5, seed=42)\n",
    "prc_weekly_matrix = close_weekly_df.T.to_numpy()\n",
    "\n",
    "\n",
    "prcCheckPrev = np.vstack((prcCheckPrev, prc_matrix, prc_matrix_noise))\n",
    "data_d = np.vstack((prcTest, prc_matrix[:, :450], prc_matrix_noise[:, :450]))\n",
    "# data_d = prcTest\n",
    "# data_w = resample_closing_prices(data_d, 14)\n",
    "# data = data_d\n",
    "\n",
    "# DATA LAYOUT #\n",
    "# 0-49 instruments\n",
    "# 50, 51, 52 acc NVDA and AMD CTVA\n",
    "# 53, 54, 55 noised NVDA and AMD and CTVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spearman_correlation_from_prices(*drop_nan_aligned(prc_matrix[0, :], prc_matrix[1, :]), True)[0])\n",
    "print(spearman_correlation_from_prices(*drop_nan_aligned(prc_matrix_noise[0, :], prc_matrix_noise[1, :]), True)[0])\n",
    "print(spearman_correlation_from_prices(*drop_nan_aligned(prc_weekly_matrix[0, :], prc_weekly_matrix[1, :]), True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corr_matrix, pval_matrix = compute_correlation_matrix(data, smooth=True, smooth_window=30, use_log_returns=True, method='spearman')\n",
    "corr_matrix = average_windowed_correlation(data_d, window_size=1, method='spearman')\n",
    "\n",
    "plot_correlation_heatmap(corr_matrix)\n",
    "# plot_significant_correlation_heatmap(corr_matrix, pval_matrix, alpha=0.01)\n",
    "log_returns = np.diff(np.log(data), axis=1)\n",
    "# for inst in range(data.shape[0]):\n",
    "inst = 18\n",
    "# lead_lag_correlation_heatmap_with_mask(log_returns, target_idx=inst, max_lag=10, method='spearman', corr_threshold=0.00, tickers=None)\n",
    "\n",
    "# print(get_top_correlated_pairs(corr_matrix, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Written Correlations\n",
    "top5 = get_top_correlated_pairs(corr_matrix, top_n=10)\n",
    "for i, j, val in top5:\n",
    "    print(f\"Instrument {i} & Instrument {j} → Correlation: {val}\")\n",
    "\n",
    "# 50 & 51 nvidia and amd\n",
    "# noised: nvidia & amd → Correlation: 0.5437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_correlated_pairs(prc_matrix, corr_matrix, top_n=5, use_returns=False, smooth=False, smooth_window=5):\n",
    "    \"\"\"\n",
    "    Plots the top N most correlated instrument pairs from a correlation matrix.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (instruments, time) closing price matrix\n",
    "        corr_matrix (ndarray): (instruments, instruments) correlation matrix\n",
    "        top_n (int): Number of top correlated pairs to plot\n",
    "        use_returns (bool): If True, plot log returns instead of prices\n",
    "        smooth (bool): Whether to apply moving average smoothing\n",
    "        smooth_window (int): Window size for smoothing (in days)\n",
    "    \"\"\"\n",
    "    # Step 1: Get top correlated index pairs\n",
    "    n = corr_matrix.shape[0]\n",
    "    upper = np.triu_indices(n, k=1)\n",
    "    corr_values = corr_matrix[upper]\n",
    "    index_pairs = list(zip(upper[0], upper[1]))\n",
    "    sorted_pairs = sorted(zip(index_pairs, corr_values), key=lambda x: x[1], reverse=True)\n",
    "    top_pairs = sorted_pairs[:top_n]\n",
    "\n",
    "    # Step 2: Compute returns or use prices\n",
    "    if use_returns:\n",
    "        log_prices = np.log(prc_matrix)\n",
    "        data = np.diff(log_prices, axis=1)\n",
    "        ylabel = \"Log Returns\"\n",
    "    else:\n",
    "        data = prc_matrix\n",
    "        ylabel = \"Price\"\n",
    "\n",
    "    # Step 3: Optional smoothing\n",
    "    if smooth:\n",
    "        # Apply moving average across time axis (axis=1)\n",
    "        kernel = np.ones(smooth_window) / smooth_window\n",
    "        data = np.array([\n",
    "            np.convolve(row, kernel, mode='valid')\n",
    "            for row in data\n",
    "        ])\n",
    "        xlabel = f\"Time (Days) [Smoothed, Window={smooth_window}]\"\n",
    "    else:\n",
    "        xlabel = \"Time (Days)\"\n",
    "\n",
    "    # Step 4: Plot each pair\n",
    "    for idx, ((i, j), corr_val) in enumerate(top_pairs, 1):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[i], label=f\"Instrument {i}\", linewidth=2)\n",
    "        plt.plot(data[j], label=f\"Instrument {j}\", linewidth=2, linestyle='--')\n",
    "        plt.title(f\"Top {idx}: Instruments {i} & {j} (Correlation = {corr_val:.4f})\")\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_top_correlated_pairs(data_d, corr_matrix, 10, use_returns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_symmetry(matrix):\n",
    "    rows = len(matrix)\n",
    "    cols = len(matrix[0])\n",
    "    \n",
    "    if rows != cols:\n",
    "        print(\"Matrix is not square, so it cannot be symmetric.\")\n",
    "        return\n",
    "\n",
    "    symmetric = True\n",
    "    for i in range(rows):\n",
    "        for j in range(i + 1, cols):  # only need to check above diagonal\n",
    "            if matrix[i][j] != matrix[j][i]:\n",
    "                print(f\"Mismatch at ({i}, {j}) and ({j}, {i}): {matrix[i][j]} != {matrix[j][i]}\")\n",
    "                symmetric = False\n",
    "    \n",
    "    if symmetric:\n",
    "        print(\"Matrix is symmetric.\")\n",
    "    else:\n",
    "        print(\"Matrix is NOT symmetric.\")\n",
    "corr_matrix == corr_matrix.T\n",
    "A = corr_matrix\n",
    "is_symmetric = check_symmetry(A)\n",
    "is_symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable Params\n",
    "cluster_cutoff = 0.5\n",
    "\n",
    "# Clustering Similar points! Use hierarchical clustering (e.g. Ward's method)\n",
    "distance_matrix = 1 - np.abs(np.round(corr_matrix, decimals=10))\n",
    "linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix, no_labels=True, color_threshold=cluster_cutoff)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Instrument Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "clusters = fcluster(linkage_matrix, t=cluster_cutoff, criterion='distance')\n",
    "\n",
    "# Sort by cluster and Plot!\n",
    "sorted_idx = np.argsort(clusters)\n",
    "sorted_corr = corr_matrix[sorted_idx][:, sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sorted_corr, cmap='coolwarm', vmin=-1, vmax=1, square=True)\n",
    "plt.title(\"Correlation Matrix (Clustered)\")\n",
    "plt.show()\n",
    "\n",
    "# Using PCA to view it visually!\n",
    "# Transpose to shape (n_samples, n_features) = (50, 449)\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_coords = pca.fit_transform(log_returns)\n",
    "# pca_coords shape: (50, 2)\n",
    "\n",
    "def compute_atr(prices, window=14):\n",
    "    \"\"\"\n",
    "    Compute Average True Range (ATR) for each price series.\n",
    "    Assumes prices is a (n_instruments, n_days) ndarray of close prices.\n",
    "    \"\"\"\n",
    "    highs = prices[:, 1:]\n",
    "    lows = prices[:, :-1]\n",
    "    closes = prices[:, :-1]\n",
    "    \n",
    "    tr = np.abs(highs - lows)  # approximate true range as abs(diff)\n",
    "    atr = np.zeros_like(prices)\n",
    "    atr[:, window:] = np.apply_along_axis(\n",
    "        lambda x: np.convolve(x, np.ones(window)/window, mode='valid'), \n",
    "        axis=1, arr=tr\n",
    "    )\n",
    "    atr[:, :window] = np.nan  # pad initial values\n",
    "    return atr\n",
    "\n",
    "def volatility_normalized_prices(prices, atr_window=14, use_log=False):\n",
    "    \"\"\"\n",
    "    Normalize price series by ATR.\n",
    "    Returns volatility-adjusted cumulative return.\n",
    "    \"\"\"\n",
    "    if use_log:\n",
    "        log_returns = np.diff(np.log(prices), axis=1)\n",
    "    else:\n",
    "        log_returns = np.diff(prices, axis=1)\n",
    "    atr = compute_atr(prices, window=atr_window)[:, 1:]  # align shape\n",
    "\n",
    "    # Avoid divide by zero\n",
    "    atr[atr == 0] = np.nan\n",
    "    norm_returns = log_returns / atr\n",
    "\n",
    "    # Cumulative sum to reconstruct a path\n",
    "    norm_prices = np.nancumsum(norm_returns, axis=1)\n",
    "    norm_prices = np.hstack([np.zeros((prices.shape[0], 1)), norm_prices])  # add zero at start\n",
    "\n",
    "    return norm_prices\n",
    "\n",
    "def log_returns_normalized_prices(prices):\n",
    "    log_returns = np.log(prices)\n",
    "    transformed = log_returns - log_returns[:, [0]]\n",
    "    return transformed\n",
    "\n",
    "def z_score_normalized_prices(prices):\n",
    "    return (prices- prices.mean(axis=1, keepdims=True)) / prices.std(axis=1, keepdims=True)\n",
    "\n",
    "def percent_change_normalized_prices(prices):\n",
    "    return prices/ prices[:, [0]]\n",
    "\n",
    "def plot_pca_projection(pca_coords, clusters):\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    # Step 5: Plot PCA with cluster coloring\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    palette = sns.color_palette(\"hsv\", n_clusters)\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        idx = np.where(clusters == cluster_id)[0]\n",
    "        plt.scatter(\n",
    "            pca_coords[idx, 0], pca_coords[idx, 1],\n",
    "            s=80, edgecolor='k', label=f\"Cluster {cluster_id}\",\n",
    "            color=palette[cluster_id - 1]\n",
    "        )\n",
    "        for i in idx:\n",
    "            plt.text(pca_coords[i, 0], pca_coords[i, 1], str(i), fontsize=8,\n",
    "                     ha='center', va='center', color='white', weight='bold')\n",
    "\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(\"PCA Projection Colored by Correlation Clusters\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_all_normalizations(\n",
    "    data,\n",
    "    cluster_labels,\n",
    "    cluster_id,\n",
    "    save_plot=False,\n",
    "    output_dir=\"cluster_all_normalizations\",\n",
    "    tickers=None,\n",
    "    show_volatility_band=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a single cluster's price series using 4 normalization methods:\n",
    "    - Log Percent Change from first value\n",
    "    - Z-Score normalization\n",
    "    - ATR Normalised\n",
    "    - ATR Log Normalised\n",
    "    Each with a cluster average line and optional volatility band.\n",
    "\n",
    "    Parameters:\n",
    "        data (ndarray): (n_instruments, n_days) price matrix\n",
    "        cluster_labels (ndarray): cluster ID array (len = n_instruments)\n",
    "        cluster_id (int): The cluster ID to plot\n",
    "        save_plot (bool): If True, saves the plot to disk\n",
    "        output_dir (str): Folder to save plots in\n",
    "        tickers (list of str): Optional list of instrument names\n",
    "        show_volatility_band (bool): If True, display ±1 std band around cluster average\n",
    "    \"\"\"\n",
    "    idx = np.where(cluster_labels == cluster_id)[0]\n",
    "    if len(idx) <= 1:\n",
    "        print(f\"Cluster {cluster_id} skipped (singleton or empty).\")\n",
    "        return\n",
    "\n",
    "    cluster_data = data[idx]\n",
    "    if tickers is not None:\n",
    "        tickers = [tickers[i] for i in idx]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(f\"Cluster {cluster_id} - Normalization Comparison with Average and Volatility Band\", fontsize=16)\n",
    "\n",
    "    norm_methods = [\"Percent Change\", \"Z-Score\", \"ATR Change\", \"ATR Log Change\"]\n",
    "\n",
    "    for k, method in enumerate(norm_methods):\n",
    "        ax = axes[k // 2, k % 2]\n",
    "\n",
    "        if method == \"Percent Change\":\n",
    "            transformed = percent_change_normalized_prices(cluster_data)\n",
    "        elif method == \"Z-Score\":\n",
    "            transformed = z_score_normalized_prices(cluster_data)\n",
    "        elif method == \"ATR Change\":\n",
    "            transformed = volatility_normalized_prices(cluster_data, 14, False)\n",
    "        elif method == \"ATR Log Change\":\n",
    "            transformed = volatility_normalized_prices(cluster_data, 14, True)\n",
    "            # returns = np.diff(cluster_data, axis=1) / cluster_data[:, :-1]\n",
    "            # returns = np.hstack([np.zeros((returns.shape[0], 1)), returns])\n",
    "            # transformed = np.cumsum(returns, axis=1)\n",
    "\n",
    "        # Plot individual lines\n",
    "        for i, series in enumerate(transformed):\n",
    "            label = tickers[i] if tickers else f\"Instrument {idx[i]}\"\n",
    "            ax.plot(series, label=label, linewidth=1.2)\n",
    "\n",
    "        # Cluster average\n",
    "        cluster_avg = np.nanmean(transformed, axis=0)\n",
    "        ax.plot(cluster_avg, label=\"Cluster Average\", linewidth=2.5, color=\"black\", zorder=10)\n",
    "\n",
    "        # Volatility band (±1 std)\n",
    "        if show_volatility_band:\n",
    "            cluster_std = np.nanstd(transformed, axis=0)\n",
    "            ax.fill_between(\n",
    "                np.arange(transformed.shape[1]),\n",
    "                cluster_avg - cluster_std,\n",
    "                cluster_avg + cluster_std,\n",
    "                color='gray',\n",
    "                alpha=0.3,\n",
    "                label=\"±1 Std Dev\"\n",
    "            )\n",
    "\n",
    "        ax.set_title(method)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Legend (outside grid so it's shared across all subplots)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=min(len(labels), 6), bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "\n",
    "    if save_plot:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.join(output_dir, f\"cluster_{cluster_id}_normalizations.png\")\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        print(f\"Saved to {filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_clusters_price_series(\n",
    "    data,\n",
    "    cluster_labels,\n",
    "    save_plots=False,\n",
    "    output_dir=\"cluster_plots_colored\",\n",
    "    show_colorbar=True,\n",
    "    color_by_correlation=True,\n",
    "    normalization_method=\"percent\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots clustered instrument price series with either:\n",
    "    - heatmap coloring by correlation to cluster average, OR\n",
    "    - distinct colors for each instrument (default style).\n",
    "\n",
    "    Parameters:\n",
    "        data (ndarray): (n_instruments, n_days) price matrix\n",
    "        cluster_labels (ndarray): cluster ID array (len = n_instruments)\n",
    "        normalize (bool): Normalize each series to start at 1\n",
    "        save_plots (bool): Save plots to disk\n",
    "        output_dir (str): Folder to save plots in (created if not exists)\n",
    "        show_colorbar (bool): Show correlation colorbar if heatmap used\n",
    "        color_by_correlation (bool): If True, use heatmap coloring\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "    if save_plots and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        idx = np.where(cluster_labels == cluster_id)[0]\n",
    "        if len(idx) <= 1:\n",
    "            continue  # skip singleton clusters\n",
    "\n",
    "        cluster_data = data[idx]\n",
    "\n",
    "        # Normalize if requested\n",
    "        if normalization_method == \"percent\":\n",
    "            cluster_data = cluster_data / cluster_data[:, [0]]\n",
    "        elif normalization_method == \"zscore\":\n",
    "            cluster_data = (cluster_data - cluster_data.mean(axis=1, keepdims=True)) / cluster_data.std(axis=1, keepdims=True)\n",
    "        elif normalization_method == \"log\":\n",
    "            cluster_data = np.log(cluster_data)\n",
    "            cluster_data = cluster_data - cluster_data[:, [0]]\n",
    "        elif normalization_method == \"cumulative_return\":\n",
    "            returns = np.diff(cluster_data, axis=1) / cluster_data[:, :-1]\n",
    "            returns = np.hstack([np.zeros((returns.shape[0], 1)), returns])\n",
    "            cluster_data = np.cumsum(returns, axis=1)\n",
    "\n",
    "\n",
    "        avg_series = cluster_data.mean(axis=0)\n",
    "\n",
    "        # Determine colors\n",
    "        if color_by_correlation:\n",
    "            corr_to_avg = []\n",
    "            for series in cluster_data:\n",
    "                if np.std(series) == 0 or np.std(avg_series) == 0:\n",
    "                    corr = 0\n",
    "                else:\n",
    "                    corr = np.corrcoef(series, avg_series)[0, 1]\n",
    "                corr_to_avg.append(corr)\n",
    "            corr_to_avg = np.clip(corr_to_avg, -1, 1)\n",
    "\n",
    "            cmap = coolwarm\n",
    "            norm = Normalize(vmin=-1, vmax=1)\n",
    "            colors = [cmap(norm(c)) for c in corr_to_avg]\n",
    "        else:\n",
    "            # Use a qualitative colormap for distinct colors\n",
    "            cmap = get_cmap(\"tab10\")  # good for ≤10 items; use 'tab20' for larger clusters\n",
    "            colors = [cmap(i % cmap.N) for i in range(len(idx))]\n",
    "\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        for i, (series, color) in zip(idx, zip(cluster_data, colors)):\n",
    "            ax.plot(series, label=f\"Instrument {i}\", linewidth=1.2, alpha=0.85, color=color)\n",
    "\n",
    "        ax.plot(avg_series, color='black', linewidth=3, label=\"Cluster Avg\")\n",
    "\n",
    "        ax.set_title(f\"Cluster {cluster_id} — {len(idx)} Instruments\")\n",
    "        ax.set_xlabel(\"Time (Days)\")\n",
    "        ax.set_ylabel(f\"{normalization_method} - Price\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "        # Optional colorbar (only for heatmap-style)\n",
    "        if color_by_correlation and show_colorbar:\n",
    "            sm = plt.cm.ScalarMappable(cmap=coolwarm, norm=Normalize(vmin=-1, vmax=1))\n",
    "            sm.set_array([])\n",
    "            cbar = fig.colorbar(sm, ax=ax, orientation='vertical', shrink=0.8)\n",
    "            cbar.set_label('Correlation with Cluster Avg', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_plots:\n",
    "            filename = f\"cluster_{cluster_id}.png\"\n",
    "            plt.savefig(os.path.join(output_dir, filename))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# plot_pca_projection(pca_coords, clusters)\n",
    "\n",
    "# Blue colour represents negatively correlated, higher shades of red represent stronger average correlation\n",
    "norm_type = \"percent\"\n",
    "# for norm_type in [\"percent\", \"zscore\", \"log\", \"cumulative_return\"]:\n",
    "for i in range(1, max(clusters)+1):\n",
    "    plot_cluster_all_normalizations(\n",
    "        data=prcCheckPrev,\n",
    "        cluster_labels=clusters,\n",
    "        cluster_id=i,\n",
    "        save_plot=False,\n",
    "        tickers=None,\n",
    "        show_volatility_band=True\n",
    "    )\n",
    "# plot_clusters_price_series(\n",
    "#     prcCheckPrev,\n",
    "#     clusters,\n",
    "#     save_plots=False,\n",
    "#     output_dir=\"cluster_charts\",\n",
    "#     show_colorbar=False,\n",
    "#     color_by_correlation=True,\n",
    "#     normalization_method=norm_type # percent, zscore, log, cumulative_return\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying significance of the correlation in terms of stationary mean!\n",
    "def adf_test_with_plot(series, title=\"ADF Test\"):\n",
    "    \"\"\"\n",
    "    Performs ADF test and plots the input time series.\n",
    "\n",
    "    Parameters:\n",
    "        series (array-like): Time series data (1D)\n",
    "        title (str): Optional title for the plot\n",
    "\n",
    "    Prints:\n",
    "        ADF statistic, p-value, and critical values\n",
    "    \"\"\"\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    stat, pval, used_lag, n_obs, crit_vals, ic_best = result\n",
    "\n",
    "    print(f\"ADF Statistic: {stat:.4f}\")\n",
    "    print(f\"P-value: {pval:.4f}\")\n",
    "    print(f\"Used Lag: {used_lag}\")\n",
    "    print(f\"Number of Observations: {n_obs}\")\n",
    "    for key, value in crit_vals.items():\n",
    "        print(f\"Critical Value ({key}): {value:.4f}\")\n",
    "\n",
    "    # Plot the series\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(series, label=\"Time Series\", color='steelblue')\n",
    "    plt.title(f\"{title}\\nADF Stat = {stat:.4f}, p = {pval:.4f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_pair_cointegration(prc_pair):\n",
    "    price_a = prc_pair[0]\n",
    "    price_b = prc_pair[1]\n",
    "    # 1. Step: OLS regression\n",
    "    beta = np.polyfit(price_b, price_a, 1)[0]\n",
    "    spread = price_a - beta * price_b\n",
    "\n",
    "    # 2. Step: ADF test on spread\n",
    "    adf_result = adfuller(spread)\n",
    "    print(\"ADF Test on Spread:\")\n",
    "    print(f\"ADF Statistic: {adf_result[0]}\")\n",
    "    print(f\"P-value: {adf_result[1]}\")\n",
    "\n",
    "    # 3. Step: Cointegration test\n",
    "    coint_t, p_value, crit = coint(price_a, price_b)\n",
    "    print(\"\\nEngle-Granger Cointegration Test:\")\n",
    "    print(f\"Test Statistic: {coint_t}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    print(f\"Critical Values: {crit}\")\n",
    "\n",
    "def adf_on_log_ratio(prc_pair, title=\"ADF on Log Price Ratio\"):\n",
    "    \"\"\"\n",
    "    Computes the log ratio of two price series and runs ADF test on it.\n",
    "\n",
    "    Parameters:\n",
    "        price_a (array-like): Price series of instrument A\n",
    "        price_b (array-like): Price series of instrument B\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    price_a = prc_pair[0]\n",
    "    price_b = prc_pair[1]\n",
    "    # Step 1: Compute log ratio\n",
    "    log_ratio = np.log(price_a) - np.log(price_b)\n",
    "\n",
    "    # Step 2: ADF test\n",
    "    adf_result = adfuller(log_ratio)\n",
    "    stat, pval, used_lag, n_obs, crit_vals, _ = adf_result\n",
    "\n",
    "    print(f\"ADF Test on log(A/B):\")\n",
    "    print(f\"  ADF Statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {pval:.4f}\")\n",
    "    print(f\"  Used Lag: {used_lag}\")\n",
    "    print(f\"  Observations: {n_obs}\")\n",
    "    for key, value in crit_vals.items():\n",
    "        print(f\"  Critical Value ({key}): {value:.4f}\")\n",
    "\n",
    "    # Step 3: Plot log ratio\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(log_ratio, label=\"log(A / B)\", color='slateblue')\n",
    "    plt.axhline(0, linestyle='--', color='black', alpha=0.7)\n",
    "    plt.title(f\"{title}\\nADF Stat = {stat:.4f}, p = {pval:.4f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Log Price Ratio\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def adf_on_ols_residuals(prc_pair, title=\"ADF on OLS Residuals\"):\n",
    "    \"\"\"\n",
    "    Regresses price_a on price_b, computes residuals, runs ADF test on residuals,\n",
    "    and plots the residual series.\n",
    "\n",
    "    Parameters:\n",
    "        price_a (array-like): Dependent variable (e.g. stock A)\n",
    "        price_b (array-like): Independent variable (e.g. stock B)\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    price_a = prc_pair[0]\n",
    "    price_b = prc_pair[1]\n",
    "    # Step 1: Regress A on B\n",
    "    price_b_const = sm.add_constant(price_b)\n",
    "    model = sm.OLS(price_a, price_b_const)\n",
    "    result = model.fit()\n",
    "    residuals = result.resid\n",
    "\n",
    "    # Step 2: ADF test on residuals\n",
    "    adf_result = adfuller(residuals)\n",
    "    stat, pval, used_lag, n_obs, crit_vals, _ = adf_result\n",
    "\n",
    "    print(f\"ADF Test on Residuals:\")\n",
    "    print(f\"  ADF Statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {pval:.4f}\")\n",
    "    print(f\"  Used Lag: {used_lag}\")\n",
    "    print(f\"  Observations: {n_obs}\")\n",
    "    for key, value in crit_vals.items():\n",
    "        print(f\"  Critical Value ({key}): {value:.4f}\")\n",
    "\n",
    "    # Step 3: Plot residuals\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(residuals, label=\"OLS Residuals\", color='darkorange')\n",
    "    plt.axhline(0, color='black', linestyle='--', alpha=0.7)\n",
    "    plt.title(f\"{title}\\nADF Stat = {stat:.4f}, p = {pval:.4f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def adf_on_ols_residuals_single(price_a, price_b):\n",
    "    \"\"\"\n",
    "    Run OLS regression of price_a on price_b, compute residuals,\n",
    "    perform ADF test on residuals, and return p-value and coefficients.\n",
    "    \"\"\"\n",
    "    price_a = np.asarray(price_a)\n",
    "    price_b = np.asarray(price_b)\n",
    "    \n",
    "    # Add intercept\n",
    "    price_b_const = sm.add_constant(price_b)\n",
    "    model = sm.OLS(price_a, price_b_const)\n",
    "    result = model.fit()\n",
    "    \n",
    "    residuals = result.resid\n",
    "    adf_result = adfuller(residuals)\n",
    "    p_value = adf_result[1]  # p-value\n",
    "    \n",
    "    alpha = result.params[0]\n",
    "    beta = result.params[1]\n",
    "    \n",
    "    return p_value, beta, alpha\n",
    "\n",
    "\n",
    "def find_top_cointegrated_instruments(prc_matrix, target_idx, top_n=5):\n",
    "    \"\"\"\n",
    "    Finds top N instruments most cointegrated with target_idx instrument.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (n_instruments, n_days) price matrix\n",
    "        target_idx (int): index of target instrument\n",
    "        top_n (int): number of top pairs to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(instrument_idx, p_value, beta, intercept), ...] sorted by p_value ascending\n",
    "    \"\"\"\n",
    "    n = prc_matrix.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if i == target_idx:\n",
    "            continue\n",
    "        \n",
    "        p_val, beta, alpha = adf_on_ols_residuals_single(prc_matrix[target_idx], prc_matrix[i])\n",
    "        results.append((i, p_val, beta, alpha))\n",
    "\n",
    "    # Sort by p-value ascending (lowest p = strongest cointegration)\n",
    "    results.sort(key=lambda x: x[1])\n",
    "\n",
    "    return results[:top_n]\n",
    "\n",
    "inst_1 = 48\n",
    "inst_2 = 49\n",
    "prc = prcTest[[inst_1, inst_2], :]\n",
    "prcz = z_score_normalized_prices(prc)\n",
    "prcatr = volatility_normalized_prices(prc)\n",
    "prcatrlog = volatility_normalized_prices(prc, 14, True)\n",
    "\n",
    "# adf_on_ols_residuals([data_d[42], np.zeros(450)]) # OLS price difference\n",
    "adf_on_ols_residuals(prcz) # OLS price difference\n",
    "adf_on_ols_residuals(prcatr) # OLS price difference\n",
    "adf_on_ols_residuals(prcatrlog) # OLS price difference\n",
    "# adf_on_log_ratio(prc)\n",
    "\n",
    "\n",
    "target = inst_1\n",
    "top_cointegrated = find_top_cointegrated_instruments(data_d, target, 5)\n",
    "    \n",
    "print(f\"Top {5} cointegrated instruments with instrument {target}:\")\n",
    "for inst_idx, pval, beta, alpha in top_cointegrated:\n",
    "    print(f\"Instrument {inst_idx}: p-value={pval:.6f}, beta={beta:.4f}, intercept={alpha:.4f}\")\n",
    "# check_pair_cointegration(prcz)\n",
    "\n",
    "# check_pair_cointegration(prcz)\n",
    "# check_pair_cointegration()\n",
    "# check_pair_cointegration(prcz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cointegration_pval_matrix_with_params(prc_matrix, adf_alpha=0.1):\n",
    "    \"\"\"\n",
    "    Computes pairwise cointegration p-values from OLS residuals and returns\n",
    "    corresponding alpha and beta matrices. Skips pairs if either instrument is\n",
    "    individually stationary based on ADF test.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (n_instruments, n_days) price matrix\n",
    "        adf_alpha (float): significance level for skipping stationary instruments (default 0.1)\n",
    "\n",
    "    Returns:\n",
    "        pval_matrix (ndarray): cointegration p-values\n",
    "        beta_matrix (ndarray): OLS slope for each pair\n",
    "        alpha_matrix (ndarray): OLS intercept for each pair\n",
    "    \"\"\"\n",
    "    n = prc_matrix.shape[0]\n",
    "    pval_matrix = np.ones((n, n))\n",
    "    beta_matrix = np.full((n, n), np.nan)\n",
    "    alpha_matrix = np.full((n, n), np.nan)\n",
    "\n",
    "    # Check for stationarity of individual instruments\n",
    "    stationary_flags = []\n",
    "    for i in range(n):\n",
    "        price = prc_matrix[i]\n",
    "        price = price[~np.isnan(price)]\n",
    "        if len(price) < 30:\n",
    "            stationary_flags.append(True)  # treat short series as stationary to skip\n",
    "            continue\n",
    "        stat, pval, *_ = adfuller(price)\n",
    "        stationary_flags.append(pval < adf_alpha)\n",
    "\n",
    "    # Pairwise cointegration test via OLS residuals\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if stationary_flags[i] or stationary_flags[j]:\n",
    "                continue  # skip if either series is stationary\n",
    "\n",
    "            price_i = prc_matrix[i]\n",
    "            price_j = prc_matrix[j]\n",
    "\n",
    "            # Drop aligned NaNs\n",
    "            mask = ~np.isnan(price_i) & ~np.isnan(price_j)\n",
    "            if np.sum(mask) < 30:\n",
    "                continue\n",
    "\n",
    "            y = price_i[mask]\n",
    "            x = price_j[mask]\n",
    "            x_const = sm.add_constant(x)\n",
    "            model = sm.OLS(y, x_const).fit()\n",
    "            residuals = model.resid\n",
    "\n",
    "            try:\n",
    "                adf_stat, pval, *_ = adfuller(residuals)\n",
    "                pval_matrix[i, j] = pval\n",
    "                beta_matrix[i, j] = model.params[1]\n",
    "                alpha_matrix[i, j] = model.params[0]\n",
    "            except:\n",
    "                # If regression or adfuller fails, leave values at default\n",
    "                pass\n",
    "\n",
    "    return pval_matrix, beta_matrix, alpha_matrix\n",
    "\n",
    "def plot_cointegration_heatmap(pval_matrix, alpha=0.05, tickers=None):\n",
    "    \"\"\"\n",
    "    Plot heatmap of cointegration p-values with optional significance masking.\n",
    "\n",
    "    Parameters:\n",
    "        pval_matrix (ndarray): (n, n) matrix of p-values\n",
    "        alpha (float): Significance threshold (e.g., 0.05)\n",
    "        tickers (list[str]): Optional list of labels\n",
    "    \"\"\"\n",
    "    mask = np.triu(np.ones_like(pval_matrix, dtype=bool))\n",
    "    sig_mask = (pval_matrix > alpha)\n",
    "    if tickers is None:\n",
    "        tickers = [f\"{i}\" for i in range(pval_matrix.shape[0])]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(pval_matrix,\n",
    "                mask=mask,\n",
    "                cmap='viridis_r',\n",
    "                annot=False,\n",
    "                fmt=\".2f\",\n",
    "                cbar_kws={\"label\": \"Cointegration p-value\"},\n",
    "                linewidths=0.5,\n",
    "                square=True,\n",
    "                xticklabels=tickers,\n",
    "                yticklabels=tickers,\n",
    "                vmax=0.2)  # upper limit set to emphasize low p-values\n",
    "    plt.title(f'Pairwise Cointegration Heatmap (α = {alpha})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_significant_cointegrated_pairs(pval_matrix, beta_matrix, alpha=0.05, tickers=None, low_beta=0.7, high_beta=1.3):\n",
    "    \"\"\"\n",
    "    Extracts and prints all instrument pairs with cointegration p-values below alpha.\n",
    "\n",
    "    Parameters:\n",
    "        pval_matrix (ndarray): (n, n) matrix of cointegration p-values\n",
    "        alpha (float): significance level threshold\n",
    "        tickers (list of str, optional): list of instrument names\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: [(i, j, pval), ...] where i < j and pval < alpha\n",
    "    \"\"\"\n",
    "    n = pval_matrix.shape[0]\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pval = pval_matrix[i, j]\n",
    "            beta = beta_matrix[i, j]\n",
    "            if pval < alpha and low_beta <= beta and beta <= high_beta:\n",
    "                name_i = tickers[i] if tickers else i\n",
    "                name_j = tickers[j] if tickers else j\n",
    "                pairs.append((name_i, name_j, pval))\n",
    "\n",
    "    print(f\"Found {len(pairs)} cointegrated pairs with p-value < {alpha}\")\n",
    "    for name_i, name_j, pval in sorted(pairs, key=lambda x: x[2]):\n",
    "        print(f\"{name_i} & {name_j} → p = {pval:.4f}\")\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def plot_cointegrated_pair_spread(prc_matrix, i, j, beta_matrix, alpha_matrix, show_residual=False):\n",
    "    \"\"\"\n",
    "    Plots the price of instrument i and the linear combination (alpha + beta * price_j) of instrument j.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (n_instruments, n_days) price matrix\n",
    "        i (int): index of first instrument\n",
    "        j (int): index of second instrument\n",
    "        beta_matrix (ndarray): matrix of OLS betas\n",
    "        alpha_matrix (ndarray): matrix of OLS alphas\n",
    "        show_residual (bool): whether to plot residual spread\n",
    "    \"\"\"\n",
    "    price_i = prc_matrix[i]\n",
    "    price_j = prc_matrix[j]\n",
    "\n",
    "    # Drop NaNs while keeping alignment\n",
    "    mask = ~np.isnan(price_i) & ~np.isnan(price_j)\n",
    "    if np.sum(mask) < 30:\n",
    "        print(\"Insufficient data after NaN removal.\")\n",
    "        return\n",
    "\n",
    "    price_i = price_i[mask]\n",
    "    price_j = price_j[mask]\n",
    "\n",
    "    beta = beta_matrix[i, j]\n",
    "    alpha = alpha_matrix[i, j]\n",
    "    hedge_line = alpha + beta * price_j\n",
    "    residual = price_i - hedge_line\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(price_i, label=f'Instrument {i} Price', linewidth=2)\n",
    "    plt.plot(hedge_line, label=f'Hedge Line: α + β·Instrument {j}', linestyle='--', linewidth=2)\n",
    "\n",
    "    if show_residual:\n",
    "        plt.plot(residual, label=\"Residual (Spread)\", color='gray', alpha=0.5)\n",
    "\n",
    "    plt.title(f\"Cointegrated Pair: {i} & {j} | β = {beta:.2f}, α = {alpha:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "alpha_1 = 0.05\n",
    "pval_matrix, beta_matrix, alpha_matrix = compute_cointegration_pval_matrix_with_params(prcTest, adf_alpha=alpha_1) # alpha excludes already stationary ones!\n",
    "plot_cointegration_heatmap(pval_matrix, alpha=0.05)\n",
    "significant_pairs = get_significant_cointegrated_pairs(pval_matrix, beta_matrix, alpha=alpha_1, low_beta=-10, high_beta=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (i, j, p) in significant_pairs[:]:  # Top 5 pairs\n",
    "    plot_cointegrated_pair_spread(prcCheckPrev, i, j, beta_matrix, alpha_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mean_reversion(prc_future, i, j, beta_matrix, alpha_matrix, plot=True):\n",
    "    \"\"\"\n",
    "    Evaluates mean reversion on a cointegrated pair using future price data.\n",
    "\n",
    "    Parameters:\n",
    "        prc_train (ndarray): (n_instruments, n_days_train) training price data\n",
    "        prc_future (ndarray): (n_instruments, n_days_future) future price data\n",
    "        i (int): Index of instrument A\n",
    "        j (int): Index of instrument B\n",
    "        beta_matrix (ndarray): beta hedge ratio matrix from training\n",
    "        alpha_matrix (ndarray): alpha intercept matrix from training\n",
    "        plot (bool): Whether to plot the spread and mean line\n",
    "\n",
    "    Returns:\n",
    "        adf_stat (float): ADF statistic on the spread during future period\n",
    "        pval (float): p-value from ADF test\n",
    "        spread (ndarray): time series of the spread\n",
    "    \"\"\"\n",
    "    if np.isnan(beta_matrix[i, j]) or np.isnan(alpha_matrix[i, j]):\n",
    "        print(\"Warning: No regression values available for this pair.\")\n",
    "        return None, None, None\n",
    "\n",
    "    beta = beta_matrix[i, j]\n",
    "    alpha = alpha_matrix[i, j]\n",
    "\n",
    "    price_a = prc_future[i]\n",
    "    price_b = prc_future[j]\n",
    "\n",
    "    # Drop aligned NaNs\n",
    "    mask = ~np.isnan(price_a) & ~np.isnan(price_b)\n",
    "    price_a = price_a[mask]\n",
    "    price_b = price_b[mask]\n",
    "\n",
    "    if len(price_a) < 30:\n",
    "        print(\"Insufficient data in future period.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Compute spread = A - β * B - α\n",
    "    spread = price_a - beta * price_b - alpha\n",
    "\n",
    "    # Perform ADF test on the spread\n",
    "    adf_result = adfuller(spread)\n",
    "    adf_stat = adf_result[0]\n",
    "    pval = adf_result[1]\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(spread, label=\"Spread (A - βB - α)\", color='teal')\n",
    "        plt.axhline(spread.mean(), color='red', linestyle='--', label='Mean')\n",
    "        plt.title(f\"Spread in Future Period - Inst {i} & {j} - (ADF p-value = {pval:.4f})\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Spread\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return adf_stat, pval, spread\n",
    "\n",
    "\n",
    "def evaluate_mean_reversion_change(prc_train, prc_future, i, j, beta_matrix, alpha_matrix, plot=True):\n",
    "    \"\"\"\n",
    "    Evaluates change in cointegration strength (ADF p-value) by comparing training vs full period.\n",
    "\n",
    "    Parameters:\n",
    "        prc_train (ndarray): (n_instruments, n_days_train)\n",
    "        prc_future (ndarray): (n_instruments, n_days_future)\n",
    "        i (int): Index of instrument A\n",
    "        j (int): Index of instrument B\n",
    "        beta_matrix (ndarray): Trained beta matrix\n",
    "        alpha_matrix (ndarray): Trained alpha matrix\n",
    "        plot (bool): If True, plots full spread and mean\n",
    "\n",
    "    Returns:\n",
    "        pval_train (float): ADF p-value on training spread\n",
    "        pval_full (float): ADF p-value on combined spread\n",
    "        delta_pval (float): Difference (pval_full - pval_train)\n",
    "    \"\"\"\n",
    "    if np.isnan(beta_matrix[i, j]) or np.isnan(alpha_matrix[i, j]):\n",
    "        print(\"No regression coefficients for pair.\")\n",
    "        return None, None, None\n",
    "\n",
    "    beta = beta_matrix[i, j]\n",
    "    alpha = alpha_matrix[i, j]\n",
    "\n",
    "    # Combine data\n",
    "    price_a = np.concatenate([prc_train[i], prc_future[i]])\n",
    "    price_b = np.concatenate([prc_train[j], prc_future[j]])\n",
    "\n",
    "    # Mask to drop nan\n",
    "    mask = ~np.isnan(price_a) & ~np.isnan(price_b)\n",
    "    price_a = price_a[mask]\n",
    "    price_b = price_b[mask]\n",
    "\n",
    "    # Split into train/full spans after nan removal\n",
    "    n_train = prc_train.shape[1]\n",
    "    n_used = len(price_a)\n",
    "    split_idx = min(n_train, n_used)\n",
    "\n",
    "    price_a_train = price_a[:split_idx]\n",
    "    price_b_train = price_b[:split_idx]\n",
    "\n",
    "    # Compute spreads\n",
    "    spread_train = price_a_train - beta * price_b_train - alpha\n",
    "    spread_full = price_a - beta * price_b - alpha\n",
    "\n",
    "    # ADF tests\n",
    "    pval_train = adfuller(spread_train)[1]\n",
    "    pval_full = adfuller(spread_full)[1]\n",
    "    delta_pval = pval_full - pval_train\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(spread_full, label=\"Spread (A - βB - α)\", color='steelblue')\n",
    "        plt.axvline(split_idx, color='grey', linestyle='--', label='Train/Test Split')\n",
    "        plt.axhline(np.mean(spread_train), color='red', linestyle='--', label='Train Mean')\n",
    "        plt.title(f\"ADF Inst {i} & {j} - p  (Train: {pval_train:.4f} → Full: {pval_full:.4f}, Δ = {delta_pval:.4f})\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Spread\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return pval_train, pval_full, delta_pval\n",
    "\n",
    "keeping = list()\n",
    "\n",
    "alpha_2 = 0.01\n",
    "\n",
    "for (i, j, p) in significant_pairs[:]:  # Top 5 pairs\n",
    "\n",
    "    pval_train, pval_full, delta_pval = evaluate_mean_reversion_change(\n",
    "        prcTest, prcCheck,\n",
    "        i=i, j=j,\n",
    "        beta_matrix=beta_matrix,\n",
    "        alpha_matrix=alpha_matrix,\n",
    "        plot=False\n",
    "    )\n",
    "    if pval_full < alpha_2: \n",
    "        keeping.append((i, j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keeping)\n",
    "print(len(keeping))\n",
    "\n",
    "for i, j in keeping:\n",
    "    print(beta_matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscore_spreads(prc_matrix, keeping, beta_matrix, alpha_matrix, save=False, folder=\"zscore_spreads\"):\n",
    "    \"\"\"\n",
    "    Plots the z-score of the spread for each pair in 'keeping' using stored beta and alpha.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (n_instruments, n_days) price matrix\n",
    "        keeping (list of tuples): List of (i, j) index pairs\n",
    "        beta_matrix (ndarray): Matrix of regression beta values\n",
    "        alpha_matrix (ndarray): Matrix of regression alpha values\n",
    "        save (bool): If True, saves the plots to disk\n",
    "        folder (str): Folder name to save plots (created if not exists)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if save and not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    for i, j in keeping:\n",
    "        beta = beta_matrix[i, j]\n",
    "        alpha = alpha_matrix[i, j]\n",
    "\n",
    "        if np.isnan(beta) or np.isnan(alpha):\n",
    "            print(f\"Skipping pair ({i}, {j}) due to missing beta/alpha.\")\n",
    "            continue\n",
    "\n",
    "        price_a = prc_matrix[i]\n",
    "        price_b = prc_matrix[j]\n",
    "\n",
    "        # Remove nan jointly\n",
    "        mask = ~np.isnan(price_a) & ~np.isnan(price_b)\n",
    "        price_a = price_a[mask]\n",
    "        price_b = price_b[mask]\n",
    "\n",
    "        if len(price_a) < 10:\n",
    "            print(f\"Skipping pair ({i}, {j}) due to insufficient data.\")\n",
    "            continue\n",
    "\n",
    "        # Compute spread and z-score\n",
    "        spread = price_a - beta * price_b - alpha\n",
    "        spread_mean = np.mean(spread)\n",
    "        spread_std = np.std(spread)\n",
    "        zscore = (spread - spread_mean) / spread_std\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(zscore, label=f'Z-score (A{i} - βB{j} - α)', color='darkblue')\n",
    "        plt.axhline(0, linestyle='--', color='black')\n",
    "        plt.axhline(1, linestyle='--', color='green')\n",
    "        plt.axhline(-1, linestyle='--', color='red')\n",
    "        plt.title(f\"Z-score of Spread: Instrument {i} vs {j}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Z-score\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(os.path.join(folder, f\"zscore_{i}_{j}.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "plot_zscore_spreads(prc_matrix=prcCheckPrev, keeping=keeping, \n",
    "                    beta_matrix=beta_matrix, alpha_matrix=alpha_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "keeping_positive = list()\n",
    "for i, j in keeping:\n",
    "    if beta_matrix[i][j] > 0:\n",
    "        keeping_positive.append((i, j))\n",
    "\n",
    "print(keeping_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "final_keep = keeping\n",
    "\n",
    "# Save everything to a file\n",
    "with open(\"mean_reversion_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"beta_matrix\": beta_matrix,\n",
    "        \"alpha_matrix\": alpha_matrix,\n",
    "        \"pairs\": final_keep\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating rolling spearman correlation function\n",
    "def rolling_spearman(data_arr, inst_1, inst_2, window):\n",
    "    \"\"\"\n",
    "    Computes rolling Spearman correlation between two 1D price series.\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): 1D array of prices for instrument A\n",
    "        y (array-like): 1D array of prices for instrument B\n",
    "        window (int): rolling window size (number of days)\n",
    "\n",
    "    Returns:\n",
    "        corr_series (np.ndarray): array of Spearman correlations (length = len(x) - window + 1)\n",
    "    \"\"\"\n",
    "    x = np.asarray(data_arr[inst_1])\n",
    "    y = np.asarray(data_arr[inst_2])\n",
    "\n",
    "    assert x.shape == y.shape, \"Input arrays must be of the same length\"\n",
    "    assert x.ndim == y.ndim == 1, \"Inputs must be 1D arrays\"\n",
    "    assert window > 1 and window <= len(x), \"Invalid window size\"\n",
    "\n",
    "    corrs = []\n",
    "    for i in range(len(x) - window + 1):\n",
    "        x_window = x[i:i + window]\n",
    "        y_window = y[i:i + window]\n",
    "\n",
    "        if np.isnan(x_window).any() or np.isnan(y_window).any():\n",
    "            corrs.append(np.nan)\n",
    "        else:\n",
    "            corr, _ = spearmanr(x_window, y_window)\n",
    "            corrs.append(corr)\n",
    "\n",
    "    return np.array(corrs)\n",
    "\n",
    "def plot_colored_rolling_spearman(data_arr, inst_1, inst_2, window=30, high_thresh=0.2, low_thresh=-0.2, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots rolling Spearman correlation with color highlighting for high and low correlation zones.\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): 1D price series of instrument A\n",
    "        y (array-like): 1D price series of instrument B\n",
    "        window (int): rolling window size\n",
    "        high_thresh (float): threshold above which correlation is considered high\n",
    "        low_thresh (float): threshold below which correlation is considered low\n",
    "    \"\"\"\n",
    "    x = np.asarray(data_arr[inst_1])\n",
    "    y = np.asarray(data_arr[inst_2])\n",
    "\n",
    "\n",
    "    assert x.shape == y.shape, \"Input arrays must be of same length\"\n",
    "    if normalize:\n",
    "        x_plot = x / x[0]\n",
    "        y_plot = y / y[0]\n",
    "    else:\n",
    "        x_plot = x\n",
    "        y_plot = y\n",
    "\n",
    "    # Compute log returns\n",
    "    log_ret_x = np.diff(np.log(x))\n",
    "    log_ret_y = np.diff(np.log(y))\n",
    "\n",
    "    # Rolling Spearman correlation on log returns\n",
    "    corrs = []\n",
    "    for i in range(len(log_ret_x) - window + 1):\n",
    "        x_win = log_ret_x[i:i + window]\n",
    "        y_win = log_ret_y[i:i + window]\n",
    "\n",
    "        if np.isnan(x_win).any() or np.isnan(y_win).any():\n",
    "            corrs.append(np.nan)\n",
    "        else:\n",
    "            c, _ = spearmanr(x_win, y_win)\n",
    "            corrs.append(c)\n",
    "\n",
    "    corrs = np.array(corrs)\n",
    "    t_corr = np.arange(window, len(x))  # Match with price time axis\n",
    "\n",
    "    # Plot prices with background coloring\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(x_plot, label='Instrument A', color='steelblue', linewidth=2)\n",
    "    plt.plot(y_plot, label='Instrument B', color='darkorange', linewidth=2)\n",
    "\n",
    "    # Background shading\n",
    "    for i, t in enumerate(t_corr):\n",
    "        corr = corrs[i]\n",
    "        if np.isnan(corr):\n",
    "            continue\n",
    "        elif corr >= high_thresh:\n",
    "            color = 'lightcoral'  # strong positive\n",
    "        elif corr <= low_thresh:\n",
    "            color = 'lightblue'   # strong negative\n",
    "        else:\n",
    "            color = 'lightgray'   # neutral\n",
    "\n",
    "        plt.axvspan(t, t + 1, color=color, alpha=0.3, linewidth=0)\n",
    "    \n",
    "    plt.title(f\"Rolling Spearman Correlation Inst {inst_1} & {inst_2} (window={window})\")\n",
    "    plt.xlabel(\"Time (Days)\")\n",
    "    plt.ylabel(\"Normalized Price\" if normalize else \"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "inst_2 = 17\n",
    "window = 30\n",
    "for i in range(50):\n",
    "    inst_1 = i\n",
    "\n",
    "    spearman_corrs = rolling_spearman(data, inst_1, inst_2, window=window)\n",
    "    plot_colored_rolling_spearman(data, inst_1, inst_2, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_instrument_returns(prc_matrix, corr_matrix, target_idx, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Predict the log returns of the target instrument as a weighted sum of\n",
    "    other instruments' log returns, weighted by their correlation with the target.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): shape (num_instruments, num_days), closing prices\n",
    "        corr_matrix (ndarray): shape (num_instruments, num_instruments), correlation matrix of returns\n",
    "        target_idx (int): index of the instrument to predict\n",
    "\n",
    "    Returns:\n",
    "        predicted_returns (ndarray): predicted log returns for the target instrument, shape (num_days-1,)\n",
    "    \"\"\"\n",
    "    n = prc_matrix.shape[0]\n",
    "    assert corr_matrix.shape == (n, n), \"Correlation matrix shape mismatch\"\n",
    "\n",
    "    # Calculate log returns\n",
    "    log_prices = np.log(prc_matrix)\n",
    "    log_returns = np.diff(log_prices, axis=1)  # shape (n, num_days-1)\n",
    "\n",
    "    # Get weights from correlation matrix, zero self-correlation\n",
    "    weights = corr_matrix[target_idx].copy()\n",
    "    weights[target_idx] = 0\n",
    "\n",
    "    # zero out weak correlation\n",
    "    weights[np.abs(weights) < threshold] = 0\n",
    "\n",
    "    # Normalize weights by sum of absolute values to keep scale\n",
    "    sum_weights = np.sum(np.abs(weights))\n",
    "    if sum_weights != 0:\n",
    "        weights /= sum_weights\n",
    "\n",
    "    # Weighted sum of other instruments' returns to predict target returns\n",
    "    predicted_returns = weights @ log_returns  # shape (num_days-1,)\n",
    "\n",
    "    return predicted_returns\n",
    "\n",
    "\n",
    "def reconstruct_price_from_returns(start_price, returns):\n",
    "    \"\"\"\n",
    "    Reconstruct price series from starting price and log returns.\n",
    "\n",
    "    Parameters:\n",
    "        start_price (float): initial price before returns start\n",
    "        returns (ndarray): log returns array shape (num_days-1,)\n",
    "\n",
    "    Returns:\n",
    "        prices (ndarray): reconstructed price series shape (num_days,)\n",
    "    \"\"\"\n",
    "    cum_log_return = np.cumsum(returns)\n",
    "    prices = start_price * np.exp(cum_log_return)\n",
    "    prices = np.insert(prices, 0, start_price)  # include starting price\n",
    "    return prices\n",
    "\n",
    "\n",
    "def evaluate_predicted_returns(prc_matrix, predicted_returns, target_idx):\n",
    "    \"\"\"\n",
    "    Compare predicted returns with actual returns and compute metrics.\n",
    "\n",
    "    Parameters:\n",
    "        prc_matrix (ndarray): (num_instruments, num_days) price matrix\n",
    "        predicted_returns (ndarray): predicted log returns for target instrument, shape (num_days-1,)\n",
    "        target_idx (int): index of the instrument being predicted\n",
    "\n",
    "    Returns:\n",
    "        metrics (dict): mse, mae, r2 scores\n",
    "    \"\"\"\n",
    "    # Calculate actual log returns\n",
    "    log_prices = np.log(prc_matrix)\n",
    "    actual_returns = np.diff(log_prices[target_idx])\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(actual_returns, predicted_returns)\n",
    "    mae = mean_absolute_error(actual_returns, predicted_returns)\n",
    "    r2 = r2_score(actual_returns, predicted_returns)\n",
    "\n",
    "    print(f\"Evaluation for Instrument {target_idx}:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "    # Plot actual vs predicted returns\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(actual_returns, label='Actual Returns', alpha=0.7)\n",
    "    plt.plot(predicted_returns, label='Predicted Returns', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted Returns for Instrument {target_idx}')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Log Return')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "data = prcCheck\n",
    "for target_idx in [13, 44, 14, 16, 33]:\n",
    "    predicted_ret = predict_instrument_returns(data, corr_matrix, target_idx, 0.05)\n",
    "\n",
    "    # Reconstruct predicted prices starting from actual first price\n",
    "    predicted_prices = reconstruct_price_from_returns(data[target_idx, 0], predicted_ret)\n",
    "\n",
    "    plt.plot(data[target_idx], label=\"Actual Prices\")\n",
    "    plt.plot(predicted_prices, label=\"Predicted Prices from Correlation\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Actual vs Predicted Prices for Instrument {target_idx}\")\n",
    "    plt.show()\n",
    "    metrics = evaluate_predicted_returns(data, predicted_ret, target_idx)\n",
    "    r2.append(metrics['r2'])\n",
    "print(r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running granger causality tests!\n",
    "def run_granger_causality_tests_by_index(returns_df, target_idx, max_lag=5, alpha=0.05, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs Granger causality tests from every instrument to a target instrument by index.\n",
    "\n",
    "    Parameters:\n",
    "        returns_df (pd.DataFrame): shape (days, instruments)\n",
    "        target_idx (int): Index of the target instrument (column in DataFrame)\n",
    "        max_lag (int): Max lag to test\n",
    "        alpha (float): Significance level\n",
    "        verbose (bool): If True, print test output\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: summary of p-values and significance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for predictor_idx in returns_df.columns:\n",
    "        if predictor_idx == target_idx:\n",
    "            continue\n",
    "\n",
    "        # Format: [target, predictor]\n",
    "        data = returns_df[[target_idx, predictor_idx]].dropna().to_numpy()\n",
    "\n",
    "        try:\n",
    "            test_result = grangercausalitytests(data, maxlag=max_lag)\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {predictor_idx} → {target_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        min_pval = min(test_result[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1))\n",
    "\n",
    "        results.append({\n",
    "            'Predictor': predictor_idx,\n",
    "            'Target': target_idx,\n",
    "            'Min_p_value': min_pval,\n",
    "            'Significant': min_pval < alpha\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by='Min_p_value')\n",
    "\n",
    "def plot_granger_results(results_df, alpha=0.05, target_idx=None):\n",
    "    \"\"\"\n",
    "    Plots Granger causality results as a bar chart of minimum p-values.\n",
    "\n",
    "    Parameters:\n",
    "        results_df (pd.DataFrame): Output of run_granger_causality_tests_by_index\n",
    "                                   Must contain columns ['Predictor', 'Min_p_value', 'Significant']\n",
    "        alpha (float): Significance level used in test\n",
    "        target_idx (int or str, optional): Target instrument index or name for title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bars = plt.bar(\n",
    "        results_df['Predictor'].astype(str),\n",
    "        results_df['Min_p_value'],\n",
    "        color=['red' if sig else 'gray' for sig in results_df['Significant']]\n",
    "    )\n",
    "\n",
    "    plt.axhline(y=alpha, color='blue', linestyle='--', label=f'Alpha = {alpha}')\n",
    "    plt.xlabel('Predictor Instrument')\n",
    "    plt.ylabel('Minimum p-value')\n",
    "    plt.title(f'Granger Causality to Target {target_idx}' if target_idx is not None else 'Granger Causality Results')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def top_5_significant_granger(returns_df, target_idx, max_lag=5, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Runs Granger causality tests from all instruments to target_idx,\n",
    "    then extracts top 5 most significant causal relationships with lag info.\n",
    "\n",
    "    Parameters:\n",
    "        returns_df (pd.DataFrame): shape (days, instruments)\n",
    "        target_idx (int): target instrument index\n",
    "        max_lag (int): max lag to test\n",
    "        alpha (float): significance threshold\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with columns:\n",
    "          - Predictor (int)\n",
    "          - Best_Lag (int)\n",
    "          - Min_p_value (float)\n",
    "          - Significant (bool)\n",
    "          - Description (str)\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for predictor_idx in returns_df.columns:\n",
    "        if predictor_idx == target_idx:\n",
    "            continue\n",
    "\n",
    "        data = returns_df[[target_idx, predictor_idx]].dropna().to_numpy()\n",
    "\n",
    "        try:\n",
    "            test_result = grangercausalitytests(data, maxlag=max_lag)\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {predictor_idx} → {target_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract lag with smallest p-value\n",
    "        lag_pvals = []\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            pval = test_result[lag][0]['ssr_ftest'][1]\n",
    "            lag_pvals.append((lag, pval))\n",
    "        best_lag, min_pval = min(lag_pvals, key=lambda x: x[1])\n",
    "        significant = min_pval < alpha\n",
    "\n",
    "        description = (\n",
    "            f\"Predictor {predictor_idx} Granger-causes Target {target_idx} \"\n",
    "            f\"at lag {best_lag} with p-value={min_pval:.4g} \"\n",
    "            f\"{'(Significant)' if significant else '(Not significant)'}\"\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            'Predictor': predictor_idx,\n",
    "            'Best_Lag': best_lag,\n",
    "            'Min_p_value': min_pval,\n",
    "            'Significant': significant,\n",
    "            'Description': description\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df_sorted = df.sort_values(by='Min_p_value').head(5).reset_index(drop=True)\n",
    "    return df_sorted\n",
    "data = prcTest\n",
    "# Suppose 'returns_df' is a DataFrame with daily returns, columns = tickers\n",
    "log_returns = np.diff(np.log(data), axis=1)\n",
    "# Transpose and convert to DataFrame so columns = instruments, rows = time\n",
    "returns_df = pd.DataFrame(log_returns.T, columns=np.arange(log_returns.shape[0]))\n",
    "alpha = 0.01\n",
    "target_inst = 0\n",
    "granger_results = run_granger_causality_tests_by_index(returns_df, target_idx=target_inst, max_lag=5, alpha=alpha)\n",
    "\n",
    "plot_granger_results(granger_results, alpha=alpha, target_idx=0)\n",
    "top5 = top_5_significant_granger(returns_df, target_idx=target_inst, max_lag=5, alpha=0.05)\n",
    "# print(top5[['Predictor', 'Best_Lag', 'Min_p_value', 'Significant']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(top5[['Predictor', 'Best_Lag', 'Min_p_value', 'Significant']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = prcTest\n",
    "# Suppose 'returns_df' is a DataFrame with daily returns, columns = tickers\n",
    "inst_target = 0\n",
    "inst_prc = data[inst_target, :]\n",
    "log_returns = np.diff(np.log(inst_prc))\n",
    "plot_acf(log_returns, lags=30)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "log_returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_similarity_matrix(series, window=30):\n",
    "    \"\"\"\n",
    "    Computes correlation between all sliding windows of the time series.\n",
    "\n",
    "    Returns:\n",
    "        matrix (2D array): (n_windows x n_windows) correlation matrix\n",
    "    \"\"\"\n",
    "    series = np.array(series)\n",
    "    n = len(series) - window + 1\n",
    "    matrix = np.empty((n, n))\n",
    "\n",
    "    # Extract all sliding windows\n",
    "    segments = np.array([series[i:i+window] for i in range(n)])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            matrix[i, j] = np.corrcoef(segments[i], segments[j])[0, 1]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Compute and visualize\n",
    "sim_matrix = self_similarity_matrix(log_returns, window=30)\n",
    "\n",
    "sns.heatmap(sim_matrix, cmap='coolwarm', center=0)\n",
    "plt.title(\"Self-Similarity Matrix\")\n",
    "plt.xlabel(\"Time Window Index\")\n",
    "plt.ylabel(\"Time Window Index\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
