{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regime_inference.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _ols_slope(y: np.ndarray) -> float:\n",
    "    t = np.arange(len(y))\n",
    "    X = np.vstack([t, np.ones_like(t)]).T\n",
    "    m, _ = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    return m\n",
    "\n",
    "\n",
    "def _slope_vol_reg(close: np.ndarray,\n",
    "                   idx:   int,\n",
    "                   slope_win: int = 30,\n",
    "                   vol_win:   int = 100\n",
    "                  ) -> float | int:\n",
    "    logp = np.log(close)\n",
    "\n",
    "    # 1) slope\n",
    "    slope_series = (\n",
    "        pd.Series(logp)\n",
    "          .rolling(slope_win, min_periods=slope_win)\n",
    "          .apply(_ols_slope, raw=True)\n",
    "    )\n",
    "    rtn        = pd.Series(logp).diff()\n",
    "    vol_series = rtn.rolling(vol_win, min_periods=1).std()\n",
    "\n",
    "    slope = slope_series.iloc[idx]\n",
    "    vol   = vol_series.iloc[idx]\n",
    "    if np.isnan(slope) or np.isnan(vol):\n",
    "        return np.nan\n",
    "\n",
    "    # 3) causal median of vol_series up to idx\n",
    "    median_vol = vol_series.iloc[: idx + 1].median()\n",
    "\n",
    "    return 2 if (slope > 0 and vol < median_vol) else 0\n",
    "\n",
    "\n",
    "\n",
    "def compute_regime_features_window(prices_window: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    n_inst, win_len = prices_window.shape\n",
    "    idx = win_len - 1                   \n",
    "\n",
    "    out = np.full((n_inst, 9), np.nan)\n",
    "    sqrt_weights = np.arange(1, 46, dtype=float) ** 0.5\n",
    "    sqrt_weights /= sqrt_weights.sum()\n",
    "\n",
    "    for i in range(n_inst):\n",
    "        close = prices_window[i]\n",
    "        logp  = np.log(close)\n",
    "\n",
    "        # MA regime\n",
    "        ma_s = pd.Series(logp).rolling(5).mean().iloc[idx]\n",
    "        ma_l = pd.Series(logp).rolling(70).mean().iloc[idx]\n",
    "        ma_reg = 0 if ma_l > ma_s else 2\n",
    "\n",
    "        # EMA regime\n",
    "        ema_s = pd.Series(logp).ewm(span=5,  adjust=False).mean().iloc[idx]\n",
    "        ema_l = pd.Series(logp).ewm(span=50, adjust=False).mean().iloc[idx]\n",
    "        ema_reg = 2 if ema_s > ema_l else 0\n",
    "\n",
    "        # Slope/Vol regime\n",
    "        sv_reg = _slope_vol_reg(close, idx)\n",
    "\n",
    "        # MACD regime\n",
    "        macd_line = (\n",
    "            pd.Series(logp).ewm(50, adjust=False).mean()\n",
    "            - pd.Series(logp).ewm(90, adjust=False).mean()\n",
    "        )\n",
    "        signal_line = macd_line.ewm(span=40, adjust=False).mean()\n",
    "        macd_reg = 2 if macd_line.iloc[idx] > signal_line.iloc[idx] else 0\n",
    "\n",
    "        # Kalman trend regime\n",
    "        proc_var, meas_var = 0.01, 10.0\n",
    "        x_est = np.zeros(win_len)\n",
    "        P     = np.zeros(win_len)\n",
    "        x_est[0], P[0] = logp[0], 1.0\n",
    "        for t in range(1, win_len):\n",
    "            x_pred = x_est[t - 1]\n",
    "            P_pred = P[t - 1] + proc_var\n",
    "            K      = P_pred / (P_pred + meas_var)\n",
    "            x_est[t] = x_pred + K * (logp[t] - x_pred)\n",
    "            P[t]     = (1 - K) * P_pred\n",
    "        kalman_reg = 2 if logp[idx] > x_est[idx] else 0\n",
    "\n",
    "        # Fibonacci regime\n",
    "        if idx >= 50:\n",
    "            win50 = close[idx - 49 : idx + 1]\n",
    "            hi, lo = win50.max(), win50.min()\n",
    "            rng = hi - lo\n",
    "            upper, lower = lo + 0.786 * rng, lo + 0.618 * rng\n",
    "            fib_reg = 2 if close[idx] > upper else 0 if close[idx] < lower else 1\n",
    "        else:\n",
    "            fib_reg = np.nan\n",
    "\n",
    "        # PSAR regime\n",
    "        psar = np.empty(win_len)\n",
    "        trend_up, af, max_af = True, 0.01, 0.10\n",
    "        ep = close[0]\n",
    "        psar[0] = close[0]\n",
    "        for t in range(1, win_len):\n",
    "            psar[t] = psar[t - 1] + af * (ep - psar[t - 1])\n",
    "            if trend_up:\n",
    "                if close[t] < psar[t]:\n",
    "                    trend_up, psar[t], ep, af = False, ep, close[t], 0.01\n",
    "                elif close[t] > ep:\n",
    "                    ep, af = close[t], min(af + 0.01, max_af)\n",
    "            else:\n",
    "                if close[t] > psar[t]:\n",
    "                    trend_up, psar[t], ep, af = True, ep, close[t], 0.01\n",
    "                elif close[t] < ep:\n",
    "                    ep, af = close[t], min(af + 0.01, max_af)\n",
    "        psar_reg = 2 if close[idx] > psar[idx] else 0\n",
    "\n",
    "        # Z-score regime\n",
    "        ma90 = pd.Series(close).rolling(90).mean().iloc[idx]\n",
    "        sd90 = pd.Series(close).rolling(90).std().iloc[idx]\n",
    "        if np.isnan(ma90) or np.isnan(sd90):\n",
    "            zscore_reg = np.nan\n",
    "        else:\n",
    "            z = (close[idx] - ma90) / sd90\n",
    "            zscore_reg = 2 if z > 0.5 else 0 if z < -0.5 else 1\n",
    "\n",
    "        # Weighted-return regime\n",
    "        if idx >= 45:\n",
    "            r = pd.Series(close).pct_change().iloc[idx - 44 : idx + 1].values\n",
    "            wr = np.dot(r, sqrt_weights)\n",
    "            wret_reg = 2 if wr > 0 else 0 if wr < 0 else 1\n",
    "        else:\n",
    "            wret_reg = np.nan\n",
    "\n",
    "        out[i] = [\n",
    "            ma_reg, ema_reg, sv_reg, macd_reg, kalman_reg,\n",
    "            fib_reg, psar_reg, zscore_reg, wret_reg,\n",
    "        ]\n",
    "\n",
    "    return out\n",
    "\n",
    "def _extract_window(price_file: str,\n",
    "                    timestep: int,\n",
    "                    win_len: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Slice the latest `win_len` bars (inclusive) ending at `timestep` from the\n",
    "    price file and transpose to (n_inst, win_len).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(price_file, sep=r\"\\s+\", header=None)\n",
    "    n_rows, n_inst = df.shape\n",
    "\n",
    "    if not (0 <= timestep < n_rows):\n",
    "        raise ValueError(f\"timestep {timestep} out of range (0 … {n_rows-1})\")\n",
    "    if timestep < win_len - 1:\n",
    "        raise ValueError(\"Not enough history to build a 100-bar window.\")\n",
    "\n",
    "    slice_df = df.iloc[timestep - win_len + 1 : timestep + 1, :]\n",
    "    return slice_df.to_numpy().T            # (n_inst, win_len)\n",
    "\n",
    "\n",
    "def infer_from_file(price_file: str,\n",
    "                    timestep: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    High-level convenience wrapper:\n",
    "    1. read prices.txt\n",
    "    2. build the (50,100) window ending at `timestep`\n",
    "    3. run the regime-feature pipeline\n",
    "    \"\"\"\n",
    "    window = _extract_window(price_file, timestep, win_len=100)\n",
    "    #print(len(window[0]))\n",
    "    return compute_regime_features_window(window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from precision_labeller import plot_all_regimes_long\n",
    "\n",
    "def build_feature_label_csv(price_file: str,\n",
    "                            N: int = 740,\n",
    "                            output_csv: str = \"features_labels.csv\"):\n",
    "    \"\"\"\n",
    "    Runs through timesteps 0..N-1 of prices.txt, builds the 9-regime features\n",
    "    for each instrument at each t, pulls in the true_autolabel, and saves\n",
    "    a long-form CSV indexed by instrument->time.\n",
    "    \"\"\"\n",
    "    # 1) load prices once\n",
    "    df_price = pd.read_csv(price_file, sep=r\"\\s+\", header=None)\n",
    "    n_rows, n_inst = df_price.shape\n",
    "    assert N <= n_rows, f\"N={N} exceeds available rows={n_rows}\"\n",
    "\n",
    "    # 2) precompute true regimes for each instrument\n",
    "    #    this returns an array length N for each inst\n",
    "    true_regs = {\n",
    "        inst: plot_all_regimes_long(end_point=N + 10, plot_graph=False, inst=inst)\n",
    "        for inst in range(n_inst)\n",
    "    }\n",
    "\n",
    "    # 3) iterate timesteps and call infer_from_file\n",
    "    records = []\n",
    "    for t in range(N):\n",
    "        try:\n",
    "            # infer_from_file expects timestep index in [0..]\n",
    "            feats_t = infer_from_file(price_file, timestep=t)\n",
    "            # feats_t is shape (n_inst, 9)\n",
    "        except ValueError:\n",
    "            # not enough history (t < 99), fill with NaNs\n",
    "            feats_t = np.full((n_inst, 9), np.nan)\n",
    "\n",
    "        for inst in range(n_inst):\n",
    "            row = {\n",
    "                \"inst\": inst,\n",
    "                \"time\": t,\n",
    "                \"ma\":          feats_t[inst, 0],\n",
    "                \"ema\":         feats_t[inst, 1],\n",
    "                \"slope_vol\":   feats_t[inst, 2],\n",
    "                \"macd\":        feats_t[inst, 3],\n",
    "                \"kalman\":      feats_t[inst, 4],\n",
    "                \"fib\":         feats_t[inst, 5],\n",
    "                \"psar\":        feats_t[inst, 6],\n",
    "                \"zscore\":      feats_t[inst, 7],\n",
    "                \"wret\":        feats_t[inst, 8],\n",
    "                \"true_regime\": true_regs[inst][t]\n",
    "            }\n",
    "            records.append(row)\n",
    "\n",
    "    # 5) build DataFrame & save\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df = df.sort_values([\"inst\", \"time\"]).reset_index(drop=True)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Wrote {len(df)} rows to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_feature_label_csv(\"prices.txt\", N=740, output_csv=\"features_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_t451 = infer_from_file(\"prices.txt\", timestep=99)\n",
    "# features_t451.shape  ->  (50, 9)\n",
    "print(features_t451)  # Example output for the first instrument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ── 2) Hyperparameters\n",
    "SEQ_LEN     = 20\n",
    "BATCH_SIZE  = 64\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "LR          = 1e-3\n",
    "NUM_EPOCHS  = 20\n",
    "TEST_SIZE   = 0.3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ── 3) Dataset & Model\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class RegimeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)      # (batch, seq_len, hidden)\n",
    "        out = out[:, -1, :]        # last time-step\n",
    "        return self.fc(out)        # (batch, num_classes)\n",
    "\n",
    "# ── 4) Load & preprocess data\n",
    "df = pd.read_csv(\"features_all_models_FINAL.csv\")  # features with true_regime\n",
    "# drop first-100 warmups per instrument\n",
    "df = (\n",
    "    df\n",
    "    .groupby(\"inst\", group_keys=False)\n",
    "    .apply(lambda g: g.iloc[100:])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "price_df = pd.read_csv(\"prices.txt\", sep=r\"\\s+\", header=None)\n",
    "\n",
    "# build sequences for LSTM\n",
    "X_raw = df.drop([\"inst\",\"time\",\"true_regime\"], axis=1).values\n",
    "y_raw = df[\"true_regime\"].values\n",
    "\n",
    "X_seqs, y_seqs, inst_map = [], [], []\n",
    "for inst in df[\"inst\"].unique():\n",
    "    mask = df[\"inst\"] == inst\n",
    "    Xi, yi = X_raw[mask], y_raw[mask]\n",
    "    for t in range(SEQ_LEN, len(Xi)):\n",
    "        X_seqs.append(Xi[t-SEQ_LEN : t])  # past SEQ_LEN steps\n",
    "        y_seqs.append(yi[t])              # regime label at t\n",
    "        inst_map.append(inst)\n",
    "\n",
    "X_seqs   = np.stack(X_seqs)  # (N, SEQ_LEN, D)\n",
    "y_seqs   = np.array(y_seqs)  # (N,)\n",
    "inst_map = np.array(inst_map)\n",
    "\n",
    "# ── 5) Train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_seqs, y_seqs,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_seqs,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(RegimeDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(RegimeDataset(X_val,   y_val),   batch_size=BATCH_SIZE)\n",
    "\n",
    "# ── 6) Build & train the LSTM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RegimeLSTM(\n",
    "    input_size  = X_seqs.shape[2],\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    num_layers  = NUM_LAYERS,\n",
    "    num_classes = int(y_raw.max())+1,\n",
    "    dropout     = DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(Xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total   += yb.size(0)\n",
    "    print(f\"Epoch {epoch:02d}  Loss: {total_loss/total:.4f}  Acc: {correct/total:.4f}\")\n",
    "\n",
    "# ── 7) Inference & separate plotting per instrument\n",
    "def get_segments(reg):\n",
    "    changes = np.flatnonzero(reg[1:] != reg[:-1])\n",
    "    starts  = np.concatenate(([0], changes+1))\n",
    "    ends    = np.concatenate((changes, [len(reg)-1]))\n",
    "    return list(zip(starts, ends, reg[starts]))\n",
    "\n",
    "# Flip coloring: red for bullish, grey for neutral, green for bearish\n",
    "true_cmap = ListedColormap([\"#ffcccc\", \"#f0f0f0\", \"#ccffcc\"])\n",
    "pred_cmap = ListedColormap([\"#ff6666\", \"#b0b0b0\", \"#66cc66\"])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inst in sorted(np.unique(inst_map)):\n",
    "        # assemble data\n",
    "        mask      = inst_map == inst\n",
    "        Xi        = torch.from_numpy(X_seqs[mask]).float().to(device)\n",
    "        true_i    = y_seqs[mask]\n",
    "        pred_i    = model(Xi).argmax(dim=1).cpu().numpy()\n",
    "        # reconstruct full-length arrays\n",
    "        Ni        = df[df[\"inst\"] == inst].shape[0]\n",
    "        price_i   = price_df.iloc[100:100+Ni, inst].values\n",
    "        true_full = np.concatenate([np.full(SEQ_LEN, np.nan), true_i])\n",
    "        pred_full = np.concatenate([np.full(SEQ_LEN, np.nan), pred_i])\n",
    "        # plot true regimes\n",
    "        fig, ax = plt.subplots(figsize=(12,4))\n",
    "        for s,e,lbl in get_segments(true_full[~np.isnan(true_full)].astype(int)):\n",
    "            ax.axvspan(s+SEQ_LEN, e+SEQ_LEN, color=true_cmap(lbl), alpha=0.3)\n",
    "        ax.plot(price_i, color=\"k\", label=\"Price\")\n",
    "        ax.set_title(f\"Inst {inst} — TRUE regimes vs Price\")\n",
    "        ax.set_xlabel(\"Time Step\")\n",
    "        ax.set_ylabel(\"Price\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        # plot predicted regimes\n",
    "        fig, ax = plt.subplots(figsize=(12,4))\n",
    "        for s,e,lbl in get_segments(pred_full[~np.isnan(pred_full)].astype(int)):\n",
    "            ax.axvspan(s+SEQ_LEN, e+SEQ_LEN, color=pred_cmap(lbl), alpha=0.3)\n",
    "        ax.plot(price_i, color=\"k\", label=\"Price\")\n",
    "        ax.set_title(f\"Inst {inst} — PREDICTED regimes vs Price\")\n",
    "        ax.set_xlabel(\"Time Step\")\n",
    "        ax.set_ylabel(\"Price\")\n",
    "        ax.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────\n",
    "FEATURE_CSV    = \"features_labels.csv\"  # must include inst,time,ma…wret,true_regime\n",
    "PRICE_CSV      = \"prices.txt\"\n",
    "PAST_LEN       = 20     # how many past bars\n",
    "FUTURE_LEN     = 20     # how many future bars for look-ahead\n",
    "TRAIN_START    = 100    # drop first 100 warm-ups\n",
    "TRAIN_END      = 600    # train up to t=599\n",
    "TEST_START     = 600    # test from t=600\n",
    "TEST_END       = 1000    # …to t=749\n",
    "BATCH_SIZE     = 64\n",
    "HIDDEN_SIZE    = 64\n",
    "NUM_LAYERS     = 2\n",
    "DROPOUT        = 0.2\n",
    "LR             = 1e-3\n",
    "NUM_EPOCHS     = 10\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEAT_COLS      = [\"ma\",\"ema\",\"slope_vol\",\"macd\",\"kalman\",\"fib\",\"psar\",\"zscore\",\"wret\"]\n",
    "\n",
    "# ─── DATASETS ───────────────────────────────────────────────────────────────\n",
    "class LookaheadDataset(Dataset):\n",
    "    def __init__(self, df, start_t, end_t, past_len, future_len, feat_cols):\n",
    "        self.X, self.Y = [], []\n",
    "        self.meta = []  # store (inst, time)\n",
    "        for inst, g in df.groupby(\"inst\"):\n",
    "            g = g.sort_values(\"time\").set_index(\"time\")\n",
    "            feats = g[feat_cols].values\n",
    "            labs  = g[\"true_regime\"].values\n",
    "            times = g.index.values\n",
    "            for i, t in enumerate(times):\n",
    "                if t < start_t or t >= end_t: continue\n",
    "                if i < past_len or i+future_len >= len(times): continue\n",
    "                past   = feats[i-past_len  : i    ]\n",
    "                future = feats[i+1          : i+1+future_len]\n",
    "                window = np.vstack([past, future])\n",
    "                self.X.append(window)\n",
    "                self.Y.append(labs[i])\n",
    "                self.meta.append((inst, t))\n",
    "        self.X = torch.from_numpy(np.stack(self.X)).float()\n",
    "        self.Y = torch.from_numpy(np.array(self.Y)).long()\n",
    "\n",
    "    def __len__(self): return len(self.Y)\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
    "\n",
    "class PastSelfLabelDataset(Dataset):\n",
    "    def __init__(self, past_windows, labels):\n",
    "        self.X = torch.from_numpy(np.stack(past_windows)).float()\n",
    "        self.Y = torch.from_numpy(np.array(labels)).long()\n",
    "\n",
    "    def __len__(self): return len(self.Y)\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
    "\n",
    "# ─── MODEL ─────────────────────────────────────────────────────────────────\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden, layers, n_labels, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = feat_dim,\n",
    "            hidden_size  = hidden,\n",
    "            num_layers   = layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = dropout,\n",
    "            bidirectional= True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden*2, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out    = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# ─── 1) LOAD & INITIAL TRAIN/INFER ──────────────────────────────────────────\n",
    "df = pd.read_csv(FEATURE_CSV)\n",
    "df = df[df.time >= TRAIN_START].reset_index(drop=True)\n",
    "n_labels = int(df.true_regime.max())+1\n",
    "feat_dim = len(FEAT_COLS)\n",
    "\n",
    "# initial train\n",
    "train_ds = LookaheadDataset(df, TRAIN_START, TRAIN_END, PAST_LEN, FUTURE_LEN, FEAT_COLS)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model1 = BiLSTM(feat_dim, HIDDEN_SIZE, NUM_LAYERS, n_labels, DROPOUT).to(DEVICE)\n",
    "opt1  = torch.optim.Adam(model1.parameters(), lr=LR)\n",
    "crit  = nn.CrossEntropyLoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model1.train(); total_loss=0; total_n=0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model1(Xb); loss = crit(logits, yb)\n",
    "        opt1.zero_grad(); loss.backward(); opt1.step()\n",
    "        total_loss += loss.item()*Xb.size(0); total_n += Xb.size(0)\n",
    "    print(f\"[Init Epoch {epoch}] Loss={total_loss/total_n:.4f}\")\n",
    "\n",
    "# initial inference and collect self-labels\n",
    "test_ds = LookaheadDataset(df, TEST_START, TEST_END, PAST_LEN, FUTURE_LEN, FEAT_COLS)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "preds1, metas, past_windows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for idx, (Xb, _) in enumerate(test_loader):\n",
    "        Xb = Xb.to(DEVICE)\n",
    "        logits = model1(Xb); batch_preds = logits.argmax(dim=1).cpu().tolist()\n",
    "        for b, p in enumerate(batch_preds):\n",
    "            preds1.append(p)\n",
    "            metas.append(test_ds.meta[idx*BATCH_SIZE + b])\n",
    "            # extract past window and duplicate\n",
    "            past = Xb[b,:,:PAST_LEN].cpu().numpy()\n",
    "            past_dup = np.vstack([past, past])\n",
    "            past_windows.append(past_dup)\n",
    "\n",
    "# ─── 2) SELF-LABEL RETRAIN ─────────────────────────────────────────────────\n",
    "self_ds = PastSelfLabelDataset(past_windows, preds1)\n",
    "self_loader = DataLoader(self_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model2 = BiLSTM(feat_dim, HIDDEN_SIZE, NUM_LAYERS, n_labels, DROPOUT).to(DEVICE)\n",
    "opt2  = torch.optim.Adam(model2.parameters(), lr=LR)\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model2.train(); total_loss=0; total_n=0\n",
    "    for Xb, yb in self_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model2(Xb); loss = crit(logits, yb)\n",
    "        opt2.zero_grad(); loss.backward(); opt2.step()\n",
    "        total_loss += loss.item()*Xb.size(0); total_n += Xb.size(0)\n",
    "    print(f\"[Self Epoch {epoch}] Loss={total_loss/total_n:.4f}\")\n",
    "\n",
    "# ─── 3) PAST-ONLY INFERENCE & PLOTTING ───────────────────────────────────────\n",
    "price_df = pd.read_csv(PRICE_CSV, sep=r\"\\s+\", header=None)\n",
    "true_cmap = ListedColormap([\"#ffcccc\",\"#f0f0f0\",\"#ccffcc\"])\n",
    "pred_cmap = ListedColormap([\"#ff6666\",\"#b0b0b0\",\"#66cc66\"])\n",
    "preds2 = []\n",
    "with torch.no_grad():\n",
    "    for pw in past_windows:\n",
    "        xb = torch.from_numpy(pw).unsqueeze(0).float().to(DEVICE)\n",
    "        logits = model2(xb); preds2.append(logits.argmax(dim=1).item())\n",
    "\n",
    "from collections import defaultdict\n",
    "grouped_preds2 = defaultdict(list); grouped_true = defaultdict(list); grouped_times = defaultdict(list)\n",
    "for (inst,t), p2, p1 in zip(metas, preds2, preds1):\n",
    "    # use original true for background\n",
    "    true_lbl = int(df[(df.inst==inst)&(df.time==t)].true_regime)\n",
    "    grouped_preds2[inst].append(p2)\n",
    "    grouped_true[inst].append(true_lbl)\n",
    "    grouped_times[inst].append(t)\n",
    "\n",
    "# plot each instrument\n",
    "for inst in sorted(grouped_preds2):\n",
    "    times = np.array(grouped_times[inst])\n",
    "    true_r = np.array(grouped_true[inst])\n",
    "    pred_r = np.array(grouped_preds2[inst])\n",
    "    price_series = price_df.iloc[:,inst]\n",
    "    price_slice  = price_series.iloc[times].values\n",
    "    x = np.arange(len(times))\n",
    "    fig,(axT,axP) = plt.subplots(2,1, sharex=True, figsize=(12,5))\n",
    "    # true\n",
    "    ch = np.flatnonzero(true_r[:-1]!=true_r[1:])\n",
    "    st = np.concatenate(([0], ch+1)); en = np.concatenate((ch, [len(true_r)-1]))\n",
    "    for s,e,l in zip(st, en, true_r[st]): axT.axvspan(s,e, color=true_cmap(l), alpha=0.3, lw=0)\n",
    "    axT.plot(x, price_slice, \"k-\", lw=1); axT.set_title(f\"Inst {inst:02d} — TRUE\")\n",
    "    # pred\n",
    "    ch2 = np.flatnonzero(pred_r[:-1]!=pred_r[1:])\n",
    "    st2 = np.concatenate(([0], ch2+1)); en2 = np.concatenate((ch2, [len(pred_r)-1]))\n",
    "    for s,e,l in zip(st2, en2, pred_r[st2]): axP.axvspan(s,e, color=pred_cmap(l), alpha=0.3, lw=0)\n",
    "    axP.plot(x, price_slice, \"k-\", lw=1); axP.set_title(f\"Inst {inst:02d} — SELF-LABEL PREDICTIONS\")\n",
    "    axP.set_xlabel(\"Test index\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────\n",
    "FEATURE_CSV    = \"features_labels.csv\"  \n",
    "PRICE_CSV      = \"prices.txt\"\n",
    "PAST_LEN       = 20\n",
    "FUTURE_LEN     = 20\n",
    "TRAIN_START    = 100\n",
    "TRAIN_END      = 600\n",
    "TEST_START     = 600\n",
    "TEST_END       = 1000\n",
    "BATCH_SIZE     = 64\n",
    "HIDDEN_SIZE    = 64\n",
    "NUM_LAYERS     = 2\n",
    "DROPOUT        = 0.2\n",
    "LR             = 1e-3\n",
    "NUM_EPOCHS     = 10\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CONF_THRESH    = 0.90    # only accept pseudo‐labels with ≥ 90% confidence\n",
    "WEIGHT_DECAY   = 1e-4    # light L2 regularization\n",
    "FEAT_COLS      = [\"ma\",\"ema\",\"slope_vol\",\"macd\",\"kalman\",\"fib\",\"psar\",\"zscore\",\"wret\"]\n",
    "\n",
    "# ─── DATASETS ───────────────────────────────────────────────────────────────\n",
    "class LookaheadDataset(Dataset):\n",
    "    def __init__(self, df, start_t, end_t, past_len, future_len, feat_cols):\n",
    "        self.X, self.Y, self.meta = [], [], []\n",
    "        for inst, g in df.groupby(\"inst\"):\n",
    "            g = g.sort_values(\"time\").set_index(\"time\")\n",
    "            feats = g[feat_cols].values\n",
    "            labs  = g[\"true_regime\"].values\n",
    "            times = g.index.values\n",
    "            for i, t in enumerate(times):\n",
    "                if t < start_t or t > end_t: \n",
    "                    continue\n",
    "                if i < past_len or i + future_len >= len(times):\n",
    "                    continue\n",
    "                past   = feats[i-past_len  : i    ]\n",
    "                future = feats[i+1          : i+1+future_len]\n",
    "                window = np.vstack([past, future])\n",
    "                self.X.append(window)\n",
    "                self.Y.append(labs[i])\n",
    "                self.meta.append((inst, t))\n",
    "        self.X = torch.from_numpy(np.stack(self.X)).float()\n",
    "        self.Y = torch.from_numpy(np.array(self.Y)).long()\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.Y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "\n",
    "class PastSelfLabelDataset(Dataset):\n",
    "    def __init__(self, past_windows, labels):\n",
    "        self.X = torch.from_numpy(np.stack(past_windows)).float()\n",
    "        self.Y = torch.from_numpy(np.array(labels)).long()\n",
    "\n",
    "    def __len__(self): return len(self.Y)\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
    "\n",
    "# ─── MODEL ─────────────────────────────────────────────────────────────────\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden, layers, n_labels, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = feat_dim,\n",
    "            hidden_size  = hidden,\n",
    "            num_layers   = layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = dropout,\n",
    "            bidirectional= True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden*2, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out    = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# ─── 1) LOAD & INITIAL TRAIN/INFER ──────────────────────────────────────────\n",
    "df = pd.read_csv(FEATURE_CSV)\n",
    "df = df[df.time >= TRAIN_START].reset_index(drop=True)\n",
    "n_labels = int(df.true_regime.max()) + 1\n",
    "feat_dim = len(FEAT_COLS)\n",
    "\n",
    "# INITIAL TRAIN\n",
    "train_ds     = LookaheadDataset(df, TRAIN_START, TRAIN_END, PAST_LEN, FUTURE_LEN, FEAT_COLS)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model1       = BiLSTM(feat_dim, HIDDEN_SIZE, NUM_LAYERS, n_labels, DROPOUT).to(DEVICE)\n",
    "opt1         = torch.optim.Adam(model1.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "crit         = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model1.train()\n",
    "    total_loss = 0.0\n",
    "    total_n    = 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model1(Xb)\n",
    "        loss   = crit(logits, yb)\n",
    "        opt1.zero_grad()\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        total_n    += Xb.size(0)\n",
    "    print(f\"[Init Epoch {epoch:02d}] Loss = {total_loss/total_n:.4f}\")\n",
    "\n",
    "# INITIAL INFERENCE + CONFIDENCE FILTERING\n",
    "test_ds     = LookaheadDataset(df, TEST_START, TEST_END, PAST_LEN, FUTURE_LEN, FEAT_COLS)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "preds1      = []\n",
    "past_windows= []\n",
    "meta_keep   = []\n",
    "\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (Xb, _) in enumerate(test_loader):\n",
    "        Xb = Xb.to(DEVICE)\n",
    "        logits = model1(Xb)                     # (B, n_labels)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        conf, batch_preds = probs.max(dim=1)    # (B,)\n",
    "        for b, (c, p) in enumerate(zip(conf.cpu().tolist(),\n",
    "                                       batch_preds.cpu().tolist())):\n",
    "            if c < CONF_THRESH:\n",
    "                continue\n",
    "            idx_global = idx * BATCH_SIZE + b\n",
    "            preds1.append(p)\n",
    "            meta_keep.append(test_ds.meta[idx_global])\n",
    "            # only keep past for the student\n",
    "            past = Xb[b,:,:PAST_LEN].cpu().numpy()\n",
    "            past_dup = np.vstack([past, past])\n",
    "            past_windows.append(past_dup)\n",
    "\n",
    "# ─── 2) SELF-LABEL RETRAIN ─────────────────────────────────────────────────\n",
    "self_ds     = PastSelfLabelDataset(past_windows, preds1)\n",
    "self_loader = DataLoader(self_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model2      = BiLSTM(feat_dim, HIDDEN_SIZE, NUM_LAYERS, n_labels, DROPOUT).to(DEVICE)\n",
    "opt2        = torch.optim.Adam(model2.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model2.train()\n",
    "    total_loss = 0.0\n",
    "    total_n    = 0\n",
    "    for Xb, yb in self_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model2(Xb)\n",
    "        loss   = crit(logits, yb)\n",
    "        opt2.zero_grad()\n",
    "        loss.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        total_n    += Xb.size(0)\n",
    "    print(f\"[Self  Epoch {epoch:02d}] Loss = {total_loss/total_n:.4f}\")\n",
    "\n",
    "# ─── 3) PAST‐ONLY INFERENCE & PLOTTING ───────────────────────────────────────\n",
    "price_df   = pd.read_csv(PRICE_CSV, sep=r\"\\s+\", header=None)\n",
    "true_cmap  = ListedColormap([\"#ffcccc\",\"#f0f0f0\",\"#ccffcc\"])\n",
    "pred_cmap  = ListedColormap([\"#ff6666\",\"#b0b0b0\",\"#66cc66\"])\n",
    "\n",
    "# collect model2 predictions\n",
    "preds2 = []\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for pw in past_windows:\n",
    "        xb = torch.from_numpy(pw).unsqueeze(0).float().to(DEVICE)\n",
    "        logits = model2(xb)\n",
    "        preds2.append(logits.argmax(dim=1).item())\n",
    "\n",
    "from collections import defaultdict\n",
    "grouped_preds2 = defaultdict(list)\n",
    "grouped_true   = defaultdict(list)\n",
    "grouped_times  = defaultdict(list)\n",
    "\n",
    "for (inst,t), p2 in zip(meta_keep, preds2):\n",
    "    true_lbl = int(df[(df.inst==inst)&(df.time==t)].true_regime)\n",
    "    grouped_preds2[inst].append(p2)\n",
    "    grouped_true[inst].append(true_lbl)\n",
    "    grouped_times[inst].append(t)\n",
    "\n",
    "def get_segments(r):\n",
    "    ch = np.flatnonzero(r[:-1]!=r[1:])\n",
    "    st = np.concatenate(([0], ch+1))\n",
    "    en = np.concatenate((ch, [len(r)-1]))\n",
    "    return list(zip(st,en,r[st]))\n",
    "\n",
    "green = \"#ccffcc\"; red = \"#ffcccc\"\n",
    "\n",
    "for inst in sorted(grouped_preds2):\n",
    "    times     = np.array(grouped_times[inst])\n",
    "    true_r    = np.array(grouped_true[inst])\n",
    "    pred_r    = np.array(grouped_preds2[inst])\n",
    "    price     = price_df.iloc[times, inst].values\n",
    "    x         = np.arange(len(times))\n",
    "\n",
    "    fig, (axT, axP) = plt.subplots(2,1, sharex=True, figsize=(12,5))\n",
    "    # TRUE\n",
    "    for s,e,l in get_segments(true_r):\n",
    "        color = green if l==2 else red if l==0 else \"lightgrey\"\n",
    "        axT.axvspan(x[s], x[e], color=color, alpha=0.3, lw=0)\n",
    "    axT.plot(x, price, \"k-\", lw=1)\n",
    "    axT.set_title(f\"Inst {inst:02d} — TRUE regimes\")\n",
    "    # PREDICTED\n",
    "    for s,e,l in get_segments(pred_r):\n",
    "        color = green if l==2 else red if l==0 else \"lightgrey\"\n",
    "        axP.axvspan(x[s], x[e], color=color, alpha=0.3, lw=0)\n",
    "    axP.plot(x, price, \"k-\", lw=1)\n",
    "    axP.set_title(f\"Inst {inst:02d} — SELF-LABEL PREDICTIONS (conf≥{CONF_THRESH})\")\n",
    "    axP.set_xlabel(\"Time Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────\n",
    "FEATURE_CSV    = \"features_labels.csv\"  \n",
    "PRICE_CSV      = \"prices.txt\"\n",
    "PAST_LEN       = 20\n",
    "FUTURE_LEN     = 20\n",
    "TRAIN_START    = 100\n",
    "TRAIN_END      = 600\n",
    "TEST_START     = 600\n",
    "TEST_END       = 1000\n",
    "BATCH_SIZE     = 64\n",
    "HIDDEN_SIZE    = 64\n",
    "NUM_LAYERS     = 2\n",
    "DROPOUT        = 0.2\n",
    "LR             = 1e-3\n",
    "NUM_EPOCHS     = 10\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CONF_THRESH    = 0.70    # only accept pseudo‐labels with ≥ 90% confidence\n",
    "WEIGHT_DECAY   = 1e-4    # light L2 regularization\n",
    "FEAT_COLS      = [\"ma\",\"ema\",\"slope_vol\",\"macd\",\"kalman\",\"fib\",\"psar\",\"zscore\",\"wret\"]\n",
    "\n",
    "# ─── DATASETS ───────────────────────────────────────────────────────────────\n",
    "class LookaheadDataset(Dataset):\n",
    "    def __init__(self, df, start_t, end_t, past_len, future_len, feat_cols):\n",
    "        self.X, self.Y, self.meta = [], [], []\n",
    "        for inst, g in df.groupby(\"inst\"):\n",
    "            g = g.sort_values(\"time\").set_index(\"time\")\n",
    "            feats = g[feat_cols].values\n",
    "            labs  = g[\"true_regime\"].values\n",
    "            times = g.index.values\n",
    "            for i, t in enumerate(times):\n",
    "                if t < start_t or t > end_t: \n",
    "                    continue\n",
    "                if i < past_len or i + future_len >= len(times):\n",
    "                    continue\n",
    "                past   = feats[i-past_len  : i    ]\n",
    "                future = feats[i+1          : i+1+future_len]\n",
    "                window = np.vstack([past, future])\n",
    "                self.X.append(window)\n",
    "                self.Y.append(labs[i])\n",
    "                self.meta.append((inst, t))\n",
    "        self.X = torch.from_numpy(np.stack(self.X)).float()\n",
    "        self.Y = torch.from_numpy(np.array(self.Y)).long()\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.Y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "\n",
    "class PastSelfLabelDataset(Dataset):\n",
    "    def __init__(self, past_windows, labels):\n",
    "        self.X = torch.from_numpy(np.stack(past_windows)).float()\n",
    "        self.Y = torch.from_numpy(np.array(labels)).long()\n",
    "\n",
    "    def __len__(self): return len(self.Y)\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
    "\n",
    "# ─── MODEL ─────────────────────────────────────────────────────────────────\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden, layers, n_labels, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = feat_dim,\n",
    "            hidden_size  = hidden,\n",
    "            num_layers   = layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = dropout,\n",
    "            bidirectional= True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden*2, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out    = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# ─── 1) LOAD & INITIAL TRAIN/INFER ──────────────────────────────────────────\n",
    "df = pd.read_csv(FEATURE_CSV)\n",
    "df = df[df.time >= TRAIN_START].reset_index(drop=True)\n",
    "n_labels = int(df.true_regime.max()) + 1\n",
    "feat_dim = len(FEAT_COLS)\n",
    "\n",
    "# INITIAL TRAIN\n",
    "train_ds     = LookaheadDataset(df, TRAIN_START, TRAIN_END, PAST_LEN, FUTURE_LEN, FEAT_COLS)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model1       = BiLSTM(feat_dim, HIDDEN_SIZE, NUM_LAYERS, n_labels, DROPOUT).to(DEVICE)\n",
    "opt1         = torch.optim.Adam(model1.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "crit         = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model1.train()\n",
    "    total_loss = 0.0\n",
    "    total_n    = 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model1(Xb)\n",
    "        loss   = crit(logits, yb)\n",
    "        opt1.zero_grad()\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        total_n    += Xb.size(0)\n",
    "    print(f\"[Init Epoch {epoch:02d}] Loss = {total_loss/total_n:.4f}\")\n",
    "\n",
    "# INITIAL INFERENCE + CONFIDENCE FILTERING\n",
    "test_ds     = LookaheadDataset(df, TEST_START, TEST_END, PAST_LEN, FUTURE_LEN, FEAT_COLS)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "preds1      = []\n",
    "past_windows= []\n",
    "meta_keep   = []\n",
    "\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (Xb, _) in enumerate(test_loader):\n",
    "        Xb = Xb.to(DEVICE)\n",
    "        logits = model1(Xb)                     # (B, n_labels)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        conf, batch_preds = probs.max(dim=1)    # (B,)\n",
    "        for b, (c, p) in enumerate(zip(conf.cpu().tolist(),\n",
    "                                       batch_preds.cpu().tolist())):\n",
    "            if c < CONF_THRESH:\n",
    "                continue\n",
    "            idx_global = idx * BATCH_SIZE + b\n",
    "            preds1.append(p)\n",
    "            meta_keep.append(test_ds.meta[idx_global])\n",
    "            # only keep past for the student\n",
    "            past = Xb[b,:,:PAST_LEN].cpu().numpy()\n",
    "            past_dup = np.vstack([past, past])\n",
    "            past_windows.append(past_dup)\n",
    "\n",
    "# ─── 2) SELF-LABEL RETRAIN ─────────────────────────────────────────────────\n",
    "self_ds     = PastSelfLabelDataset(past_windows, preds1)\n",
    "self_loader = DataLoader(self_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model2      = BiLSTM(feat_dim, HIDDEN_SIZE, NUM_LAYERS, n_labels, DROPOUT).to(DEVICE)\n",
    "opt2        = torch.optim.Adam(model2.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model2.train()\n",
    "    total_loss = 0.0\n",
    "    total_n    = 0\n",
    "    for Xb, yb in self_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model2(Xb)\n",
    "        loss   = crit(logits, yb)\n",
    "        opt2.zero_grad()\n",
    "        loss.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        total_n    += Xb.size(0)\n",
    "    print(f\"[Self  Epoch {epoch:02d}] Loss = {total_loss/total_n:.4f}\")\n",
    "\n",
    "# ─── 3) PAST‐ONLY INFERENCE & PLOTTING ───────────────────────────────────────\n",
    "price_df   = pd.read_csv(PRICE_CSV, sep=r\"\\s+\", header=None)\n",
    "true_cmap  = ListedColormap([\"#ffcccc\",\"#f0f0f0\",\"#ccffcc\"])\n",
    "pred_cmap  = ListedColormap([\"#ff6666\",\"#b0b0b0\",\"#66cc66\"])\n",
    "\n",
    "# collect model2 predictions\n",
    "preds2 = []\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for pw in past_windows:\n",
    "        xb = torch.from_numpy(pw).unsqueeze(0).float().to(DEVICE)\n",
    "        logits = model2(xb)\n",
    "        preds2.append(logits.argmax(dim=1).item())\n",
    "\n",
    "from collections import defaultdict\n",
    "grouped_preds2 = defaultdict(list)\n",
    "grouped_true   = defaultdict(list)\n",
    "grouped_times  = defaultdict(list)\n",
    "\n",
    "for (inst,t), p2 in zip(meta_keep, preds2):\n",
    "    true_lbl = int(df[(df.inst==inst)&(df.time==t)].true_regime)\n",
    "    grouped_preds2[inst].append(p2)\n",
    "    grouped_true[inst].append(true_lbl)\n",
    "    grouped_times[inst].append(t)\n",
    "\n",
    "def get_segments(r):\n",
    "    ch = np.flatnonzero(r[:-1]!=r[1:])\n",
    "    st = np.concatenate(([0], ch+1))\n",
    "    en = np.concatenate((ch, [len(r)-1]))\n",
    "    return list(zip(st,en,r[st]))\n",
    "\n",
    "green = \"#ccffcc\"; red = \"#ffcccc\"\n",
    "\n",
    "for inst in sorted(grouped_preds2):\n",
    "    times     = np.array(grouped_times[inst])\n",
    "    true_r    = np.array(grouped_true[inst])\n",
    "    pred_r    = np.array(grouped_preds2[inst])\n",
    "    price     = price_df.iloc[times, inst].values\n",
    "    x         = np.arange(len(times))\n",
    "\n",
    "    fig, (axT, axP) = plt.subplots(2,1, sharex=True, figsize=(12,5))\n",
    "    # TRUE\n",
    "    for s,e,l in get_segments(true_r):\n",
    "        color = green if l==2 else red if l==0 else \"lightgrey\"\n",
    "        axT.axvspan(x[s], x[e], color=color, alpha=0.3, lw=0)\n",
    "    axT.plot(x, price, \"k-\", lw=1)\n",
    "    axT.set_title(f\"Inst {inst:02d} — TRUE regimes\")\n",
    "    # PREDICTED\n",
    "    for s,e,l in get_segments(pred_r):\n",
    "        color = green if l==2 else red if l==0 else \"lightgrey\"\n",
    "        axP.axvspan(x[s], x[e], color=color, alpha=0.3, lw=0)\n",
    "    axP.plot(x, price, \"k-\", lw=1)\n",
    "    axP.set_title(f\"Inst {inst:02d} — SELF-LABEL PREDICTIONS (conf≥{CONF_THRESH})\")\n",
    "    axP.set_xlabel(\"Time Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
