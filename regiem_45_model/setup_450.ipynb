{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pre_proc_pipeline import pipeline_regiem\n",
    "from pre_proc_labelling_long import plot_all_regimes_long\n",
    "\n",
    "'''\n",
    "Expecting around 72 +/- 4% accuracy\n",
    "'''\n",
    "\n",
    "df = pipeline_regiem()            \n",
    "\n",
    "label_frames = []\n",
    "for inst, inst_df in df.groupby(level=\"inst\", sort=False):\n",
    "\n",
    "    labels = plot_all_regimes_long(len(inst_df), False, inst)\n",
    "\n",
    "    valid_idx = inst_df.index[: len(labels) ]\n",
    "\n",
    "    s = pd.Series(labels, index=valid_idx, name=\"regime\")\n",
    "\n",
    "    label_frames.append(s)\n",
    "\n",
    "regimes = pd.concat(label_frames) \n",
    "\n",
    "df2 = df.copy()\n",
    "df2[\"regime\"] = regimes         \n",
    "df2[\"target\"] = (df2.groupby(level=\"inst\")[\"regime\"].shift(-1))\n",
    "\n",
    "df2 = df2.dropna(subset=[\"target\"])\n",
    "\n",
    "X = df2.drop(columns=[\"regime\",\"target\"])\n",
    "y = df2[\"target\"].astype(int)\n",
    "\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "\n",
    "train_X_parts, train_y_parts = [], []\n",
    "test_X_parts,  test_y_parts  = [], []\n",
    "\n",
    "for inst, X_inst in X.groupby(level='inst', sort=False):\n",
    "    idx = X_inst.index\n",
    "    train_idx, test_idx = idx[:500], idx[500:]\n",
    "    train_X_parts.append(X.loc[train_idx])\n",
    "    train_y_parts.append(y.loc[train_idx])\n",
    "    test_X_parts .append(X.loc[test_idx])\n",
    "    test_y_parts .append(y.loc[test_idx])\n",
    "\n",
    "X_train = pd.concat(train_X_parts)\n",
    "y_train = pd.concat(train_y_parts)\n",
    "X_test  = pd.concat(test_X_parts)\n",
    "y_test  = pd.concat(test_y_parts)\n",
    "\n",
    "#Mapping 2.0 -> 1.0 for clarity\n",
    "y_train = (y_train == 2).astype(int)\n",
    "y_test  = (y_test  == 2).astype(int)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "print(\"Before scaling →\", \"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled  = scaler.transform(X_test)      \n",
    "\n",
    "\n",
    "'''\n",
    "Dataset Context:    \n",
    "\n",
    "X_train: (25000, 59) X_test: (6950, 59)\n",
    "59 features,\n",
    "Labels: 0 -> Bear, 1 -> Bull\n",
    "Derived from only close price data over 50 instruments, we train on 500 timesteps of data per instrument\n",
    "Models Aim: We want to maximise consistency in regiem identifcation, regiems often last around 150 time steps long, but can range from 30 to 500.\n",
    "\n",
    "Acess data by (all data is shuffled, segregated by bound 500):\n",
    "X_train_scaled : Training set first 500 timesteps of each instrument\n",
    "y_train : Labelled data for X_train_scaled already aligned\n",
    "\n",
    "\n",
    "X_test_scaled : Test set last 250 timesteps of each instrument\n",
    "y_test : Labelled data for X_test_scaled already aligned\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─── 1) Build per‐instrument arrays of exactly 750 timesteps ──────────────\n",
    "raw_X_parts = []\n",
    "raw_y_parts = []\n",
    "for inst, grp in X.groupby(level='inst', sort=False):\n",
    "    arr  = grp.iloc[:750].values                # (750,59)\n",
    "    labs = y.loc[grp.index[:750]].values        # (750,)\n",
    "    raw_X_parts.append(arr)\n",
    "    raw_y_parts.append(labs)\n",
    "\n",
    "X_all = np.stack(raw_X_parts)  # (n_inst, 750, 59)\n",
    "y_all = np.stack(raw_y_parts)  # (n_inst, 750)\n",
    "\n",
    "n_inst, T, F = X_all.shape\n",
    "\n",
    "# ─── 2) Remap labels 1→0, 2→1 and cast to float32 ────────────────────────\n",
    "y_all = (y_all == 2).astype(np.float32)\n",
    "\n",
    "# ─── 3) Fit scaler on train portion (first 450 steps) ────────────────────\n",
    "scaler = StandardScaler()\n",
    "flat_train = X_all[:, :450, :].reshape(-1, F)   # (n_inst*450, 59)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "# apply to all data and cast to float32\n",
    "X_all_scaled = scaler.transform(X_all.reshape(-1, F)) \\\n",
    "                   .reshape(n_inst, T, F) \\\n",
    "                   .astype(np.float32)\n",
    "\n",
    "# ─── 4) Create sliding windows (L=60) and split by last index ────────────\n",
    "L = 60\n",
    "Xw_train, yw_train = [], []\n",
    "Xw_val,   yw_val   = [], []\n",
    "Xw_test,  yw_test  = [], []\n",
    "\n",
    "for inst_idx in range(n_inst):\n",
    "    series = X_all_scaled[inst_idx]  # (750,59), dtype=float32\n",
    "    labels = y_all[inst_idx]         # (750,), dtype=float32\n",
    "    for i in range(T - L + 1):\n",
    "        window = series[i : i + L]       # (60,59)\n",
    "        lab    = labels[i + L - 1]       # scalar float32\n",
    "        end_t  = i + L - 1\n",
    "        if end_t < 450:\n",
    "            Xw_train.append(window); yw_train.append(lab)\n",
    "        elif end_t < 600:\n",
    "            Xw_val.append(window);   yw_val.append(lab)\n",
    "        else:\n",
    "            Xw_test.append(window);  yw_test.append(lab)\n",
    "\n",
    "# stack and ensure float32\n",
    "X_seq_train = np.stack(Xw_train).astype(np.float32)\n",
    "y_seq_train = np.array(yw_train, dtype=np.float32)\n",
    "X_seq_val   = np.stack(Xw_val).astype(np.float32)\n",
    "y_seq_val   = np.array(yw_val,   dtype=np.float32)\n",
    "X_seq_test  = np.stack(Xw_test).astype(np.float32)\n",
    "y_seq_test  = np.array(yw_test,  dtype=np.float32)\n",
    "\n",
    "print(\"Train windows:\", X_seq_train.shape, y_seq_train.shape)\n",
    "print(\" Val windows:\", X_seq_val.shape,   y_seq_val.shape)\n",
    "print(\"Test windows:\", X_seq_test.shape,  y_seq_test.shape)\n",
    "\n",
    "# ─── 5) Dataset + DataLoader ───────────────────────────────────────────────\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # both X and y are np.float32\n",
    "        self.X = torch.from_numpy(X)       # yields FloatTensor\n",
    "        self.y = torch.from_numpy(y)       # FloatTensor\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(RegimeDataset(X_seq_train, y_seq_train),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(RegimeDataset(X_seq_val,   y_seq_val),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(RegimeDataset(X_seq_test,  y_seq_test),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ─── 6) LSTM model ─────────────────────────────────────────────────────────\n",
    "class LSTMRegime(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = n_features,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = (dropout if num_layers>1 else 0.0)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is FloatTensor\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last        = h_n[-1]           # (batch, hidden_dim)\n",
    "        h_last        = self.drop(h_last)\n",
    "        return torch.sigmoid(self.fc(h_last)).squeeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegime(n_features=F).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "crit   = nn.BCELoss()\n",
    "\n",
    "# ─── 7) Train on first 450, validate on next 150 ──────────────────────────\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — training —\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{num_epochs}\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = crit(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # — validation (frozen) —\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb  = xb.to(device), yb.to(device)\n",
    "            plabels = (model(xb) > 0.5).float()\n",
    "            correct += (plabels == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ─── 8) Final test evaluation ───────────────────────────────────────────────\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb  = xb.to(device), yb.to(device)\n",
    "        plabels = (model(xb) > 0.5).float()\n",
    "        correct += (plabels == yb).sum().item()\n",
    "        total   += yb.numel()\n",
    "test_acc = correct / total\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch, joblib\n",
    "\n",
    "def save_artifacts(model, scaler, dirname=\"artifacts\"):\n",
    "    \"\"\"\n",
    "    Creates ./artifacts/ (if missing) and writes:\n",
    "      - regime_lstm_state_dict.pth\n",
    "      - scaler.pkl\n",
    "    \"\"\"\n",
    "    # 1) Point at ./artifacts\n",
    "    ART_PATH = Path().cwd() / dirname\n",
    "    ART_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 2) Save the model weights\n",
    "    model_file  = ART_PATH / \"regime_lstm_state_dict.pth\"\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    \n",
    "    # 3) Save the scaler\n",
    "    scaler_file = ART_PATH / \"scaler.pkl\"\n",
    "    joblib.dump(scaler, scaler_file)\n",
    "    \n",
    "    print(f\"✅ Saved model to  {model_file}\")\n",
    "    print(f\"✅ Saved scaler to {scaler_file}\")\n",
    "\n",
    "# ─── USAGE ──────────────────────────────────────────────────────────────────\n",
    "# … after you train your `model` and fit your `scaler`, just call:\n",
    "save_artifacts(model, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Inference Speed Benchmark ────────────────────────────────────────────────\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy batch of the same shape as your real inputs\n",
    "# (batch_size, sequence_length, n_features)\n",
    "batch_size, L, F = 256, 60, 59\n",
    "dummy_input = torch.randn(batch_size, L, F, device=device)\n",
    "\n",
    "# Warm‐up runs (to stabilize CUDA kernels / caches)\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "# Actual timing\n",
    "n_runs = 100\n",
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_runs):\n",
    "        _ = model(dummy_input)\n",
    "end = time.perf_counter()\n",
    "\n",
    "total_time = end - start\n",
    "avg_batch_time = total_time / n_runs\n",
    "avg_sample_time = avg_batch_time / batch_size\n",
    "\n",
    "print(f\"Ran {n_runs} forward passes on batches of {batch_size} sequences.\")\n",
    "print(f\"→ Total time:        {total_time:.4f} s\")\n",
    "print(f\"→ Avg time / batch:  {avg_batch_time:.6f} s\")\n",
    "print(f\"→ Avg time / sample: {avg_sample_time:.6f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
